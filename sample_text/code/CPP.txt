// This file is needed for compilation on some platforms e.g. with XCode generator
// Related issue: https://gitlab.kitware.com/cmake/cmake/-/issues/17457

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include <algorithm>
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
template <typename T>
struct AbsDiff
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0,
const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::vec128 & v_dst) const
{
v_dst = internal::vabdq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0,
const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::vec64 & v_dst) const
{
v_dst = internal::vabd(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, T * dst) const
{
dst[0] = src0[0] >= src1[0] ? src0[0] - src1[0] : src1[0] - src0[0];
}
};
template <typename T>
struct AbsDiffSigned
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0,
const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::vec128 & v_dst) const
{
typename internal::VecTraits<T>::vec128 v_min = internal::vminq(v_src0, v_src1);
typename internal::VecTraits<T>::vec128 v_max = internal::vmaxq(v_src0, v_src1);
v_dst = internal::vqsubq(v_max, v_min);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0,
const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::vec64 & v_dst) const
{
typename internal::VecTraits<T>::vec64 v_min = internal::vmin(v_src0, v_src1);
typename internal::VecTraits<T>::vec64 v_max = internal::vmax(v_src0, v_src1);
v_dst = internal::vqsub(v_max, v_min);
}
void operator() (const T * src0, const T * src1, T * dst) const
{
dst[0] = internal::saturate_cast<T>(src0[0] >= src1[0] ? (s64)src0[0] - src1[0] : (s64)src1[0] - src0[0]);
}
};
} // namespace
#endif
void absDiff(const Size2D &size,
const u8 *src0Base, ptrdiff_t src0Stride,
const u8 *src1Base, ptrdiff_t src1Stride,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiff<u8>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void absDiff(const Size2D &size,
const u16 *src0Base, ptrdiff_t src0Stride,
const u16 *src1Base, ptrdiff_t src1Stride,
u16 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiff<u16>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void absDiff(const Size2D &size,
const s8 *src0Base, ptrdiff_t src0Stride,
const s8 *src1Base, ptrdiff_t src1Stride,
s8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiffSigned<s8>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void absDiff(const Size2D &size,
const s16 *src0Base, ptrdiff_t src0Stride,
const s16 *src1Base, ptrdiff_t src1Stride,
s16 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiffSigned<s16>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void absDiff(const Size2D &size,
const s32 *src0Base, ptrdiff_t src0Stride,
const s32 *src1Base, ptrdiff_t src1Stride,
s32 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiffSigned<s32>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void absDiff(const Size2D &size,
const f32 * src0Base, ptrdiff_t src0Stride,
const f32 * src1Base, ptrdiff_t src1Stride,
f32 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, AbsDiff<f32>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/

#include "common.hpp"
#include "vtransform.hpp"
#include <cstring>
namespace CAROTENE_NS {
void accumulate(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
s16 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8* src = internal::getRowPtr(srcBase, srcStride, i);
s16* dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
internal::prefetch(dst + j);
uint8x16_t v_src = vld1q_u8(src + j);
int16x8_t v_dst0 = vld1q_s16(dst + j);
int16x8_t v_dst1 = vld1q_s16(dst + j + 8);
int16x8_t v_src0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src)));
int16x8_t v_src1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src)));
v_dst0 = vqaddq_s16(v_dst0, v_src0);
v_dst1 = vqaddq_s16(v_dst1, v_src1);
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
uint8x8_t v_src = vld1_u8(src + j);
int16x8_t v_src16 = vreinterpretq_s16_u16(vmovl_u8(v_src));
int16x8_t v_dst = vld1q_s16(dst + j);
v_dst = vqaddq_s16(v_dst, v_src16);
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
dst[j] = internal::saturate_cast<s16>(src[j] + dst[j]);
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
#ifdef CAROTENE_NEON
namespace {
template <int shift>
void accumulateSquareConst(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
s16 *dstBase, ptrdiff_t dstStride)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8* src = internal::getRowPtr(srcBase, srcStride, i);
s16* dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
internal::prefetch(dst + j);
uint8x16_t v_src = vld1q_u8(src + j);
int16x8_t v_dst0 = vld1q_s16(dst + j), v_dst1 = vld1q_s16(dst + j + 8);
int16x8_t v_src0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src)));
int16x8_t v_src1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src)));
int16x4_t v_srclo = vget_low_s16(v_src0), v_srchi = vget_high_s16(v_src0);
v_dst0 = vcombine_s16(vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srclo, v_srclo), shift), vget_low_s16(v_dst0))),
vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srchi, v_srchi), shift), vget_high_s16(v_dst0))));
v_srclo = vget_low_s16(v_src1);
v_srchi = vget_high_s16(v_src1);
v_dst1 = vcombine_s16(vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srclo, v_srclo), shift), vget_low_s16(v_dst1))),
vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srchi, v_srchi), shift), vget_high_s16(v_dst1))));
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src + j)));
int16x8_t v_dst = vld1q_s16(dst + j);
int16x4_t v_srclo = vget_low_s16(v_src), v_srchi = vget_high_s16(v_src);
v_dst = vcombine_s16(vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srclo, v_srclo), shift), vget_low_s16(v_dst))),
vqmovn_s32(vaddw_s16(vshrq_n_s32(vmull_s16(v_srchi, v_srchi), shift), vget_high_s16(v_dst))));
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
{
s32 srcVal = src[j];
dst[j] = internal::saturate_cast<s16>(dst[j] + ((srcVal * srcVal) >> shift));
}
}
}
template <>
void accumulateSquareConst<0>(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
s16 *dstBase, ptrdiff_t dstStride)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8* src = internal::getRowPtr(srcBase, srcStride, i);
s16* dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
internal::prefetch(dst + j);
uint8x16_t v_src = vld1q_u8(src + j);
int16x8_t v_dst0 = vld1q_s16(dst + j), v_dst1 = vld1q_s16(dst + j + 8);
int16x8_t v_src0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src)));
int16x8_t v_src1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src)));
int16x4_t v_srclo = vget_low_s16(v_src0), v_srchi = vget_high_s16(v_src0);
v_dst0 = vcombine_s16(vqmovn_s32(vaddw_s16(vmull_s16(v_srclo, v_srclo), vget_low_s16(v_dst0))),
vqmovn_s32(vaddw_s16(vmull_s16(v_srchi, v_srchi), vget_high_s16(v_dst0))));
v_srclo = vget_low_s16(v_src1);
v_srchi = vget_high_s16(v_src1);
v_dst1 = vcombine_s16(vqmovn_s32(vaddw_s16(vmull_s16(v_srclo, v_srclo), vget_low_s16(v_dst1))),
vqmovn_s32(vaddw_s16(vmull_s16(v_srchi, v_srchi), vget_high_s16(v_dst1))));
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src + j)));
int16x8_t v_dst = vld1q_s16(dst + j);
int16x4_t v_srclo = vget_low_s16(v_src), v_srchi = vget_high_s16(v_src);
v_dst = vcombine_s16(vqmovn_s32(vaddw_s16(vmull_s16(v_srclo, v_srclo), vget_low_s16(v_dst))),
vqmovn_s32(vaddw_s16(vmull_s16(v_srchi, v_srchi), vget_high_s16(v_dst))));
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
{
s32 srcVal = src[j];
dst[j] = internal::saturate_cast<s16>(dst[j] + srcVal * srcVal);
}
}
}
typedef void (* accumulateSquareConstFunc)(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
s16 *dstBase, ptrdiff_t dstStride);
} // namespace
#endif
void accumulateSquare(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
s16 *dstBase, ptrdiff_t dstStride,
u32 shift)
{
if (shift >= 16)
{
for (size_t i = 0; i < size.height; ++i)
{
s16 * dst = internal::getRowPtr(dstBase, dstStride, i);
std::memset(dst, 0, sizeof(s16) * size.width);
}
return;
}
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
// this ugly contruction is needed to avoid:
// /usr/lib/gcc/arm-linux-gnueabihf/4.8/include/arm_neon.h:3581:59: error: argument must be a constant
// return (int16x8_t)__builtin_neon_vshr_nv8hi (__a, __b, 1);
accumulateSquareConstFunc funcs[16] =
{
accumulateSquareConst<0>,
accumulateSquareConst<1>,
accumulateSquareConst<2>,
accumulateSquareConst<3>,
accumulateSquareConst<4>,
accumulateSquareConst<5>,
accumulateSquareConst<6>,
accumulateSquareConst<7>,
accumulateSquareConst<8>,
accumulateSquareConst<9>,
accumulateSquareConst<10>,
accumulateSquareConst<11>,
accumulateSquareConst<12>,
accumulateSquareConst<13>,
accumulateSquareConst<14>,
accumulateSquareConst<15>
}, func = funcs[shift];
func(size, srcBase, srcStride, dstBase, dstStride);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)shift;
#endif
}
#ifdef CAROTENE_NEON
namespace {
struct AccumulateWeightedHalf
{
typedef u8 type;
void operator() (const uint8x16_t & v_src0, const uint8x16_t & v_src1,
uint8x16_t & v_dst) const
{
v_dst = vhaddq_u8(v_src0, v_src1);
}
void operator() (const uint8x8_t & v_src0, const uint8x8_t & v_src1,
uint8x8_t & v_dst) const
{
v_dst = vhadd_u8(v_src0, v_src1);
}
void operator() (const u8 * src0, const u8 * src1, u8 * dst) const
{
dst[0] = ((u16)(src0[0]) + src1[0]) >> 1;
}
};
struct AccumulateWeighted
{
typedef u8 type;
float alpha, beta;
float32x4_t v_alpha, v_beta;
explicit AccumulateWeighted(float _alpha) :
alpha(_alpha), beta(1 - _alpha)
{
v_alpha = vdupq_n_f32(alpha);
v_beta = vdupq_n_f32(beta);
}
void operator() (const uint8x16_t & v_src0, const uint8x16_t & v_src1,
uint8x16_t & v_dst) const
{
uint16x8_t v_src0_p = vmovl_u8(vget_low_u8(v_src0));
uint16x8_t v_src1_p = vmovl_u8(vget_low_u8(v_src1));
float32x4_t v_dst0f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src1_p))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src0_p))));
float32x4_t v_dst1f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src1_p))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src0_p))));
uint16x8_t v_dst0 = vcombine_u16(vmovn_u32(vcvtq_u32_f32(v_dst0f)),
vmovn_u32(vcvtq_u32_f32(v_dst1f)));
v_src0_p = vmovl_u8(vget_high_u8(v_src0));
v_src1_p = vmovl_u8(vget_high_u8(v_src1));
v_dst0f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src1_p))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src0_p))));
v_dst1f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src1_p))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src0_p))));
uint16x8_t v_dst1 = vcombine_u16(vmovn_u32(vcvtq_u32_f32(v_dst0f)),
vmovn_u32(vcvtq_u32_f32(v_dst1f)));
v_dst = vcombine_u8(vmovn_u16(v_dst0), vmovn_u16(v_dst1));
}
void operator() (const uint8x8_t & _v_src0, const uint8x8_t & _v_src1,
uint8x8_t & v_dst) const
{
uint16x8_t v_src0 = vmovl_u8(_v_src0), v_src1 = vmovl_u8(_v_src1);
float32x4_t v_dst0f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src1))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_low_u16(v_src0))));
float32x4_t v_dst1f = vmlaq_f32(vmulq_f32(vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src1))), v_beta),
v_alpha, vcvtq_f32_u32(vmovl_u16(vget_high_u16(v_src0))));
uint16x8_t _v_dst = vcombine_u16(vmovn_u32(vcvtq_u32_f32(v_dst0f)),
vmovn_u32(vcvtq_u32_f32(v_dst1f)));
v_dst = vmovn_u16(_v_dst);
}
void operator() (const u8 * src0, const u8 * src1, u8 * dst) const
{
dst[0] = beta * src1[0] + alpha * src0[0];
}
};
} // namespace
#endif
void accumulateWeighted(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
u8 *dstBase, ptrdiff_t dstStride,
f32 alpha)
{
if (alpha == 0.0f)
return;
if (alpha == 1.0f)
{
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
std::memcpy(dst, src, sizeof(u8) * size.width);
}
return;
}
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
// in this case we can use the following scheme:
// dst[p] = (src[p] + dst[p]) >> 1
// which is faster
if (alpha == 0.5f)
{
internal::vtransform(size,
srcBase, srcStride,
dstBase, dstStride,
dstBase, dstStride,
AccumulateWeightedHalf());
return;
}
internal::vtransform(size,
srcBase, srcStride,
dstBase, dstStride,
dstBase, dstStride,
AccumulateWeighted(alpha));
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)alpha;
#endif
}
} //namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
template <typename T, typename WT>
struct AddWrap
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0,
const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::vec128 & v_dst) const
{
v_dst = internal::vaddq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0,
const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::vec64 & v_dst) const
{
v_dst = internal::vadd(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, T * dst) const
{
dst[0] = (T)((WT)src0[0] + (WT)src1[0]);
}
};
template <typename T, typename WT>
struct AddSaturate
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0,
const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::vec128 & v_dst) const
{
v_dst = internal::vqaddq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0,
const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::vec64 & v_dst) const
{
v_dst = internal::vqadd(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, T * dst) const
{
dst[0] = internal::saturate_cast<T>((WT)src0[0] + (WT)src1[0]);
}
};
} // namespace
#endif
void add(const Size2D &size,
const u8 * src0Base, ptrdiff_t src0Stride,
const u8 * src1Base, ptrdiff_t src1Stride,
u8 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<u8, u16>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<u8, u16>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const s8 * src0Base, ptrdiff_t src0Stride,
const s8 * src1Base, ptrdiff_t src1Stride,
s8 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<s8, s16>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<s8, s16>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const u8 * src0Base, ptrdiff_t src0Stride,
const u8 * src1Base, ptrdiff_t src1Stride,
s16 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src0 = internal::getRowPtr(src0Base, src0Stride, i);
const u8 * src1 = internal::getRowPtr(src1Base, src1Stride, i);
u16 * dst = internal::getRowPtr((u16 *)dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw32; j += 32)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
uint8x16_t v_src00 = vld1q_u8(src0 + j), v_src01 = vld1q_u8(src0 + j + 16);
uint8x16_t v_src10 = vld1q_u8(src1 + j), v_src11 = vld1q_u8(src1 + j + 16);
vst1q_u16(dst + j, vaddl_u8(vget_low_u8(v_src00), vget_low_u8(v_src10)));
vst1q_u16(dst + j + 8, vaddl_u8(vget_high_u8(v_src00), vget_high_u8(v_src10)));
vst1q_u16(dst + j + 16, vaddl_u8(vget_low_u8(v_src01), vget_low_u8(v_src11)));
vst1q_u16(dst + j + 24, vaddl_u8(vget_high_u8(v_src01), vget_high_u8(v_src11)));
}
for (; j < roiw8; j += 8)
{
uint8x8_t v_src0 = vld1_u8(src0 + j);
uint8x8_t v_src1 = vld1_u8(src1 + j);
vst1q_u16(dst + j, vaddl_u8(v_src0, v_src1));
}
for (; j < size.width; j++)
dst[j] = (u16)src0[j] + (u16)src1[j];
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void add(const Size2D &size,
const u8 * src0Base, ptrdiff_t src0Stride,
const s16 * src1Base, ptrdiff_t src1Stride,
s16 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src0 = internal::getRowPtr(src0Base, src0Stride, i);
const s16 * src1 = internal::getRowPtr(src1Base, src1Stride, i);
s16 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
if (policy == CONVERT_POLICY_SATURATE)
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
uint8x16_t v_src0 = vld1q_u8(src0 + j);
int16x8_t v_src00 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src0)));
int16x8_t v_src01 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src0)));
int16x8_t v_src10 = vld1q_s16(src1 + j), v_src11 = vld1q_s16(src1 + j + 8);
int16x8_t v_dst0 = vqaddq_s16(v_src00, v_src10);
int16x8_t v_dst1 = vqaddq_s16(v_src01, v_src11);
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src0 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src0 + j)));
int16x8_t v_src1 = vld1q_s16(src1 + j);
int16x8_t v_dst = vqaddq_s16(v_src0, v_src1);
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
dst[j] = internal::saturate_cast<s16>((s32)src0[j] + (s32)src1[j]);
}
else
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
uint8x16_t v_src0 = vld1q_u8(src0 + j);
int16x8_t v_src00 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src0)));
int16x8_t v_src01 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src0)));
int16x8_t v_src10 = vld1q_s16(src1 + j), v_src11 = vld1q_s16(src1 + j + 8);
int16x8_t v_dst0 = vaddq_s16(v_src00, v_src10);
int16x8_t v_dst1 = vaddq_s16(v_src01, v_src11);
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src0 = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src0 + j)));
int16x8_t v_src1 = vld1q_s16(src1 + j);
int16x8_t v_dst = vaddq_s16(v_src0, v_src1);
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
dst[j] = (s16)((s32)src0[j] + (s32)src1[j]);
}
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const s16 * src0Base, ptrdiff_t src0Stride,
const s16 * src1Base, ptrdiff_t src1Stride,
s16 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<s16, s32>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<s16, s32>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const u16 * src0Base, ptrdiff_t src0Stride,
const u16 * src1Base, ptrdiff_t src1Stride,
u16 * dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<u16, u32>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<u16, u32>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const s32 * src0Base, ptrdiff_t src0Stride,
const s32 * src1Base, ptrdiff_t src1Stride,
s32 *dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<s32, s64>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<s32, s64>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const u32 * src0Base, ptrdiff_t src0Stride,
const u32 * src1Base, ptrdiff_t src1Stride,
u32 * dstBase, ptrdiff_t dstStride,
CONVERT_POLICY policy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (policy == CONVERT_POLICY_SATURATE)
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddSaturate<u32, u64>());
}
else
{
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<u32, u64>());
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)policy;
#endif
}
void add(const Size2D &size,
const f32 * src0Base, ptrdiff_t src0Stride,
const f32 * src1Base, ptrdiff_t src1Stride,
f32 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
AddWrap<f32, f32>());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
#include "vround_helper.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
using namespace internal;
template <typename T> struct TypeTraits;
template <> struct TypeTraits< u8> { typedef u16 wide;                     typedef  u8 unsign; typedef  uint8x16_t vec128; };
template <> struct TypeTraits< s8> { typedef s16 wide;                     typedef  u8 unsign; typedef   int8x16_t vec128; };
template <> struct TypeTraits<u16> { typedef u32 wide; typedef  u8 narrow; typedef u16 unsign; typedef  uint16x8_t vec128; };
template <> struct TypeTraits<s16> { typedef s32 wide; typedef  s8 narrow; typedef u16 unsign; typedef   int16x8_t vec128; };
template <> struct TypeTraits<u32> { typedef u64 wide; typedef u16 narrow; typedef u32 unsign; typedef  uint32x4_t vec128; };
template <> struct TypeTraits<s32> { typedef s64 wide; typedef s16 narrow; typedef u32 unsign; typedef   int32x4_t vec128; };
template <> struct TypeTraits<f32> { typedef f64 wide;                                         typedef float32x4_t vec128; };
template <typename T> struct wAdd
{
typedef T type;
f32 alpha, beta, gamma;
typedef typename TypeTraits<T>::wide wtype;
wAdd<wtype> wideAdd;
wAdd(f32 _alpha, f32 _beta, f32 _gamma):
alpha(_alpha), beta(_beta), gamma(_gamma),
wideAdd(_alpha, _beta, _gamma) {}
void operator() (const typename VecTraits<T>::vec128 & v_src0,
const typename VecTraits<T>::vec128 & v_src1,
typename VecTraits<T>::vec128 & v_dst) const
{
typename VecTraits<wtype>::vec128 vrl, vrh;
wideAdd(vmovl( vget_low(v_src0)), vmovl( vget_low(v_src1)), vrl);
wideAdd(vmovl(vget_high(v_src0)), vmovl(vget_high(v_src1)), vrh);
v_dst = vcombine(vqmovn(vrl), vqmovn(vrh));
}
void operator() (const typename VecTraits<T>::vec64 & v_src0,
const typename VecTraits<T>::vec64 & v_src1,
typename VecTraits<T>::vec64 & v_dst) const
{
typename VecTraits<wtype>::vec128 vr;
wideAdd(vmovl(v_src0), vmovl(v_src1), vr);
v_dst = vqmovn(vr);
}
void operator() (const T * src0, const T * src1, T * dst) const
{
dst[0] = saturate_cast<T>(alpha*src0[0] + beta*src1[0] + gamma);
}
};
template <> struct wAdd<s32>
{
typedef s32 type;
f32 alpha, beta, gamma;
float32x4_t valpha, vbeta, vgamma;
wAdd(f32 _alpha, f32 _beta, f32 _gamma):
alpha(_alpha), beta(_beta), gamma(_gamma)
{
valpha = vdupq_n_f32(_alpha);
vbeta = vdupq_n_f32(_beta);
vgamma = vdupq_n_f32(_gamma);
}
void operator() (const VecTraits<s32>::vec128 & v_src0,
const VecTraits<s32>::vec128 & v_src1,
VecTraits<s32>::vec128 & v_dst) const
{
float32x4_t vs1 = vcvtq_f32_s32(v_src0);
float32x4_t vs2 = vcvtq_f32_s32(v_src1);
vs1 = vmlaq_f32(vgamma, vs1, valpha);
vs1 = vmlaq_f32(vs1, vs2, vbeta);
v_dst = vroundq_s32_f32(vs1);
}
void operator() (const VecTraits<s32>::vec64 & v_src0,
const VecTraits<s32>::vec64 & v_src1,
VecTraits<s32>::vec64 & v_dst) const
{
float32x2_t vs1 = vcvt_f32_s32(v_src0);
float32x2_t vs2 = vcvt_f32_s32(v_src1);
vs1 = vmla_f32(vget_low(vgamma), vs1, vget_low(valpha));
vs1 = vmla_f32(vs1, vs2, vget_low(vbeta));
v_dst = vround_s32_f32(vs1);
}
void operator() (const s32 * src0, const s32 * src1, s32 * dst) const
{
dst[0] = saturate_cast<s32>(alpha*src0[0] + beta*src1[0] + gamma);
}
};
template <> struct wAdd<u32>
{
typedef u32 type;
f32 alpha, beta, gamma;
float32x4_t valpha, vbeta, vgamma;
wAdd(f32 _alpha, f32 _beta, f32 _gamma):
alpha(_alpha), beta(_beta), gamma(_gamma)
{
valpha = vdupq_n_f32(_alpha);
vbeta = vdupq_n_f32(_beta);
vgamma = vdupq_n_f32(_gamma);
}
void operator() (const VecTraits<u32>::vec128 & v_src0,
const VecTraits<u32>::vec128 & v_src1,
VecTraits<u32>::vec128 & v_dst) const
{
float32x4_t vs1 = vcvtq_f32_u32(v_src0);
float32x4_t vs2 = vcvtq_f32_u32(v_src1);
vs1 = vmlaq_f32(vgamma, vs1, valpha);
vs1 = vmlaq_f32(vs1, vs2, vbeta);
v_dst = vroundq_u32_f32(vs1);
}
void operator() (const VecTraits<u32>::vec64 & v_src0,
const VecTraits<u32>::vec64 & v_src1,
VecTraits<u32>::vec64 & v_dst) const
{
float32x2_t vs1 = vcvt_f32_u32(v_src0);
float32x2_t vs2 = vcvt_f32_u32(v_src1);
vs1 = vmla_f32(vget_low(vgamma), vs1, vget_low(valpha));
vs1 = vmla_f32(vs1, vs2, vget_low(vbeta));
v_dst = vround_u32_f32(vs1);
}
void operator() (const u32 * src0, const u32 * src1, u32 * dst) const
{
dst[0] = saturate_cast<u32>(alpha*src0[0] + beta*src1[0] + gamma);
}
};
template <> struct wAdd<f32>
{
typedef f32 type;
f32 alpha, beta, gamma;
float32x4_t valpha, vbeta, vgamma;
wAdd(f32 _alpha, f32 _beta, f32 _gamma):
alpha(_alpha), beta(_beta), gamma(_gamma)
{
valpha = vdupq_n_f32(_alpha);
vbeta = vdupq_n_f32(_beta);
vgamma = vdupq_n_f32(_gamma + 0.5);
}
void operator() (const VecTraits<f32>::vec128 & v_src0,
const VecTraits<f32>::vec128 & v_src1,
VecTraits<f32>::vec128 & v_dst) const
{
float32x4_t vs1 = vmlaq_f32(vgamma, v_src0, valpha);
v_dst = vmlaq_f32(vs1, v_src1, vbeta);
}
void operator() (const VecTraits<f32>::vec64 & v_src0,
const VecTraits<f32>::vec64 & v_src1,
VecTraits<f32>::vec64 & v_dst) const
{
float32x2_t vs1 = vmla_f32(vget_low(vgamma), v_src0, vget_low(valpha));
v_dst = vmla_f32(vs1, v_src1, vget_low(vbeta));
}
void operator() (const f32 * src0, const f32 * src1, f32 * dst) const
{
dst[0] = alpha*src0[0] + beta*src1[0] + gamma;
}
};
} // namespace
#define IMPL_ADDWEIGHTED(type)                                \
void addWeighted(const Size2D &size,                          \
const type * src0Base, ptrdiff_t src0Stride, \
const type * src1Base, ptrdiff_t src1Stride, \
type * dstBase, ptrdiff_t dstStride,         \
f32 alpha, f32 beta, f32 gamma)              \
{                                                             \
internal::assertSupportedConfiguration();                 \
wAdd<type> wgtAdd(alpha,                                  \
beta,                                   \
gamma);                                 \
internal::vtransform(size,                                \
src0Base, src0Stride,                \
src1Base, src1Stride,                \
dstBase, dstStride,                  \
wgtAdd);                             \
}
#else
#define IMPL_ADDWEIGHTED(type)                                \
void addWeighted(const Size2D &,                              \
const type *, ptrdiff_t,                     \
const type *, ptrdiff_t,                     \
type *, ptrdiff_t,                           \
f32, f32, f32)                               \
{                                                             \
internal::assertSupportedConfiguration();                 \
}
#endif
IMPL_ADDWEIGHTED(u8)
IMPL_ADDWEIGHTED(s8)
IMPL_ADDWEIGHTED(u16)
IMPL_ADDWEIGHTED(s16)
IMPL_ADDWEIGHTED(u32)
IMPL_ADDWEIGHTED(s32)
IMPL_ADDWEIGHTED(f32)
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
struct BitwiseAnd
{
typedef u8 type;
void operator() (const uint8x16_t & v_src0, const uint8x16_t & v_src1,
uint8x16_t & v_dst) const
{
v_dst = vandq_u8(v_src0, v_src1);
}
void operator() (const uint8x8_t & v_src0, const uint8x8_t & v_src1,
uint8x8_t & v_dst) const
{
v_dst = vand_u8(v_src0, v_src1);
}
void operator() (const u8 * src0, const u8 * src1, u8 * dst) const
{
dst[0] = src0[0] & src1[0];
}
};
struct BitwiseOr
{
typedef u8 type;
void operator() (const uint8x16_t & v_src0, const uint8x16_t & v_src1,
uint8x16_t & v_dst) const
{
v_dst = vorrq_u8(v_src0, v_src1);
}
void operator() (const uint8x8_t & v_src0, const uint8x8_t & v_src1,
uint8x8_t & v_dst) const
{
v_dst = vorr_u8(v_src0, v_src1);
}
void operator() (const u8 * src0, const u8 * src1, u8 * dst) const
{
dst[0] = src0[0] | src1[0];
}
};
struct BitwiseXor
{
typedef u8 type;
void operator() (const uint8x16_t & v_src0, const uint8x16_t & v_src1,
uint8x16_t & v_dst) const
{
v_dst = veorq_u8(v_src0, v_src1);
}
void operator() (const uint8x8_t & v_src0, const uint8x8_t & v_src1,
uint8x8_t & v_dst) const
{
v_dst = veor_u8(v_src0, v_src1);
}
void operator() (const u8 * src0, const u8 * src1, u8 * dst) const
{
dst[0] = src0[0] ^ src1[0];
}
};
#endif
void bitwiseNot(const Size2D &size,
const u8 *srcBase, ptrdiff_t srcStride,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8* src = internal::getRowPtr(srcBase, srcStride, i);
u8* dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw32; j += 32)
{
internal::prefetch(src + j);
uint8x16_t v_src0 = vld1q_u8(src + j), v_src1 = vld1q_u8(src + j + 16);
uint8x16_t v_dst0 = vmvnq_u8(v_src0), v_dst1 = vmvnq_u8(v_src1);
vst1q_u8(dst + j, v_dst0);
vst1q_u8(dst + j + 16, v_dst1);
}
for (; j < roiw8; j += 8)
{
uint8x8_t v_src = vld1_u8(src + j);
uint8x8_t v_dst = vmvn_u8(v_src);
vst1_u8(dst + j, v_dst);
}
for (; j < size.width; j++)
{
dst[j] = ~src[j];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void bitwiseAnd(const Size2D &size,
const u8 *src0Base, ptrdiff_t src0Stride,
const u8 *src1Base, ptrdiff_t src1Stride,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, BitwiseAnd());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void bitwiseOr(const Size2D &size,
const u8 *src0Base, ptrdiff_t src0Stride,
const u8 *src1Base, ptrdiff_t src1Stride,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, BitwiseOr());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void bitwiseXor(const Size2D &size,
const u8 *src0Base, ptrdiff_t src0Stride,
const u8 *src1Base, ptrdiff_t src1Stride,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride, BitwiseXor());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include <vector>
#include "common.hpp"
#include "saturate_cast.hpp"
#include "vround_helper.hpp"
namespace CAROTENE_NS {
bool isBlur3x3Supported(const Size2D &size, BORDER_MODE border)
{
return isSupportedConfiguration() && size.width >= 8 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REPLICATE);
}
void blur3x3(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isBlur3x3Supported(size, border));
#ifdef CAROTENE_NEON
const int16x8_t v_scale = vmovq_n_s16(3640);
const uint16x8_t v_border_x3 = vdupq_n_u16(borderValue * 3);
const uint16x8_t v_zero = vdupq_n_u16(0);
const uint8x8_t v_border = vdup_n_u8(borderValue);
uint16x8_t tprev = v_zero, tcurr = v_zero, tnext = v_zero;
uint16x8_t t0 = v_zero, t1 = v_zero, t2 = v_zero;
ptrdiff_t width = (ptrdiff_t)size.width, height = (ptrdiff_t)size.height;
for (ptrdiff_t y = 0; y < height; ++y)
{
const u8 * srow0 = y == 0 && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::max<ptrdiff_t>(y - 1, 0));
const u8 * srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8 * srow2 = y + 1 == height && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::min(y + 1, height - 1));
u8 * drow = internal::getRowPtr(dstBase, dstStride, y);
s16 prevx = 0, currx = 0, nextx = 0;
ptrdiff_t x = 0;
const ptrdiff_t bwidth = y + 2 < height ? width : (width - 8);
// perform vertical convolution
for ( ; x <= bwidth; x += 8)
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = !srow0 ? v_border : vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = !srow2 ? v_border : vld1_u8(srow2 + x);
// calculate values for plain CPU part below if needed
if (x + 8 >= bwidth)
{
ptrdiff_t x3 = x == width ? width - 1 : x;
ptrdiff_t x4 = border == BORDER_MODE_CONSTANT ? x3 - 1 : std::max<ptrdiff_t>(x3 - 1, 0);
if (border == BORDER_MODE_CONSTANT && x4 < 0)
prevx = borderValue;
else
prevx = (srow2 ? srow2[x4] : borderValue) + srow1[x4] + (srow0 ? srow0[x4] : borderValue);
currx = (srow2 ? srow2[x3] : borderValue) + srow1[x3] + (srow0 ? srow0[x3] : borderValue);
}
// make shift
if (x)
{
tprev = tcurr;
tcurr = tnext;
}
// and calculate next value
tnext = vaddw_u8(vaddl_u8(x0, x1), x2);
// make extrapolation for the first elements
if (!x)
{
// make border
if (border == BORDER_MODE_CONSTANT)
tcurr = v_border_x3;
else if (border == BORDER_MODE_REPLICATE)
tcurr = vdupq_n_u16(vgetq_lane_u16(tnext, 0));
continue;
}
// combine 3 "shifted" vectors
t0 = vextq_u16(tprev, tcurr, 7);
t1 = tcurr;
t2 = vextq_u16(tcurr, tnext, 1);
// and add them
t0 = vqaddq_u16(t0, vqaddq_u16(t1, t2));
int16x8_t tt0 = vqrdmulhq_s16(vreinterpretq_s16_u16(t0), v_scale);
uint8x8_t it0 = vmovn_u16(vreinterpretq_u16_s16(tt0));
vst1_u8(drow + x - 8, it0);
}
x -= 8;
if (x == width)
--x;
for ( ; x < width; ++x)
{
// make extrapolation for the last elements
if (x + 1 >= width)
{
if (border == BORDER_MODE_CONSTANT)
nextx = borderValue * 3;
else if (border == BORDER_MODE_REPLICATE)
nextx = srow2[x] + srow1[x] + srow0[x];
}
else
nextx = (srow2 ? srow2[x + 1] : borderValue) +
srow1[x + 1] +
(srow0 ? srow0[x + 1] : borderValue);
f32 val = (prevx + currx + nextx) * (1 / 9.f) + 0.5f;
drow[x] = internal::saturate_cast<u8>((s32)val);
// make shift
prevx = currx;
currx = nextx;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
#endif
}
bool isBlurU8Supported(const Size2D &size, s32 cn, BORDER_MODE border)
{
return isSupportedConfiguration() &&
cn > 0 && cn <= 4 &&
size.width*cn >= 8 && size.height >= 2 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REFLECT101 ||
border == BORDER_MODE_REFLECT ||
border == BORDER_MODE_REPLICATE);
}
void blur3x3(const Size2D &size, s32 cn,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, u8 borderValue)
{
internal::assertSupportedConfiguration(isBlurU8Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
//#define FLOAT_VARIANT_1_9
#ifdef FLOAT_VARIANT_1_9
float32x4_t v1_9 = vdupq_n_f32 (1.0/9.0);
#else
const int16x8_t vScale = vmovq_n_s16(3640);
#endif
size_t colsn = size.width*cn;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 2*cn, borderValue);
tmp = &_tmp[cn];
}
uint16x8_t tprev = vdupq_n_u16(0x0);
uint16x8_t tcurr = tprev;
uint16x8_t tnext = tprev;
uint16x8_t t0, t1, t2;
if(cn == 1)
{
for( size_t y = 0; y < size.height; y++ )
{
const u8* srow0;
const u8* srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8* srow2;
u8* drow = internal::getRowPtr(dstBase, dstStride, y);
if (borderType == BORDER_MODE_REFLECT101) {
srow0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 1);
srow2 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-2);
} else  if (borderType == BORDER_MODE_CONSTANT) {
srow0 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
srow2 =  y < size.height-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
} else { // BORDER_MODE_REFLECT || BORDER_MODE_REPLICATE
srow0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
srow2 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-1);
}
// do vertical convolution
size_t x = 0;
const size_t bcols = y + 2 < size.height ? colsn : (colsn - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = vld1_u8(srow2 + x);
tprev = tcurr;
tcurr = tnext;
tnext = vaddw_u8(vaddl_u8(x0, x1), x2);
if(!x) {
tcurr = tnext;
// make border
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 7);
}
else // borderType == BORDER_MODE_REFLECT || borderType == BORDER_MODE_REPLICATE
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 7);
}
continue;
}
t0 = vextq_u16(tprev, tcurr, 7);
t1 = tcurr;
t2 = vextq_u16(tcurr, tnext, 1);
t0 = vqaddq_u16(t0, vqaddq_u16(t1, t2));
#ifdef FLOAT_VARIANT_1_9
uint32x4_t tres1 = vmovl_u16(vget_low_u16(t0));
uint32x4_t tres2 = vmovl_u16(vget_high_u16(t0));
float32x4_t vf1 = vmulq_f32(v1_9, vcvtq_f32_u32(tres1));
float32x4_t vf2 = vmulq_f32(v1_9, vcvtq_f32_u32(tres2));
tres1 = internal::vroundq_u32_f32(vf1);
tres2 = internal::vroundq_u32_f32(vf2);
t0 = vcombine_u16(vmovn_u32(tres1),vmovn_u32(tres2));
vst1_u8(drow + x - 8, vmovn_u16(t0));
#else
int16x8_t tt0 = vqrdmulhq_s16(vreinterpretq_s16_u16(t0), vScale);
uint8x8_t it0 = vmovn_u16(vreinterpretq_u16_s16(tt0));
vst1_u8(drow + x - 8, it0);
#endif
}
x -= 8;
if(x == colsn){
x--;
}
s16 prevx, rowx, nextx;
prevx = srow2[x-1] + srow1[x-1] + srow0[x-1];
rowx = srow2[x] + srow1[x] + srow0[x];
for( ; x < colsn; x++ )
{
if(x+1 >= colsn) {
// make border
if (borderType == BORDER_MODE_CONSTANT)
{
nextx = borderValue;
} else if (borderType == BORDER_MODE_REFLECT101)
{
nextx = srow2[x-1] + srow1[x-1] + srow0[x-1];
} else {
nextx = srow2[x] + srow1[x] + srow0[x];
}
} else {
nextx = srow2[x+1] + srow1[x+1] + srow0[x+1];
}
*(drow+x) = internal::saturate_cast<u8>((prevx + rowx + nextx)*(1/9.));
prevx = rowx;
rowx = nextx;
}
}
}
else
{
for( size_t y = 0; y < size.height; y++ )
{
const u8* srow0;
const u8* srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8* srow2;
u8* drow = internal::getRowPtr(dstBase, dstStride, y);
if (borderType == BORDER_MODE_REFLECT101) {
srow0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 1);
srow2 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-2);
} else  if (borderType == BORDER_MODE_CONSTANT) {
srow0 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
srow2 =  y < size.height-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
} else { // BORDER_MODE_REFLECT || BORDER_MODE_REPLICATE
srow0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
srow2 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-1);
}
// do vertical convolution
size_t x = 0;
const size_t bcols = y + 2 < size.height ? colsn : (colsn - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = vld1_u8(srow2 + x);
tprev = tcurr;
tcurr = tnext;
tnext = vaddw_u8(vaddl_u8(x0, x1), x2);
if(!x) {
tcurr = tnext;
// make border
switch(cn)
{
case 2:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 6);
}
else
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 7);
}
break;
case 3:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 5);
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 4),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 5),tcurr, 7);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 5);
}
else
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 5);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 7);
}
break;
case 4:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 4);
tcurr = vsetq_lane_u16(borderValue, tcurr, 5);
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType != BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 4);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 5);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 7);
}
break;
}
continue;
}
if(cn==2)
t0 = vextq_u16(tprev, tcurr, 6);
else if(cn==3)
t0 = vextq_u16(tprev, tcurr, 5);
else if(cn==4)
t0 = vextq_u16(tprev, tcurr, 4);
t1 = tcurr;
if(cn==2)
t2 = vextq_u16(tcurr, tnext, 2);
else if(cn==3)
t2 = vextq_u16(tcurr, tnext, 3);
else if(cn==4)
t2 = vextq_u16(tcurr, tnext, 4);
t0 = vqaddq_u16(t0, vqaddq_u16(t1, t2));
#ifdef FLOAT_VARIANT_1_9
uint32x4_t tres1 = vmovl_u16(vget_low_u16(t0));
uint32x4_t tres2 = vmovl_u16(vget_high_u16(t0));
float32x4_t vf1 = vmulq_f32(v1_9, vcvtq_f32_u32(tres1));
float32x4_t vf2 = vmulq_f32(v1_9, vcvtq_f32_u32(tres2));
tres1 = internal::vroundq_u32_f32(vf1);
tres2 = internal::vroundq_u32_f32(vf2);
t0 = vcombine_u16(vmovn_u32(tres1),vmovn_u32(tres2));
vst1_u8(drow + x - 8, vmovn_u16(t0));
#else
int16x8_t tt0 = vqrdmulhq_s16(vreinterpretq_s16_u16(t0), vScale);
uint8x8_t it0 = vmovn_u16(vreinterpretq_u16_s16(tt0));
vst1_u8(drow + x - 8, it0);
#endif
}
x -= 8;
if(x == colsn){
x -= cn;
}
s16 prevx[4], rowx[4], nextx[4];
for( s32 k = 0; k < cn; k++ )
{
prevx[(k + x%cn)%cn] = srow2[x+k-cn] + srow1[x+k-cn] + srow0[x+k-cn];
rowx[(k + x%cn)%cn] = srow2[x+k] + srow1[x+k] + srow0[x+k];
}
for( ; x < colsn; x++ )
{
size_t xx = x%cn;
if(x+cn >= colsn) {
// make border
if (borderType == BORDER_MODE_CONSTANT)
{
nextx[xx] = borderValue;
} else if (borderType == BORDER_MODE_REFLECT101)
{
nextx[xx] = srow2[x-cn] + srow1[x-cn] + srow0[x-cn];
} else {
nextx[xx] = srow2[x] + srow1[x] + srow0[x];
}
} else {
nextx[xx] = srow2[x+cn] + srow1[x+cn] + srow0[x+cn];
}
*(drow+x) = internal::saturate_cast<u8>((prevx[xx] + rowx[xx] + nextx[xx])*(1/9.));
prevx[xx] = rowx[xx];
rowx[xx] = nextx[xx];
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
#endif
}
void blur5x5(const Size2D &size, s32 cn,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, u8 borderValue)
{
internal::assertSupportedConfiguration(isBlurU8Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
#define FLOAT_VARIANT_1_25
#ifdef FLOAT_VARIANT_1_25
float32x4_t v1_25 = vdupq_n_f32 (1.0f/25.0f);
#else
const int16x8_t vScale = vmovq_n_s16(1310);
#endif
size_t colsn = size.width*cn;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 2*cn, borderValue);
tmp = &_tmp[cn];
}
uint16x8_t tprev = vdupq_n_u16(0x0);
uint16x8_t tcurr = tprev;
uint16x8_t tnext = tprev;
uint16x8_t t0, t1, t2, t3, t4;
for( size_t y = 0; y < size.height; y++ )
{
const u8 *srow0, *srow1;
const u8 *srow2 = internal::getRowPtr(srcBase, srcStride, y);
const u8 *srow3, *srow4;
u8 *drow = internal::getRowPtr(dstBase, dstStride, y);
if (borderType == BORDER_MODE_REFLECT101) {
srow0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : 2-y);
srow1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 1);
srow3 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-2);
srow4 = internal::getRowPtr(srcBase, srcStride, y < size.height-2 ? y+2 : (size.height<<1)-4-y);
} else  if (borderType == BORDER_MODE_CONSTANT) {
srow0 = y > 1 ? internal::getRowPtr(srcBase, srcStride, y-2) : tmp;
srow1 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
srow3 =  y < size.height-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
srow4 =  y < size.height-2 ? internal::getRowPtr(srcBase, srcStride, y+2) : tmp;
} else  if (borderType == BORDER_MODE_REFLECT) {
srow0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : 1-y);
srow1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
srow3 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-1);
srow4 = internal::getRowPtr(srcBase, srcStride, y < size.height-2 ? y+2 : (size.height<<1)-3-y);
} else { // BORDER_MODE_REPLICATE
srow0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : 0);
srow1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
srow3 = internal::getRowPtr(srcBase, srcStride, y < size.height-1 ? y+1 : size.height-1);
srow4 = internal::getRowPtr(srcBase, srcStride, y < size.height-2 ? y+2 : size.height-1);
}
// do vertical convolution
size_t x = 0;
const size_t bcols = y + 3 < size.height ? colsn : (colsn - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
internal::prefetch(srow3 + x);
internal::prefetch(srow4 + x);
uint8x8_t x0 = vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = vld1_u8(srow2 + x);
uint8x8_t x3 = vld1_u8(srow3 + x);
uint8x8_t x4 = vld1_u8(srow4 + x);
tprev = tcurr;
tcurr = tnext;
tnext = vaddw_u8(vaddq_u16(vaddl_u8(x0, x1), vaddl_u8(x2, x3)), x4);
if(!x) {
tcurr = tnext;
if(borderType == BORDER_MODE_REFLECT101 && size.width < 3)
{
x = 8;
break;
}
// make border
switch(cn)
{
case 1:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 7);
}
else
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 7);
}
break;
case 2:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 4);
tcurr = vsetq_lane_u16(borderValue, tcurr, 5);
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT)
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tcurr, 4);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 5);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 7);
}
else
{
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 4);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 5);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tcurr, 7);
}
break;
case 3:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 2);
tcurr = vsetq_lane_u16(borderValue, tcurr, 3);
tcurr = vsetq_lane_u16(borderValue, tcurr, 4);
tcurr = vsetq_lane_u16(borderValue, tcurr, 5);
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 6),tcurr, 2);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 7),tprev, 3);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tprev, 5);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 4),tprev, 6);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 5),tprev, 7);
s16 lane8 = srow4[8] + srow3[8] + srow2[8] + srow1[8] + srow0[8];
tcurr = vsetq_lane_u16(lane8,tprev, 4);
}
else if (borderType == BORDER_MODE_REFLECT)
{
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 3),tcurr, 2);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 4),tprev, 3);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 5),tprev, 4);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tprev, 5);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tprev, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tprev, 7);
}
else
{
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tcurr, 2);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tprev, 3);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tprev, 4);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 0),tprev, 5);
tprev = vsetq_lane_u16(vgetq_lane_u16(tcurr, 1),tprev, 6);
tcurr = vsetq_lane_u16(vgetq_lane_u16(tcurr, 2),tprev, 7);
}
break;
case 4:
if (borderType == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_u16(borderValue, tcurr, 0);
tcurr = vsetq_lane_u16(borderValue, tcurr, 1);
tcurr = vsetq_lane_u16(borderValue, tcurr, 2);
tcurr = vsetq_lane_u16(borderValue, tcurr, 3);
tcurr = vsetq_lane_u16(borderValue, tcurr, 4);
tcurr = vsetq_lane_u16(borderValue, tcurr, 5);
tcurr = vsetq_lane_u16(borderValue, tcurr, 6);
tcurr = vsetq_lane_u16(borderValue, tcurr, 7);
}
else if (borderType == BORDER_MODE_REFLECT101)
{
s16 lane8  = srow4[ 8] + srow3[ 8] + srow2[ 8] + srow1[ 8] + srow0[ 8];
s16 lane9  = srow4[ 9] + srow3[ 9] + srow2[ 9] + srow1[ 9] + srow0[ 9];
s16 lane10 = srow4[10] + srow3[10] + srow2[10] + srow1[10] + srow0[10];
s16 lane11 = srow4[11] + srow3[11] + srow2[11] + srow1[11] + srow0[11];
tprev = vsetq_lane_u16( lane8,tcurr, 0);
tprev = vsetq_lane_u16( lane9,tprev, 1);
tprev = vsetq_lane_u16(lane10,tprev, 2);
tcurr = vsetq_lane_u16(lane11,tprev, 3);
}
else if (borderType == BORDER_MODE_REFLECT)
{
tcurr = vcombine_u16(vget_high_u16(tcurr),vget_low_u16(tcurr));//swap 64-bit parts
}
else
{
tcurr = vcombine_u16(vget_low_u16(tcurr),vget_low_u16(tcurr));//double 64-bit part
}
break;
}
continue;
}
switch(cn)
{
case 1:
t0 = vextq_u16(tprev, tcurr, 6);
t1 = vextq_u16(tprev, tcurr, 7);
t2 = tcurr;
t3 = vextq_u16(tcurr, tnext, 1);
t4 = vextq_u16(tcurr, tnext, 2);
break;
case 2:
t0 = vextq_u16(tprev, tcurr, 4);
t1 = vextq_u16(tprev, tcurr, 6);
t2 = tcurr;
t3 = vextq_u16(tcurr, tnext, 2);
t4 = vextq_u16(tcurr, tnext, 4);
break;
case 3:
t0 = vextq_u16(tprev, tcurr, 2);
t1 = vextq_u16(tprev, tcurr, 5);
t2 = tcurr;
t3 = vextq_u16(tcurr, tnext, 3);
t4 = vextq_u16(tcurr, tnext, 6);
break;
case 4:
t0 = tprev;
t1 = vextq_u16(tprev, tcurr, 4);
t2 = tcurr;
t3 = vextq_u16(tcurr, tnext, 4);
t4 = tnext;
break;
default:
internal::assertSupportedConfiguration(false);//Unsupported channels number
return;
}
t0 = vqaddq_u16(vqaddq_u16(vqaddq_u16(t0, t1), vqaddq_u16(t2, t3)), t4);
#ifdef FLOAT_VARIANT_1_25
uint32x4_t tres1 = vmovl_u16(vget_low_u16(t0));
uint32x4_t tres2 = vmovl_u16(vget_high_u16(t0));
float32x4_t vf1 = vmulq_f32(v1_25, vcvtq_f32_u32(tres1));
float32x4_t vf2 = vmulq_f32(v1_25, vcvtq_f32_u32(tres2));
tres1 = internal::vroundq_u32_f32(vf1);
tres2 = internal::vroundq_u32_f32(vf2);
t0 = vcombine_u16(vmovn_u32(tres1),vmovn_u32(tres2));
vst1_u8(drow + x - 8, vmovn_u16(t0));
#else
int16x8_t tt0 = vqrdmulhq_s16(vreinterpretq_s16_u16(t0), vScale);
uint8x8_t it0 = vmovn_u16(vreinterpretq_u16_s16(tt0));
vst1_u8(drow + x - 8, it0);
#endif
}
x -= 8;
if(x == colsn){
x -= cn;
}
s16 pprevx[4], prevx[4], rowx[4], nextx[4], nnextx[4];
ptrdiff_t px = x / cn;
for( s32 k = 0; k < cn; k++ )
{
ptrdiff_t ploc;
ploc = internal::borderInterpolate(px-2, size.width, borderType);
pprevx[k] = ploc < 0 ? 5*borderValue :
srow4[ploc*cn+k] + srow3[ploc*cn+k] + srow2[ploc*cn+k] + srow1[ploc*cn+k] + srow0[ploc*cn+k];
ploc = internal::borderInterpolate(px-1, size.width, borderType);
prevx[k]  = ploc < 0 ? 5*borderValue :
srow4[ploc*cn+k] + srow3[ploc*cn+k] + srow2[ploc*cn+k] + srow1[ploc*cn+k] + srow0[ploc*cn+k];
rowx[k]   = srow4[px*cn+k] + srow3[px*cn+k] + srow2[px*cn+k] + srow1[px*cn+k] + srow0[px*cn+k];
ploc = internal::borderInterpolate(px+1, size.width, borderType);
nextx[k]  = ploc < 0 ? 5*borderValue :
srow4[ploc*cn+k] + srow3[ploc*cn+k] + srow2[ploc*cn+k] + srow1[ploc*cn+k] + srow0[ploc*cn+k];
}
x = px*cn;
for( ; x < colsn; x+=cn, px++ )
{
for( s32 k = 0; k < cn; k++ )
{
ptrdiff_t ploc = internal::borderInterpolate(px+2, size.width, borderType);
nnextx[k] = ploc < 0 ? 5*borderValue :
srow4[ploc*cn+k] + srow3[ploc*cn+k] + srow2[ploc*cn+k] + srow1[ploc*cn+k] + srow0[ploc*cn+k];
*(drow+x+k) = internal::saturate_cast<u8>((pprevx[k] + prevx[k] + rowx[k] + nextx[k] +nnextx[k])*(1/25.));
pprevx[k] = prevx[k];
prevx[k]  = rowx[k];
rowx[k]   = nextx[k];
nextx[k]  = nnextx[k];
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
#endif
}
bool isBlurF32Supported(const Size2D &size, s32 cn, BORDER_MODE border)
{
return isSupportedConfiguration() &&
cn > 0 && cn <= 4 &&
size.width*cn >= 4 && size.height >= 2 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REFLECT101 ||
border == BORDER_MODE_REFLECT ||
border == BORDER_MODE_REPLICATE ||
border == BORDER_MODE_WRAP);
}
void blur3x3(const Size2D &size, s32 cn,
const f32 * srcBase, ptrdiff_t srcStride,
f32 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, f32 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isBlurF32Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<f32> _tmp;
f32 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 2*cn, borderValue);
tmp = &_tmp[cn];
}
ptrdiff_t idx_l = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r = internal::borderInterpolate(size.width, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//2-line buffer
std::vector<f32> _buf(4*(cn * (size.width + 2) + 32 / sizeof(f32)));
f32* lanea = internal::alignPtr(&_buf[cn], 32);
f32* laneA = internal::alignPtr(lanea + cn * (size.width + 2), 32);
f32* laneb = internal::alignPtr(laneA + cn * (size.width + 2), 32);
f32* laneB = internal::alignPtr(laneb + cn * (size.width + 2), 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = borderValue;
lanea[colsn+k] = borderValue;
laneA[-cn+k] = borderValue;
laneA[colsn+k] = borderValue;
laneb[-cn+k] = borderValue;
laneb[colsn+k] = borderValue;
laneB[-cn+k] = borderValue;
laneB[colsn+k] = borderValue;
}
size_t i = 0;
f32* dsta = internal::getRowPtr(dstBase, dstStride, 0);
for (; i < size.height-1; i+=2)
{
//vertical convolution
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const f32* ln0 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const f32* ln1 = internal::getRowPtr(srcBase, srcStride, i);
const f32* ln2 = internal::getRowPtr(srcBase, srcStride, i + 1);
const f32* ln3 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(ln1 + x);
internal::prefetch(ln2 + x);
internal::prefetch(ln0 + x);
internal::prefetch(ln3 + x);
box3x3f32_vert:
float32x4_t v1 = vld1q_f32(ln1 + x);
float32x4_t v2 = vld1q_f32(ln2 + x);
float32x4_t v0 = vld1q_f32(ln0 + x);
float32x4_t v3 = vld1q_f32(ln3 + x);
float32x4_t v = vaddq_f32(v1, v2);
float32x4_t w0 = vaddq_f32(v, v0);
float32x4_t w1 = vaddq_f32(v, v3);
vst1q_f32(lanea + x, w0);
vst1q_f32(laneb + x, w1);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3f32_vert;
}
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = lanea[idx_l + k];
lanea[colsn+k] = lanea[idx_r + k];
laneb[-cn+k] = laneb[idx_l + k];
laneb[colsn+k] = laneb[idx_r + k];
}
//horizontal convolution (2 lines from previous iteration)
if (i > 0)
{
f32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
box3x3f32_horiz:
float32x4_t lane0a = vld1q_f32(laneA + x - cn);
float32x4_t lane2a = vld1q_f32(laneA + x + cn);
float32x4_t lane1a = vld1q_f32(laneA + x);
float32x4_t lane0b = vld1q_f32(laneB + x - cn);
float32x4_t lane2b = vld1q_f32(laneB + x + cn);
float32x4_t lane1b = vld1q_f32(laneB + x);
float32x4_t va = vaddq_f32(lane0a, lane2a);
float32x4_t vb = vaddq_f32(lane0b, lane2b);
float32x4_t wa = vaddq_f32(va, lane1a);
float32x4_t wb = vaddq_f32(vb, lane1b);
vst1q_f32(dsta + x, wa);
vst1q_f32(dstb + x, wb);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3f32_horiz;
}
dsta = internal::getRowPtr(dstBase, dstStride, i);
}
std::swap(lanea, laneA);
std::swap(laneb, laneB);
}
//last line
if(i < size.height)
{
//vertical convolution
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
const f32* ln0 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const f32* ln1 = internal::getRowPtr(srcBase, srcStride, i);
const f32* ln2 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(ln0 + x);
internal::prefetch(ln1 + x);
internal::prefetch(ln2 + x);
box3x3f32_vert_ll:
float32x4_t v0 = vld1q_f32(ln0+x);
float32x4_t v1 = vld1q_f32(ln1+x);
float32x4_t v2 = vld1q_f32(ln2+x);
float32x4_t v = vaddq_f32(v0, v1);
float32x4_t w = vaddq_f32(v, v2);
vst1q_f32(lanea + x, w);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3f32_vert_ll;
}
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = lanea[idx_l + k];
lanea[colsn+k] = lanea[idx_r + k];
}
//horizontal convolution (last 3 lines)
x = 0;
f32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
f32* dstc = internal::getRowPtr(dstBase, dstStride, i);
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
internal::prefetch(lanea + x + cn);
box3x3f32_horiz_ll:
float32x4_t lane0a = vld1q_f32(laneA + x - cn);
float32x4_t lane2a = vld1q_f32(laneA + x + cn);
float32x4_t lane1a = vld1q_f32(laneA + x);
float32x4_t lane0b = vld1q_f32(laneB + x - cn);
float32x4_t lane2b = vld1q_f32(laneB + x + cn);
float32x4_t lane1b = vld1q_f32(laneB + x);
float32x4_t lane0c = vld1q_f32(lanea + x - cn);
float32x4_t lane2c = vld1q_f32(lanea + x + cn);
float32x4_t lane1c = vld1q_f32(lanea + x);
float32x4_t va = vaddq_f32(lane0a, lane2a);
float32x4_t vb = vaddq_f32(lane0b, lane2b);
float32x4_t vc = vaddq_f32(lane0c, lane2c);
float32x4_t wa = vaddq_f32(va, lane1a);
float32x4_t wb = vaddq_f32(vb, lane1b);
float32x4_t wc = vaddq_f32(vc, lane1c);
vst1q_f32(dsta + x, wa);
vst1q_f32(dstb + x, wb);
vst1q_f32(dstc + x, wc);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3f32_horiz_ll;
}
}
else
{
//horizontal convolution (last 2 lines)
f32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
box3x3f32_horiz_last2:
float32x4_t lane0a = vld1q_f32(laneA + x - cn);
float32x4_t lane2a = vld1q_f32(laneA + x + cn);
float32x4_t lane1a = vld1q_f32(laneA + x);
float32x4_t lane0b = vld1q_f32(laneB + x - cn);
float32x4_t lane2b = vld1q_f32(laneB + x + cn);
float32x4_t lane1b = vld1q_f32(laneB + x);
float32x4_t va = vaddq_f32(lane0a, lane2a);
float32x4_t vb = vaddq_f32(lane0b, lane2b);
float32x4_t wa = vaddq_f32(va, lane1a);
float32x4_t wb = vaddq_f32(vb, lane1b);
vst1q_f32(dsta + x, wa);
vst1q_f32(dstb + x, wb);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3f32_horiz_last2;
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
bool isBlurS32Supported(const Size2D &size, s32 cn, BORDER_MODE border)
{
return isSupportedConfiguration() &&
cn > 0 && cn <= 4 &&
size.width*cn >= 4 && size.height >= 2 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REFLECT101 ||
border == BORDER_MODE_REFLECT ||
border == BORDER_MODE_REPLICATE ||
border == BORDER_MODE_WRAP);
}
void blur3x3(const Size2D &size, s32 cn,
const s32 * srcBase, ptrdiff_t srcStride,
s32 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, s32 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isBlurS32Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<s32> _tmp;
s32 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 2*cn, borderValue);
tmp = &_tmp[cn];
}
ptrdiff_t idx_l = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r = internal::borderInterpolate(size.width, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//2-line buffer
std::vector<s32> _buf(4*(cn * (size.width + 2) + 32 / sizeof(s32)));
s32* lanea = internal::alignPtr(&_buf[cn], 32);
s32* laneA = internal::alignPtr(lanea + cn * (size.width + 2), 32);
s32* laneb = internal::alignPtr(laneA + cn * (size.width + 2), 32);
s32* laneB = internal::alignPtr(laneb + cn * (size.width + 2), 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = borderValue;
lanea[colsn+k] = borderValue;
laneA[-cn+k] = borderValue;
laneA[colsn+k] = borderValue;
laneb[-cn+k] = borderValue;
laneb[colsn+k] = borderValue;
laneB[-cn+k] = borderValue;
laneB[colsn+k] = borderValue;
}
size_t i = 0;
s32* dsta = internal::getRowPtr(dstBase, dstStride, 0);
for (; i < size.height-1; i+=2)
{
//vertical convolution
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const s32* ln0 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const s32* ln1 = internal::getRowPtr(srcBase, srcStride, i);
const s32* ln2 = internal::getRowPtr(srcBase, srcStride, i + 1);
const s32* ln3 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(ln1 + x);
internal::prefetch(ln2 + x);
internal::prefetch(ln0 + x);
internal::prefetch(ln3 + x);
box3x3s32_vert:
int32x4_t v1 = vld1q_s32(ln1 + x);
int32x4_t v2 = vld1q_s32(ln2 + x);
int32x4_t v0 = vld1q_s32(ln0 + x);
int32x4_t v3 = vld1q_s32(ln3 + x);
int32x4_t v = vaddq_s32(v1, v2);
int32x4_t w0 = vaddq_s32(v, v0);
int32x4_t w1 = vaddq_s32(v, v3);
vst1q_s32(lanea + x, w0);
vst1q_s32(laneb + x, w1);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3s32_vert;
}
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = lanea[idx_l + k];
lanea[colsn+k] = lanea[idx_r + k];
laneb[-cn+k] = laneb[idx_l + k];
laneb[colsn+k] = laneb[idx_r + k];
}
//horizontal convolution (2 lines from previous iteration)
if (i > 0)
{
s32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
box3x3s32_horiz:
int32x4_t lane0a = vld1q_s32(laneA + x - cn);
int32x4_t lane2a = vld1q_s32(laneA + x + cn);
int32x4_t lane1a = vld1q_s32(laneA + x);
int32x4_t lane0b = vld1q_s32(laneB + x - cn);
int32x4_t lane2b = vld1q_s32(laneB + x + cn);
int32x4_t lane1b = vld1q_s32(laneB + x);
int32x4_t va = vaddq_s32(lane0a, lane2a);
int32x4_t vb = vaddq_s32(lane0b, lane2b);
int32x4_t wa = vaddq_s32(va, lane1a);
int32x4_t wb = vaddq_s32(vb, lane1b);
vst1q_s32(dsta + x, wa);
vst1q_s32(dstb + x, wb);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3s32_horiz;
}
dsta = internal::getRowPtr(dstBase, dstStride, i);
}
std::swap(lanea, laneA);
std::swap(laneb, laneB);
}
//last line
if(i < size.height)
{
//vertical convolution
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
const s32* ln0 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const s32* ln1 = internal::getRowPtr(srcBase, srcStride, i);
const s32* ln2 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(ln0 + x);
internal::prefetch(ln1 + x);
internal::prefetch(ln2 + x);
box3x3s32_vert_ll:
int32x4_t v0 = vld1q_s32(ln0+x);
int32x4_t v1 = vld1q_s32(ln1+x);
int32x4_t v2 = vld1q_s32(ln2+x);
int32x4_t v = vaddq_s32(v0, v1);
int32x4_t w = vaddq_s32(v, v2);
vst1q_s32(lanea + x, w);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3s32_vert_ll;
}
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lanea[-cn+k] = lanea[idx_l + k];
lanea[colsn+k] = lanea[idx_r + k];
}
//horizontal convolution (last 3 lines)
x = 0;
s32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
s32* dstc = internal::getRowPtr(dstBase, dstStride, i);
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
internal::prefetch(lanea + x + cn);
box3x3s32_horiz_ll:
int32x4_t lane0a = vld1q_s32(laneA + x - cn);
int32x4_t lane2a = vld1q_s32(laneA + x + cn);
int32x4_t lane1a = vld1q_s32(laneA + x);
int32x4_t lane0b = vld1q_s32(laneB + x - cn);
int32x4_t lane2b = vld1q_s32(laneB + x + cn);
int32x4_t lane1b = vld1q_s32(laneB + x);
int32x4_t lane0c = vld1q_s32(lanea + x - cn);
int32x4_t lane2c = vld1q_s32(lanea + x + cn);
int32x4_t lane1c = vld1q_s32(lanea + x);
int32x4_t va = vaddq_s32(lane0a, lane2a);
int32x4_t vb = vaddq_s32(lane0b, lane2b);
int32x4_t vc = vaddq_s32(lane0c, lane2c);
int32x4_t wa = vaddq_s32(va, lane1a);
int32x4_t wb = vaddq_s32(vb, lane1b);
int32x4_t wc = vaddq_s32(vc, lane1c);
vst1q_s32(dsta + x, wa);
vst1q_s32(dstb + x, wb);
vst1q_s32(dstc + x, wc);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3s32_horiz_ll;
}
}
else
{
//horizontal convolution (last 2 lines)
s32* dstb = internal::getRowPtr(dstBase, dstStride, i-1);
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(laneA + x + cn);
internal::prefetch(laneB + x + cn);
box3x3s32_horiz_last2:
int32x4_t lane0a = vld1q_s32(laneA + x - cn);
int32x4_t lane2a = vld1q_s32(laneA + x + cn);
int32x4_t lane1a = vld1q_s32(laneA + x);
int32x4_t lane0b = vld1q_s32(laneB + x - cn);
int32x4_t lane2b = vld1q_s32(laneB + x + cn);
int32x4_t lane1b = vld1q_s32(laneB + x);
int32x4_t va = vaddq_s32(lane0a, lane2a);
int32x4_t vb = vaddq_s32(lane0b, lane2b);
int32x4_t wa = vaddq_s32(va, lane1a);
int32x4_t wb = vaddq_s32(vb, lane1b);
vst1q_s32(dsta + x, wa);
vst1q_s32(dstb + x, wb);
}
if(x < colsn)
{
x = colsn-4;
goto box3x3s32_horiz_last2;
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
} //namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "saturate_cast.hpp"
#include <vector>
#include <cstring>
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
struct RowFilter3x3Canny
{
inline RowFilter3x3Canny(const ptrdiff_t borderxl, const ptrdiff_t borderxr)
{
vfmask = vreinterpret_u8_u64(vmov_n_u64(borderxl ? 0x0000FFffFFffFFffULL : 0x0100FFffFFffFFffULL));
vtmask = vreinterpret_u8_u64(vmov_n_u64(borderxr ? 0x0707060504030201ULL : 0x0706050403020100ULL));
lookLeft = offsetk - borderxl;
lookRight = offsetk - borderxr;
}
inline void operator()(const u8* src, s16* dstx, s16* dsty, ptrdiff_t width)
{
uint8x8_t l = vtbl1_u8(vld1_u8(src - lookLeft), vfmask);
ptrdiff_t i = 0;
for (; i < width - 8 + lookRight; i += 8)
{
internal::prefetch(src + i);
uint8x8_t l18u = vld1_u8(src + i + 1);
uint8x8_t l2 = l18u;
uint8x8_t l0 = vext_u8(l, l18u, 6);
int16x8_t l1x2 = vreinterpretq_s16_u16(vshll_n_u8(vext_u8(l, l18u, 7), 1));
l = l18u;
int16x8_t l02 = vreinterpretq_s16_u16(vaddl_u8(l2, l0));
int16x8_t ldx = vreinterpretq_s16_u16(vsubl_u8(l2, l0));
int16x8_t ldy = vaddq_s16(l02, l1x2);
vst1q_s16(dstx + i, ldx);
vst1q_s16(dsty + i, ldy);
}
//tail
if (lookRight == 0 || i != width)
{
uint8x8_t tail0 = vld1_u8(src + (width - 9));//can't get left 1 pixel another way if width==8*k+1
uint8x8_t tail2 = vtbl1_u8(vld1_u8(src + (width - 8 + lookRight)), vtmask);
uint8x8_t tail1 = vext_u8(vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(tail0), 8*6)), tail2, 7);
int16x8_t tail02 = vreinterpretq_s16_u16(vaddl_u8(tail2, tail0));
int16x8_t tail1x2 = vreinterpretq_s16_u16(vshll_n_u8(tail1, 1));
int16x8_t taildx = vreinterpretq_s16_u16(vsubl_u8(tail2, tail0));
int16x8_t taildy = vqaddq_s16(tail02, tail1x2);
vst1q_s16(dstx + (width - 8), taildx);
vst1q_s16(dsty + (width - 8), taildy);
}
}
uint8x8_t vfmask;
uint8x8_t vtmask;
enum { offsetk = 1};
ptrdiff_t lookLeft;
ptrdiff_t lookRight;
};
template <bool L2gradient>
inline void ColFilter3x3Canny(const s16* src0, const s16* src1, const s16* src2, s16* dstx, s16* dsty, s32* mag, ptrdiff_t width)
{
ptrdiff_t j = 0;
for (; j <= width - 8; j += 8)
{
ColFilter3x3CannyL1Loop:
int16x8_t line0x = vld1q_s16(src0 + j);
int16x8_t line1x = vld1q_s16(src1 + j);
int16x8_t line2x = vld1q_s16(src2 + j);
int16x8_t line0y = vld1q_s16(src0 + j + width);
int16x8_t line2y = vld1q_s16(src2 + j + width);
int16x8_t l02 = vaddq_s16(line0x, line2x);
int16x8_t l1x2 = vshlq_n_s16(line1x, 1);
int16x8_t dy = vsubq_s16(line2y, line0y);
int16x8_t dx = vaddq_s16(l1x2, l02);
int16x8_t dya = vabsq_s16(dy);
int16x8_t dxa = vabsq_s16(dx);
int16x8_t norm = vaddq_s16(dya, dxa);
int32x4_t normh = vmovl_s16(vget_high_s16(norm));
int32x4_t norml = vmovl_s16(vget_low_s16(norm));
vst1q_s16(dsty + j, dy);
vst1q_s16(dstx + j, dx);
vst1q_s32(mag + j + 4, normh);
vst1q_s32(mag + j, norml);
}
if (j != width)
{
j = width - 8;
goto ColFilter3x3CannyL1Loop;
}
}
template <>
inline void ColFilter3x3Canny<true>(const s16* src0, const s16* src1, const s16* src2, s16* dstx, s16* dsty, s32* mag, ptrdiff_t width)
{
ptrdiff_t j = 0;
for (; j <= width - 8; j += 8)
{
ColFilter3x3CannyL2Loop:
int16x8_t line0x = vld1q_s16(src0 + j);
int16x8_t line1x = vld1q_s16(src1 + j);
int16x8_t line2x = vld1q_s16(src2 + j);
int16x8_t line0y = vld1q_s16(src0 + j + width);
int16x8_t line2y = vld1q_s16(src2 + j + width);
int16x8_t l02 = vaddq_s16(line0x, line2x);
int16x8_t l1x2 = vshlq_n_s16(line1x, 1);
int16x8_t dy = vsubq_s16(line2y, line0y);
int16x8_t dx = vaddq_s16(l1x2, l02);
int32x4_t norml = vmull_s16(vget_low_s16(dx), vget_low_s16(dx));
int32x4_t normh = vmull_s16(vget_high_s16(dy), vget_high_s16(dy));
norml = vmlal_s16(norml, vget_low_s16(dy), vget_low_s16(dy));
normh = vmlal_s16(normh, vget_high_s16(dx), vget_high_s16(dx));
vst1q_s16(dsty + j, dy);
vst1q_s16(dstx + j, dx);
vst1q_s32(mag + j, norml);
vst1q_s32(mag + j + 4, normh);
}
if (j != width)
{
j = width - 8;
goto ColFilter3x3CannyL2Loop;
}
}
template <bool L2gradient>
inline void NormCanny(const ptrdiff_t colscn, s16* _dx, s16* _dy, s32* _norm)
{
ptrdiff_t j = 0;
if (colscn >= 8)
{
int16x8_t vx = vld1q_s16(_dx);
int16x8_t vy = vld1q_s16(_dy);
for (; j <= colscn - 16; j+=8)
{
internal::prefetch(_dx);
internal::prefetch(_dy);
int16x8_t vx2 = vld1q_s16(_dx + j + 8);
int16x8_t vy2 = vld1q_s16(_dy + j + 8);
int16x8_t vabsx = vabsq_s16(vx);
int16x8_t vabsy = vabsq_s16(vy);
int16x8_t norm = vaddq_s16(vabsx, vabsy);
int32x4_t normh = vmovl_s16(vget_high_s16(norm));
int32x4_t norml = vmovl_s16(vget_low_s16(norm));
vst1q_s32(_norm + j + 4, normh);
vst1q_s32(_norm + j + 0, norml);
vx = vx2;
vy = vy2;
}
int16x8_t vabsx = vabsq_s16(vx);
int16x8_t vabsy = vabsq_s16(vy);
int16x8_t norm = vaddq_s16(vabsx, vabsy);
int32x4_t normh = vmovl_s16(vget_high_s16(norm));
int32x4_t norml = vmovl_s16(vget_low_s16(norm));
vst1q_s32(_norm + j + 4, normh);
vst1q_s32(_norm + j + 0, norml);
}
for (; j < colscn; j++)
_norm[j] = std::abs(s32(_dx[j])) + std::abs(s32(_dy[j]));
}
template <>
inline void NormCanny<true>(const ptrdiff_t colscn, s16* _dx, s16* _dy, s32* _norm)
{
ptrdiff_t j = 0;
if (colscn >= 8)
{
int16x8_t vx = vld1q_s16(_dx);
int16x8_t vy = vld1q_s16(_dy);
for (; j <= colscn - 16; j+=8)
{
internal::prefetch(_dx);
internal::prefetch(_dy);
int16x8_t vxnext = vld1q_s16(_dx + j + 8);
int16x8_t vynext = vld1q_s16(_dy + j + 8);
int32x4_t norml = vmull_s16(vget_low_s16(vx), vget_low_s16(vx));
int32x4_t normh = vmull_s16(vget_high_s16(vy), vget_high_s16(vy));
norml = vmlal_s16(norml, vget_low_s16(vy), vget_low_s16(vy));
normh = vmlal_s16(normh, vget_high_s16(vx), vget_high_s16(vx));
vst1q_s32(_norm + j + 0, norml);
vst1q_s32(_norm + j + 4, normh);
vx = vxnext;
vy = vynext;
}
int32x4_t norml = vmull_s16(vget_low_s16(vx), vget_low_s16(vx));
int32x4_t normh = vmull_s16(vget_high_s16(vy), vget_high_s16(vy));
norml = vmlal_s16(norml, vget_low_s16(vy), vget_low_s16(vy));
normh = vmlal_s16(normh, vget_high_s16(vx), vget_high_s16(vx));
vst1q_s32(_norm + j + 0, norml);
vst1q_s32(_norm + j + 4, normh);
}
for (; j < colscn; j++)
_norm[j] = s32(_dx[j])*_dx[j] + s32(_dy[j])*_dy[j];
}
template <bool L2gradient>
inline void prepareThresh(f64 low_thresh, f64 high_thresh,
s32 &low, s32 &high)
{
if (low_thresh > high_thresh)
std::swap(low_thresh, high_thresh);
#if defined __GNUC__
low = (s32)low_thresh;
high = (s32)high_thresh;
low -= (low > low_thresh);
high -= (high > high_thresh);
#else
low = internal::round(low_thresh);
high = internal::round(high_thresh);
f32 ldiff = (f32)(low_thresh - low);
f32 hdiff = (f32)(high_thresh - high);
low -= (ldiff < 0);
high -= (hdiff < 0);
#endif
}
template <>
inline void prepareThresh<true>(f64 low_thresh, f64 high_thresh,
s32 &low, s32 &high)
{
if (low_thresh > high_thresh)
std::swap(low_thresh, high_thresh);
if (low_thresh > 0) low_thresh *= low_thresh;
if (high_thresh > 0) high_thresh *= high_thresh;
#if defined __GNUC__
low = (s32)low_thresh;
high = (s32)high_thresh;
low -= (low > low_thresh);
high -= (high > high_thresh);
#else
low = internal::round(low_thresh);
high = internal::round(high_thresh);
f32 ldiff = (f32)(low_thresh - low);
f32 hdiff = (f32)(high_thresh - high);
low -= (ldiff < 0);
high -= (hdiff < 0);
#endif
}
template <bool L2gradient, bool externalSobel>
struct _normEstimator
{
ptrdiff_t magstep;
ptrdiff_t dxOffset;
ptrdiff_t dyOffset;
ptrdiff_t shxOffset;
ptrdiff_t shyOffset;
std::vector<u8> buffer;
const ptrdiff_t offsetk;
ptrdiff_t borderyt, borderyb;
RowFilter3x3Canny sobelRow;
inline _normEstimator(const Size2D &size, s32, Margin borderMargin,
ptrdiff_t &mapstep, s32** mag_buf, u8* &map):
offsetk(1),
sobelRow(std::max<ptrdiff_t>(0, offsetk - (ptrdiff_t)borderMargin.left),
std::max<ptrdiff_t>(0, offsetk - (ptrdiff_t)borderMargin.right))
{
mapstep = size.width + 2;
magstep = size.width + 2 + size.width * (4 * sizeof(s16)/sizeof(s32));
dxOffset = mapstep * sizeof(s32)/sizeof(s16);
dyOffset = dxOffset + size.width * 1;
shxOffset = dxOffset + size.width * 2;
shyOffset = dxOffset + size.width * 3;
buffer.resize( (size.width+2)*(size.height+2) + magstep*3*sizeof(s32) );
mag_buf[0] = (s32*)&buffer[0];
mag_buf[1] = mag_buf[0] + magstep;
mag_buf[2] = mag_buf[1] + magstep;
memset(mag_buf[0], 0, mapstep * sizeof(s32));
map = (u8*)(mag_buf[2] + magstep);
memset(map, 1, mapstep);
memset(map + mapstep*(size.height + 1), 1, mapstep);
borderyt = std::max<ptrdiff_t>(0, offsetk - (ptrdiff_t)borderMargin.top);
borderyb = std::max<ptrdiff_t>(0, offsetk - (ptrdiff_t)borderMargin.bottom);
}
inline void firstRow(const Size2D &size, s32,
const u8 *srcBase, ptrdiff_t srcStride,
s16*, ptrdiff_t,
s16*, ptrdiff_t,
s32** mag_buf)
{
//sobelH row #0
const u8* _src = internal::getRowPtr(srcBase, srcStride, 0);
sobelRow(_src, ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[0]) + shyOffset, size.width);
//sobelH row #1
_src = internal::getRowPtr(srcBase, srcStride, 1);
sobelRow(_src, ((s16*)mag_buf[1]) + shxOffset, ((s16*)mag_buf[1]) + shyOffset, size.width);
mag_buf[1][0] = mag_buf[1][size.width+1] = 0;
if (borderyt == 0)
{
//sobelH row #-1
_src = internal::getRowPtr(srcBase, srcStride, -1);
sobelRow(_src, ((s16*)mag_buf[2]) + shxOffset, ((s16*)mag_buf[2]) + shyOffset, size.width);
ColFilter3x3Canny<L2gradient>( ((s16*)mag_buf[2]) + shxOffset, ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[1]) + shxOffset,
((s16*)mag_buf[1]) + dxOffset,  ((s16*)mag_buf[1]) + dyOffset, mag_buf[1] + 1, size.width);
}
else
{
ColFilter3x3Canny<L2gradient>( ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[1]) + shxOffset,
((s16*)mag_buf[1]) + dxOffset,  ((s16*)mag_buf[1]) + dyOffset, mag_buf[1] + 1, size.width);
}
}
inline void nextRow(const Size2D &size, s32,
const u8 *srcBase, ptrdiff_t srcStride,
s16*, ptrdiff_t,
s16*, ptrdiff_t,
const ptrdiff_t &mapstep, s32** mag_buf,
size_t i, const s16* &_x, const s16* &_y)
{
mag_buf[2][0] = mag_buf[2][size.width+1] = 0;
if (i < size.height - borderyb)
{
const u8* _src = internal::getRowPtr(srcBase, srcStride, i+1);
//sobelH row #i+1
sobelRow(_src, ((s16*)mag_buf[2]) + shxOffset, ((s16*)mag_buf[2]) + shyOffset, size.width);
ColFilter3x3Canny<L2gradient>( ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[1]) + shxOffset, ((s16*)mag_buf[2]) + shxOffset,
((s16*)mag_buf[2]) + dxOffset,  ((s16*)mag_buf[2]) + dyOffset, mag_buf[2] + 1, size.width);
}
else if (i < size.height)
{
ColFilter3x3Canny<L2gradient>( ((s16*)mag_buf[0]) + shxOffset, ((s16*)mag_buf[1]) + shxOffset, ((s16*)mag_buf[1]) + shxOffset,
((s16*)mag_buf[2]) + dxOffset,  ((s16*)mag_buf[2]) + dyOffset, mag_buf[2] + 1, size.width);
}
else
memset(mag_buf[2], 0, mapstep*sizeof(s32));
_x = ((s16*)mag_buf[1]) + dxOffset;
_y = ((s16*)mag_buf[1]) + dyOffset;
}
};
template <bool L2gradient>
struct _normEstimator<L2gradient, true>
{
std::vector<u8> buffer;
inline _normEstimator(const Size2D &size, s32 cn, Margin,
ptrdiff_t &mapstep, s32** mag_buf, u8* &map)
{
mapstep = size.width + 2;
buffer.resize( (size.width+2)*(size.height+2) + cn*mapstep*3*sizeof(s32) );
mag_buf[0] = (s32*)&buffer[0];
mag_buf[1] = mag_buf[0] + mapstep*cn;
mag_buf[2] = mag_buf[1] + mapstep*cn;
memset(mag_buf[0], 0, /* cn* */mapstep * sizeof(s32));
map = (u8*)(mag_buf[2] + mapstep*cn);
memset(map, 1, mapstep);
memset(map + mapstep*(size.height + 1), 1, mapstep);
}
inline void firstRow(const Size2D &size, s32 cn,
const u8 *, ptrdiff_t,
s16* dxBase, ptrdiff_t dxStride,
s16* dyBase, ptrdiff_t dyStride,
s32** mag_buf)
{
s32* _norm = mag_buf[1] + 1;
s16* _dx = internal::getRowPtr(dxBase, dxStride, 0);
s16* _dy = internal::getRowPtr(dyBase, dyStride, 0);
NormCanny<L2gradient>(size.width*cn, _dx, _dy, _norm);
if(cn > 1)
{
for(size_t j = 0, jn = 0; j < size.width; ++j, jn += cn)
{
size_t maxIdx = jn;
for(s32 k = 1; k < cn; ++k)
if(_norm[jn + k] > _norm[maxIdx]) maxIdx = jn + k;
_norm[j] = _norm[maxIdx];
_dx[j] = _dx[maxIdx];
_dy[j] = _dy[maxIdx];
}
}
_norm[-1] = _norm[size.width] = 0;
}
inline void nextRow(const Size2D &size, s32 cn,
const u8 *, ptrdiff_t,
s16* dxBase, ptrdiff_t dxStride,
s16* dyBase, ptrdiff_t dyStride,
const ptrdiff_t &mapstep, s32** mag_buf,
size_t i, const s16* &_x, const s16* &_y)
{
s32* _norm = mag_buf[(i > 0) + 1] + 1;
if (i < size.height)
{
s16* _dx = internal::getRowPtr(dxBase, dxStride, i);
s16* _dy = internal::getRowPtr(dyBase, dyStride, i);
NormCanny<L2gradient>(size.width*cn, _dx, _dy, _norm);
if(cn > 1)
{
for(size_t j = 0, jn = 0; j < size.width; ++j, jn += cn)
{
size_t maxIdx = jn;
for(s32 k = 1; k < cn; ++k)
if(_norm[jn + k] > _norm[maxIdx]) maxIdx = jn + k;
_norm[j] = _norm[maxIdx];
_dx[j] = _dx[maxIdx];
_dy[j] = _dy[maxIdx];
}
}
_norm[-1] = _norm[size.width] = 0;
}
else
memset(_norm-1, 0, /* cn* */mapstep*sizeof(s32));
_x = internal::getRowPtr(dxBase, dxStride, i-1);
_y = internal::getRowPtr(dyBase, dyStride, i-1);
}
};
template <bool L2gradient, bool externalSobel>
inline void Canny3x3(const Size2D &size, s32 cn,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
s16 * dxBase, ptrdiff_t dxStride,
s16 * dyBase, ptrdiff_t dyStride,
f64 low_thresh, f64 high_thresh,
Margin borderMargin)
{
s32 low, high;
prepareThresh<L2gradient>(low_thresh, high_thresh, low, high);
ptrdiff_t mapstep;
s32* mag_buf[3];
u8* map;
_normEstimator<L2gradient, externalSobel> normEstimator(size, cn, borderMargin, mapstep, mag_buf, map);
size_t maxsize = std::max<size_t>( 1u << 10, size.width * size.height / 10 );
std::vector<u8*> stack( maxsize );
u8 **stack_top = &stack[0];
u8 **stack_bottom = &stack[0];
/* sector numbers
(Top-Left Origin)
1   2   3
*  *  *
* * *
0*******0
* * *
*  *  *
3   2   1
*/
#define CANNY_PUSH(d)    *(d) = u8(2), *stack_top++ = (d)
#define CANNY_POP(d)     (d) = *--stack_top
//i == 0
normEstimator.firstRow(size, cn, srcBase, srcStride, dxBase, dxStride, dyBase, dyStride, mag_buf);
// calculate magnitude and angle of gradient, perform non-maxima supression.
// fill the map with one of the following values:
//   0 - the pixel might belong to an edge
//   1 - the pixel can not belong to an edge
//   2 - the pixel does belong to an edge
for (size_t i = 1; i <= size.height; i++)
{
const s16 *_x, *_y;
normEstimator.nextRow(size, cn, srcBase, srcStride, dxBase, dxStride, dyBase, dyStride, mapstep, mag_buf, i, _x, _y);
u8* _map = map + mapstep*i + 1;
_map[-1] = _map[size.width] = 1;
s32* _mag = mag_buf[1] + 1; // take the central row
ptrdiff_t magstep1 = mag_buf[2] - mag_buf[1];
ptrdiff_t magstep2 = mag_buf[0] - mag_buf[1];
if ((stack_top - stack_bottom) + size.width > maxsize)
{
ptrdiff_t sz = (ptrdiff_t)(stack_top - stack_bottom);
maxsize = maxsize * 3/2;
stack.resize(maxsize);
stack_bottom = &stack[0];
stack_top = stack_bottom + sz;
}
s32 prev_flag = 0;
for (ptrdiff_t j = 0; j < (ptrdiff_t)size.width; j++)
{
#define CANNY_SHIFT 15
const s32 TG22 = (s32)(0.4142135623730950488016887242097*(1<<CANNY_SHIFT) + 0.5);
s32 m = _mag[j];
if (m > low)
{
s32 xs = _x[j];
s32 ys = _y[j];
s32 x = abs(xs);
s32 y = abs(ys) << CANNY_SHIFT;
s32 tg22x = x * TG22;
if (y < tg22x)
{
if (m > _mag[j-1] && m >= _mag[j+1]) goto __push;
}
else
{
s32 tg67x = tg22x + (x << (CANNY_SHIFT+1));
if (y > tg67x)
{
if (m > _mag[j+magstep2] && m >= _mag[j+magstep1]) goto __push;
}
else
{
s32 s = (xs ^ ys) < 0 ? -1 : 1;
if(m > _mag[j+magstep2-s] && m > _mag[j+magstep1+s]) goto __push;
}
}
}
prev_flag = 0;
_map[j] = u8(1);
continue;
__push:
if (!prev_flag && m > high && _map[j-mapstep] != 2)
{
CANNY_PUSH(_map + j);
prev_flag = 1;
}
else
_map[j] = 0;
}
// scroll the ring buffer
_mag = mag_buf[0];
mag_buf[0] = mag_buf[1];
mag_buf[1] = mag_buf[2];
mag_buf[2] = _mag;
}
// now track the edges (hysteresis thresholding)
while (stack_top > stack_bottom)
{
u8* m;
if ((size_t)(stack_top - stack_bottom) + 8u > maxsize)
{
ptrdiff_t sz = (ptrdiff_t)(stack_top - stack_bottom);
maxsize = maxsize * 3/2;
stack.resize(maxsize);
stack_bottom = &stack[0];
stack_top = stack_bottom + sz;
}
CANNY_POP(m);
if (!m[-1])         CANNY_PUSH(m - 1);
if (!m[1])          CANNY_PUSH(m + 1);
if (!m[-mapstep-1]) CANNY_PUSH(m - mapstep - 1);
if (!m[-mapstep])   CANNY_PUSH(m - mapstep);
if (!m[-mapstep+1]) CANNY_PUSH(m - mapstep + 1);
if (!m[mapstep-1])  CANNY_PUSH(m + mapstep - 1);
if (!m[mapstep])    CANNY_PUSH(m + mapstep);
if (!m[mapstep+1])  CANNY_PUSH(m + mapstep + 1);
}
// the final pass, form the final image
uint8x16_t v2 = vmovq_n_u8(2);
const u8* ptrmap = map + mapstep + 1;
for (size_t i = 0; i < size.height; i++, ptrmap += mapstep)
{
u8* _dst = internal::getRowPtr(dstBase, dstStride, i);
ptrdiff_t j = 0;
for (; j < (ptrdiff_t)size.width - 16; j += 16)
{
internal::prefetch(ptrmap);
uint8x16_t vmap = vld1q_u8(ptrmap + j);
uint8x16_t vdst = vceqq_u8(vmap, v2);
vst1q_u8(_dst+j, vdst);
}
for (; j < (ptrdiff_t)size.width; j++)
_dst[j] = (u8)-(ptrmap[j] >> 1);
}
}
} // namespace
#endif
bool isCanny3x3Supported(const Size2D &size)
{
return isSupportedConfiguration() &&
size.height >= 2 && size.width >= 9;
}
void Canny3x3L1(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
f64 low_thresh, f64 high_thresh,
Margin borderMargin)
{
internal::assertSupportedConfiguration(isCanny3x3Supported(size));
#ifdef CAROTENE_NEON
Canny3x3<false, false>(size, 1,
srcBase, srcStride,
dstBase, dstStride,
NULL, 0,
NULL, 0,
low_thresh, high_thresh,
borderMargin);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)low_thresh;
(void)high_thresh;
(void)borderMargin;
#endif
}
void Canny3x3L2(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
f64 low_thresh, f64 high_thresh,
Margin borderMargin)
{
internal::assertSupportedConfiguration(isCanny3x3Supported(size));
#ifdef CAROTENE_NEON
Canny3x3<true, false>(size, 1,
srcBase, srcStride,
dstBase, dstStride,
NULL, 0,
NULL, 0,
low_thresh, high_thresh,
borderMargin);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)low_thresh;
(void)high_thresh;
(void)borderMargin;
#endif
}
void Canny3x3L1(const Size2D &size, s32 cn,
s16 * dxBase, ptrdiff_t dxStride,
s16 * dyBase, ptrdiff_t dyStride,
u8 * dstBase, ptrdiff_t dstStride,
f64 low_thresh, f64 high_thresh)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Canny3x3<false, true>(size, cn,
NULL, 0,
dstBase, dstStride,
dxBase, dxStride,
dyBase, dyStride,
low_thresh, high_thresh,
Margin());
#else
(void)size;
(void)cn;
(void)dstBase;
(void)dstStride;
(void)dxBase;
(void)dxStride;
(void)dyBase;
(void)dyStride;
(void)low_thresh;
(void)high_thresh;
#endif
}
void Canny3x3L2(const Size2D &size, s32 cn,
s16 * dxBase, ptrdiff_t dxStride,
s16 * dyBase, ptrdiff_t dyStride,
u8 * dstBase, ptrdiff_t dstStride,
f64 low_thresh, f64 high_thresh)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Canny3x3<true, true>(size, cn,
NULL, 0,
dstBase, dstStride,
dxBase, dxStride,
dyBase, dyStride,
low_thresh, high_thresh,
Margin());
#else
(void)size;
(void)cn;
(void)dstBase;
(void)dstStride;
(void)dxBase;
(void)dxStride;
(void)dyBase;
(void)dyStride;
(void)low_thresh;
(void)high_thresh;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#define FILL_LINES2(macro,type) \
macro##_LINE(type,0) \
macro##_LINE(type,1)
#define FILL_LINES3(macro,type) \
FILL_LINES2(macro,type) \
macro##_LINE(type,2)
#define FILL_LINES4(macro,type) \
FILL_LINES3(macro,type) \
macro##_LINE(type,3)
#define  FARG_LINE(type, n) , const type * src##n##Base, ptrdiff_t src##n##Stride
#ifdef CAROTENE_NEON
#define  VROW_LINE(type, n) const type * src##n = internal::getRowPtr(src##n##Base, src##n##Stride, i);
#define  PREF_LINE(type, n) internal::prefetch(src##n + sj);
#define VLD1Q_LINE(type, n) v_dst.val[n] = vld1q_##type(src##n + sj);
#define  PRLD_LINE(type, n) internal::prefetch(src##n + sj); v_dst.val[n] = vld1q_##type(src##n + sj);
#define  VLD1_LINE(type, n) v_dst.val[n] = vld1_##type(src##n + sj);
#define   SLD_LINE(type, n) dst[dj + n] = src##n[sj];
#define MUL2(val) (val << 1)
#define MUL3(val) (MUL2(val) + val)
#define MUL4(val) (val << 2)
#define CONTSRC2 dstStride == src0Stride && \
dstStride == src1Stride &&
#define CONTSRC3 dstStride == src0Stride && \
dstStride == src1Stride && \
dstStride == src2Stride &&
#define CONTSRC4 dstStride == src0Stride && \
dstStride == src1Stride && \
dstStride == src2Stride && \
dstStride == src3Stride &&
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
#define MERGE_ASM2(sgn, bits) __asm__ ( \
"vld1." #bits " {d0-d1}, [%[in0]]             \n\t" \
"vld1." #bits " {d2-d3}, [%[in1]]             \n\t" \
"vst2." #bits " {d0, d2}, [%[out0]]           \n\t" \
"vst2." #bits " {d1, d3}, [%[out1]]           \n\t" \
: \
: [in0] "r" (src0 + sj), [in1] "r" (src1 + sj), \
[out0]  "r" (dst + dj), [out1]  "r" (dst + dj + MUL2(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3" \
);
#define MERGE_ASM3(sgn, bits) __asm__ ( \
"vld1." #bits " {d0-d1}, [%[in0]]             \n\t" \
"vld1." #bits " {d2-d3}, [%[in1]]             \n\t" \
"vld1." #bits " {d4-d5}, [%[in2]]             \n\t" \
"vst3." #bits " {d0, d2, d4}, [%[out0]]       \n\t" \
"vst3." #bits " {d1, d3, d5}, [%[out1]]       \n\t" \
: \
: [in0] "r" (src0 + sj), [in1] "r" (src1 + sj), [in2] "r" (src2 + sj), \
[out0]  "r" (dst + dj), [out1]  "r" (dst + dj + MUL3(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3","d4","d5" \
);
#define MERGE_ASM4(sgn, bits) __asm__ ( \
"vld1." #bits " {d0-d1}, [%[in0]]             \n\t" \
"vld1." #bits " {d2-d3}, [%[in1]]             \n\t" \
"vld1." #bits " {d4-d5}, [%[in2]]             \n\t" \
"vld1." #bits " {d6-d7}, [%[in3]]             \n\t" \
"vst4." #bits " {d0, d2, d4, d6}, [%[out0]]   \n\t" \
"vst4." #bits " {d1, d3, d5, d7}, [%[out1]]   \n\t" \
: \
: [in0] "r" (src0 + sj), [in1] "r" (src1 + sj), [in2] "r" (src2 + sj), [in3] "r" (src3 + sj), \
[out0]  "r" (dst + dj), [out1]  "r" (dst + dj + MUL4(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3","d4","d5","d6","d7" \
);
#define MERGE_QUAD(sgn, bits, n) { \
FILL_LINES##n(PREF, sgn##bits) \
MERGE_ASM##n(sgn, bits) \
}
#else
#define MERGE_QUAD(sgn, bits, n) { \
vec128 v_dst; \
/*FILL_LINES##n(PREF, sgn##bits) \
FILL_LINES##n(VLD1Q, sgn##bits)*/ \
FILL_LINES##n(PRLD, sgn##bits) \
vst##n##q_##sgn##bits(dst + dj, v_dst); \
}
#endif
#define COMBINE(sgn,bits,n) void combine##n(const Size2D &_size                                             \
FILL_LINES##n(FARG, sgn##bits),                                     \
sgn##bits * dstBase, ptrdiff_t dstStride)                           \
{                                                                                                           \
internal::assertSupportedConfiguration();                                                               \
Size2D size(_size);                                                                                     \
if (CONTSRC##n                                                                                          \
dstStride == (ptrdiff_t)(size.width))                                                               \
{                                                                                                       \
size.width *= size.height;                                                                          \
size.height = 1;                                                                                    \
}                                                                                                       \
typedef internal::VecTraits<sgn##bits, n>::vec128 vec128;                                               \
size_t roiw16 = size.width >= (16/sizeof(sgn##bits) - 1) ? size.width - (16/sizeof(sgn##bits) - 1) : 0; \
typedef internal::VecTraits<sgn##bits, n>::vec64 vec64;                                                 \
size_t roiw8 = size.width >= (8/sizeof(sgn##bits) - 1) ? size.width - (8/sizeof(sgn##bits) - 1) : 0;    \
\
for (size_t i = 0u; i < size.height; ++i)                                                               \
{                                                                                                       \
FILL_LINES##n(VROW, sgn##bits)                                                                      \
sgn##bits * dst = internal::getRowPtr(dstBase, dstStride, i);                                       \
size_t sj = 0u, dj = 0u;                                                                            \
\
for (; sj < roiw16; sj += 16/sizeof(sgn##bits), dj += MUL##n(16)/sizeof(sgn##bits))                 \
MERGE_QUAD(sgn, bits, n)                                                                        \
\
if ( sj < roiw8 )                                                                                   \
{                                                                                                   \
vec64 v_dst;                                                                                    \
FILL_LINES##n(VLD1, sgn##bits)                                                                  \
vst##n##_##sgn##bits(dst + dj, v_dst);                                                          \
sj += 8/sizeof(sgn##bits); dj += MUL##n(8)/sizeof(sgn##bits);                                   \
}                                                                                                   \
\
for (; sj < size.width; ++sj, dj += n)                                                              \
{                                                                                                   \
FILL_LINES##n(SLD, sgn##bits)                                                                   \
}                                                                                                   \
}                                                                                                       \
}
#define COMBINE64(sgn,n) void combine##n(const Size2D &_size                                                \
FILL_LINES##n(FARG, sgn##64),                                \
sgn##64 * dstBase, ptrdiff_t dstStride)                      \
{                                                                                                           \
internal::assertSupportedConfiguration();                                                               \
Size2D size(_size);                                                                                     \
if (CONTSRC##n                                                                                          \
dstStride == (ptrdiff_t)(size.width))                                                               \
{                                                                                                       \
size.width *= size.height;                                                                          \
size.height = 1;                                                                                    \
}                                                                                                       \
typedef internal::VecTraits<sgn##64, n>::vec64 vec64;                                                   \
\
for (size_t i = 0u; i < size.height; ++i)                                                               \
{                                                                                                       \
FILL_LINES##n(VROW, sgn##64)                                                                        \
sgn##64 * dst = internal::getRowPtr(dstBase, dstStride, i);                                         \
size_t sj = 0u, dj = 0u;                                                                            \
\
for (; sj < size.width; ++sj, dj += n)                                                              \
{                                                                                                   \
vec64 v_dst;                                                                                    \
FILL_LINES##n(VLD1, sgn##64)                                                                    \
vst##n##_##sgn##64(dst + dj, v_dst);                                                            \
/*FILL_LINES##n(SLD, sgn##64)*/                                                                 \
}                                                                                                   \
}                                                                                                       \
}
#else
#define  VOID_LINE(type, n) (void)src##n##Base; (void)src##n##Stride;
#define COMBINE(sgn,bits,n) void combine##n(const Size2D &size                                              \
FILL_LINES##n(FARG, sgn##bits),                                     \
sgn##bits * dstBase, ptrdiff_t dstStride)                           \
{                                                                                                           \
internal::assertSupportedConfiguration();                                                               \
(void)size;                                                                                             \
FILL_LINES##n(VOID, sgn##bits)                                                                          \
(void)dstBase;                                                                                          \
(void)dstStride;                                                                                        \
}
#define COMBINE64(sgn,n) COMBINE(sgn,64,n)
#endif //CAROTENE_NEON
COMBINE(u, 8,2)
COMBINE(u, 8,3)
COMBINE(u, 8,4)
COMBINE(u,16,2)
COMBINE(u,16,3)
COMBINE(u,16,4)
COMBINE(s,32,2)
COMBINE(s,32,3)
COMBINE(s,32,4)
COMBINE64(s, 2)
COMBINE64(s, 3)
COMBINE64(s, 4)
void combineYUYV(const Size2D &size,
const u8 * srcyBase, ptrdiff_t srcyStride,
const u8 * srcuBase, ptrdiff_t srcuStride,
const u8 * srcvBase, ptrdiff_t srcvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#ifndef __ANDROID__
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; i += 1)
{
const u8 * srcy = internal::getRowPtr(srcyBase, srcyStride, i);
const u8 * srcu = internal::getRowPtr(srcuBase, srcuStride, i);
const u8 * srcv = internal::getRowPtr(srcvBase, srcvStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t syj = 0u, sj = 0u, dj = 0u;
#ifndef __ANDROID__
for (; sj < roiw32; sj += 32, syj += 64, dj += 128)
{
internal::prefetch(srcy + syj);
internal::prefetch(srcu + sj);
internal::prefetch(srcv + sj);
uint8x16x2_t v_y = vld2q_u8(srcy + syj);
uint8x16x4_t v_dst;
v_dst.val[0] = v_y.val[0];
v_dst.val[1] = vld1q_u8(srcu + sj);
v_dst.val[2] = v_y.val[1];
v_dst.val[3] = vld1q_u8(srcv + sj);
vst4q_u8(dst + dj, v_dst);
v_y = vld2q_u8(srcy + syj + 32);
v_dst.val[0] = v_y.val[0];
v_dst.val[1] = vld1q_u8(srcu + sj + 16);
v_dst.val[2] = v_y.val[1];
v_dst.val[3] = vld1q_u8(srcv + sj + 16);
vst4q_u8(dst + dj + 64, v_dst);
}
#endif
for (; sj < roiw8; sj += 8, syj += 16, dj += 32)
{
uint8x8x2_t v_y = vld2_u8(srcy + syj);
uint8x8x4_t v_dst;
v_dst.val[0] = v_y.val[0];
v_dst.val[1] = vld1_u8(srcu + sj);
v_dst.val[2] = v_y.val[1];
v_dst.val[3] = vld1_u8(srcv + sj);
vst4_u8(dst + dj, v_dst);
}
for (; sj < size.width; ++sj, syj += 2, dj += 4)
{
dst[dj] = srcy[syj];
dst[dj + 1] = srcu[sj];
dst[dj + 2] = srcy[syj + 1];
dst[dj + 3] = srcv[sj];
}
}
#else
(void)size;
(void)srcyBase;
(void)srcyStride;
(void)srcuBase;
(void)srcuStride;
(void)srcvBase;
(void)srcvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void combineUYVY(const Size2D &size,
const u8 * srcyBase, ptrdiff_t srcyStride,
const u8 * srcuBase, ptrdiff_t srcuStride,
const u8 * srcvBase, ptrdiff_t srcvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#ifndef __ANDROID__
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * srcy = internal::getRowPtr(srcyBase, srcyStride, i);
const u8 * srcu = internal::getRowPtr(srcuBase, srcuStride, i);
const u8 * srcv = internal::getRowPtr(srcvBase, srcvStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t syj = 0u, sj = 0u, dj = 0u;
#ifndef __ANDROID__
for (; sj < roiw32; sj += 32, syj += 64, dj += 128)
{
internal::prefetch(srcy + syj);
internal::prefetch(srcu + sj);
internal::prefetch(srcv + sj);
uint8x16x2_t v_y = vld2q_u8(srcy + syj);
uint8x16x4_t v_dst;
v_dst.val[0] = vld1q_u8(srcu + sj);
v_dst.val[1] = v_y.val[0];
v_dst.val[2] = vld1q_u8(srcv + sj);
v_dst.val[3] = v_y.val[1];
vst4q_u8(dst + dj, v_dst);
v_y = vld2q_u8(srcy + syj + 32);
v_dst.val[0] = vld1q_u8(srcu + sj + 16);
v_dst.val[1] = v_y.val[0];
v_dst.val[2] = vld1q_u8(srcv + sj + 16);
v_dst.val[3] = v_y.val[1];
vst4q_u8(dst + dj + 64, v_dst);
}
#endif
for (; sj < roiw8; sj += 8, syj += 16, dj += 32)
{
uint8x8x2_t v_y = vld2_u8(srcy + syj);
uint8x8x4_t v_dst;
v_dst.val[0] = vld1_u8(srcu + sj);
v_dst.val[1] = v_y.val[0];
v_dst.val[2] = vld1_u8(srcv + sj);
v_dst.val[3] = v_y.val[1];
vst4_u8(dst + dj, v_dst);
}
for (; sj < size.width; ++sj, syj += 2, dj += 4)
{
dst[dj] = srcu[sj];
dst[dj + 1] = srcy[syj];
dst[dj + 2] = srcv[sj];
dst[dj + 3] = srcy[syj + 1];
}
}
#else
(void)size;
(void)srcyBase;
(void)srcyStride;
(void)srcuBase;
(void)srcuStride;
(void)srcvBase;
(void)srcvStride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
void extract2(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
u32 coi)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#ifndef __ANDROID__
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#ifndef __ANDROID__
for (; dj < roiw32; sj += 64, dj += 32)
{
internal::prefetch(src + sj);
uint8x16x2_t v_src = vld2q_u8(src + sj);
vst1q_u8(dst + dj, v_src.val[coi]);
v_src = vld2q_u8(src + sj + 32);
vst1q_u8(dst + dj + 16, v_src.val[coi]);
}
#endif
for (; dj < roiw8; sj += 16, dj += 8)
{
uint8x8x2_t v_src = vld2_u8(src + sj);
vst1_u8(dst + dj, v_src.val[coi]);
}
for (; dj < size.width; sj += 2, ++dj)
{
dst[dj] = src[sj + coi];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)coi;
#endif
}
void extract3(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
u32 coi)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#ifndef __ANDROID__
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#ifndef __ANDROID__
for (; dj < roiw32; sj += 96, dj += 32)
{
internal::prefetch(src + sj);
uint8x16x3_t v_src = vld3q_u8(src + sj);
vst1q_u8(dst + dj, v_src.val[coi]);
v_src = vld3q_u8(src + sj + 48);
vst1q_u8(dst + dj + 16, v_src.val[coi]);
}
#endif
for (; dj < roiw8; sj += 24, dj += 8)
{
uint8x8x3_t v_src = vld3_u8(src + sj);
vst1_u8(dst + dj, v_src.val[coi]);
}
for (; dj < size.width; sj += 3, ++dj)
{
dst[dj] = src[sj + coi];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)coi;
#endif
}
void extract4(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
u32 coi)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#ifndef __ANDROID__
size_t roiw32 = size.width >= 31 ? size.width - 31 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#ifndef __ANDROID__
for (; dj < roiw32; sj += 128, dj += 32)
{
internal::prefetch(src + sj);
uint8x16x4_t v_src = vld4q_u8(src + sj);
vst1q_u8(dst + dj, v_src.val[coi]);
v_src = vld4q_u8(src + sj + 64);
vst1q_u8(dst + dj + 16, v_src.val[coi]);
}
#endif
for (; dj < roiw8; sj += 32, dj += 8)
{
uint8x8x4_t v_src = vld4_u8(src + sj);
vst1_u8(dst + dj, v_src.val[coi]);
}
for (; dj < size.width; sj += 4, ++dj)
{
dst[dj] = src[sj + coi];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)coi;
#endif
}
#define FILL_LINES2(macro,type) \
macro##_LINE(type,0) \
macro##_LINE(type,1)
#define FILL_LINES3(macro,type) \
FILL_LINES2(macro,type) \
macro##_LINE(type,2)
#define FILL_LINES4(macro,type) \
FILL_LINES3(macro,type) \
macro##_LINE(type,3)
#define FARG_LINE(type, n) , type * dst##n##Base, ptrdiff_t dst##n##Stride
#ifdef CAROTENE_NEON
#define VROW_LINE(type, n) type * dst##n = internal::getRowPtr(dst##n##Base, dst##n##Stride, i);
#define VST1Q_LINE(type, n) vst1q_##type(dst##n + dj, v_src.val[n]);
#define VST1_LINE(type, n) vst1_##type(dst##n + dj, v_src.val[n]);
#define SST_LINE(type, n) dst##n[dj] = src[sj + n];
#define MUL2(val) (val << 1)
#define MUL3(val) (MUL2(val) + val)
#define MUL4(val) (val << 2)
#define CONTDST2 srcStride == dst0Stride && \
srcStride == dst1Stride &&
#define CONTDST3 srcStride == dst0Stride && \
srcStride == dst1Stride && \
srcStride == dst2Stride &&
#define CONTDST4 srcStride == dst0Stride && \
srcStride == dst1Stride && \
srcStride == dst2Stride && \
srcStride == dst3Stride &&
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
#define SPLIT_ASM2(sgn, bits) __asm__ ( \
"vld2." #bits " {d0, d2}, [%[in0]]            \n\t" \
"vld2." #bits " {d1, d3}, [%[in1]]            \n\t" \
"vst1." #bits " {d0-d1}, [%[out0]]            \n\t" \
"vst1." #bits " {d2-d3}, [%[out1]]            \n\t" \
: \
: [out0] "r" (dst0 + dj), [out1] "r" (dst1 + dj), \
[in0]  "r" (src + sj), [in1]  "r" (src + sj + MUL2(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3" \
);
#define SPLIT_ASM3(sgn, bits) __asm__ ( \
"vld3." #bits " {d0, d2, d4}, [%[in0]]        \n\t" \
"vld3." #bits " {d1, d3, d5}, [%[in1]]        \n\t" \
"vst1." #bits " {d0-d1}, [%[out0]]            \n\t" \
"vst1." #bits " {d2-d3}, [%[out1]]            \n\t" \
"vst1." #bits " {d4-d5}, [%[out2]]            \n\t" \
: \
: [out0] "r" (dst0 + dj), [out1] "r" (dst1 + dj), [out2] "r" (dst2 + dj), \
[in0]  "r" (src + sj), [in1]  "r" (src + sj + MUL3(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3","d4","d5" \
);
#define SPLIT_ASM4(sgn, bits) __asm__ ( \
"vld4." #bits " {d0, d2, d4, d6}, [%[in0]]    \n\t" \
"vld4." #bits " {d1, d3, d5, d7}, [%[in1]]    \n\t" \
"vst1." #bits " {d0-d1}, [%[out0]]            \n\t" \
"vst1." #bits " {d2-d3}, [%[out1]]            \n\t" \
"vst1." #bits " {d4-d5}, [%[out2]]            \n\t" \
"vst1." #bits " {d6-d7}, [%[out3]]            \n\t" \
: \
: [out0] "r" (dst0 + dj), [out1] "r" (dst1 + dj), [out2] "r" (dst2 + dj), [out3] "r" (dst3 + dj), \
[in0]  "r" (src + sj), [in1]  "r" (src + sj + MUL4(8)/sizeof(sgn##bits)) \
: "d0","d1","d2","d3","d4","d5","d6","d7" \
);
#define SPLIT_QUAD(sgn, bits, n) { \
internal::prefetch(src + sj); \
SPLIT_ASM##n(sgn, bits) \
}
#else
#define SPLIT_QUAD(sgn, bits, n) { \
internal::prefetch(src + sj); \
vec128 v_src = vld##n##q_##sgn##bits(src + sj); \
FILL_LINES##n(VST1Q, sgn##bits) \
}
#endif
#define SPLIT(sgn,bits,n) void split##n(const Size2D &_size,                                            \
const sgn##bits * srcBase, ptrdiff_t srcStride                      \
FILL_LINES##n(FARG, sgn##bits) )                                    \
{                                                                                                       \
internal::assertSupportedConfiguration();                                                           \
Size2D size(_size);                                                                                 \
if (CONTDST##n                                                                                      \
dst0Stride == (ptrdiff_t)(size.width))                                                          \
{                                                                                                   \
size.width *= size.height;                                                                      \
size.height = 1;                                                                                \
}                                                                                                   \
typedef internal::VecTraits<sgn##bits, n>::vec128 vec128;                                           \
size_t roiw16 = size.width >= (16/sizeof(sgn##bits)-1) ? size.width - (16/sizeof(sgn##bits)-1) : 0; \
typedef internal::VecTraits<sgn##bits, n>::vec64 vec64;                                             \
size_t roiw8 = size.width >= (8/sizeof(sgn##bits)-1) ? size.width - (8/sizeof(sgn##bits)-1) : 0;    \
\
for (size_t i = 0u; i < size.height; ++i)                                                           \
{                                                                                                   \
const sgn##bits * src = internal::getRowPtr(srcBase, srcStride, i);                             \
FILL_LINES##n(VROW, sgn##bits)                                                                  \
size_t sj = 0u, dj = 0u;                                                                        \
\
for (; dj < roiw16; sj += MUL##n(16)/sizeof(sgn##bits), dj += 16/sizeof(sgn##bits))             \
SPLIT_QUAD(sgn, bits, n)                                                                    \
\
if (dj < roiw8)                                                                                 \
{                                                                                               \
vec64 v_src = vld##n##_##sgn##bits(src + sj);                                               \
FILL_LINES##n(VST1, sgn##bits)                                                              \
sj += MUL##n(8)/sizeof(sgn##bits);                                                          \
dj += 8/sizeof(sgn##bits);                                                                  \
}                                                                                               \
\
for (; dj < size.width; sj += n, ++dj)                                                          \
{                                                                                               \
FILL_LINES##n(SST, sgn##bits)                                                               \
}                                                                                               \
}                                                                                                   \
}
#define SPLIT64(sgn,n) void split##n(const Size2D &_size,                                               \
const sgn##64 * srcBase, ptrdiff_t srcStride                       \
FILL_LINES##n(FARG, sgn##64) )                                     \
{                                                                                                       \
internal::assertSupportedConfiguration();                                                           \
Size2D size(_size);                                                                                 \
if (CONTDST##n                                                                                      \
dst0Stride == (ptrdiff_t)(size.width))                                                          \
{                                                                                                   \
size.width *= size.height;                                                                      \
size.height = 1;                                                                                \
}                                                                                                   \
typedef internal::VecTraits<sgn##64, n>::vec64 vec64;                                               \
\
for (size_t i = 0u; i < size.height; ++i)                                                           \
{                                                                                                   \
const sgn##64 * src = internal::getRowPtr(srcBase, srcStride, i);                               \
FILL_LINES##n(VROW, sgn##64)                                                                    \
size_t sj = 0u, dj = 0u;                                                                        \
\
for (; dj < size.width; sj += n, ++dj)                                                          \
{                                                                                               \
vec64 v_src = vld##n##_##sgn##64(src + sj);                                                 \
FILL_LINES##n(VST1, sgn##64)                                                                \
}                                                                                               \
}                                                                                                   \
}
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
#define ALPHA_QUAD(sgn, bits) { \
internal::prefetch(src + sj); \
__asm__ ( \
"vld4." #bits " {d0, d2, d4, d6}, [%[in0]]    \n\t" \
"vld4." #bits " {d1, d3, d5, d7}, [%[in1]]    \n\t" \
"vst3." #bits " {d0, d2, d4}, [%[out3_1]]     \n\t" \
"vst3." #bits " {d1, d3, d5}, [%[out3_2]]     \n\t" \
"vst1." #bits " {d6-d7}, [%[out1]]            \n\t" \
: \
: [out3_1] "r" (dst3 + d3j), [out3_2] "r" (dst3 + d3j + 24/sizeof(sgn##bits)), [out1] "r" (dst1 + d1j), \
[in0]  "r" (src + sj), [in1]  "r" (src + sj + 32/sizeof(sgn##bits)) \
: "d0","d1","d2","d3","d4","d5","d6","d7" \
); \
}
#else
#define ALPHA_QUAD(sgn, bits) { \
internal::prefetch(src + sj); \
union { vec128_4 v4; vec128_3 v3; } vals; \
vals.v4 = vld4q_##sgn##bits(src + sj); \
vst3q_##sgn##bits(dst3 + d3j, vals.v3); \
vst1q_##sgn##bits(dst1 + d1j, vals.v4.val[3]); \
}
#endif
#define SPLIT4ALPHA(sgn,bits) void split4(const Size2D &_size,                                          \
const sgn##bits * srcBase, ptrdiff_t srcStride,               \
sgn##bits * dst3Base, ptrdiff_t dst3Stride,                   \
sgn##bits * dst1Base, ptrdiff_t dst1Stride)                   \
{                                                                                                       \
internal::assertSupportedConfiguration();                                                           \
Size2D size(_size);                                                                                 \
if (srcStride == dst3Stride &&                                                                      \
srcStride == dst1Stride &&                                                                      \
srcStride == (ptrdiff_t)(size.width))                                                           \
{                                                                                                   \
size.width *= size.height;                                                                      \
size.height = 1;                                                                                \
}                                                                                                   \
typedef internal::VecTraits<sgn##bits, 4>::vec128 vec128_4;                                         \
typedef internal::VecTraits<sgn##bits, 3>::vec128 vec128_3;                                         \
size_t roiw16 = size.width >= (16/sizeof(sgn##bits)-1) ? size.width - (16/sizeof(sgn##bits)-1) : 0; \
typedef internal::VecTraits<sgn##bits, 4>::vec64 vec64_4;                                           \
typedef internal::VecTraits<sgn##bits, 3>::vec64 vec64_3;                                           \
size_t roiw8 = size.width >= (8/sizeof(sgn##bits)-1) ? size.width - (8/sizeof(sgn##bits)-1) : 0;    \
\
for (size_t i = 0u; i < size.height; ++i)                                                           \
{                                                                                                   \
const sgn##bits * src = internal::getRowPtr(srcBase, srcStride, i);                             \
sgn##bits * dst3 = internal::getRowPtr(dst3Base, dst3Stride, i);                                \
sgn##bits * dst1 = internal::getRowPtr(dst1Base, dst1Stride, i);                                \
size_t sj = 0u, d3j = 0u, d1j = 0u;                                                             \
\
for (; d1j < roiw16; sj += MUL4(16)/sizeof(sgn##bits), d3j += MUL3(16)/sizeof(sgn##bits),       \
d1j += 16/sizeof(sgn##bits))             \
ALPHA_QUAD(sgn, bits)                                                                       \
\
if (d1j < roiw8)                                                                                \
{                                                                                               \
union { vec64_4 v4; vec64_3 v3; } vals;                                                     \
vals.v4 = vld4_##sgn##bits(src + sj);                                                       \
vst3_u8(dst3 + d3j, vals.v3);                                                               \
vst1_u8(dst1 + d1j, vals.v4.val[3]);                                                        \
sj += MUL4(8)/sizeof(sgn##bits);                                                            \
d3j += MUL3(8)/sizeof(sgn##bits);                                                           \
d1j += 8/sizeof(sgn##bits);                                                                 \
}                                                                                               \
\
for (; d1j < size.width; sj += 4, d3j += 3, ++d1j)                                              \
{                                                                                               \
dst3[d3j+0] = src[sj + 0];                                                                  \
dst3[d3j+1] = src[sj + 1];                                                                  \
dst3[d3j+2] = src[sj + 2];                                                                  \
dst1[d1j]   = src[sj + 3];                                                                  \
}                                                                                               \
}                                                                                                   \
}
#else
#define VOID_LINE(type, n) (void)dst##n##Base; (void)dst##n##Stride;
#define SPLIT(sgn,bits,n) void split##n(const Size2D &size,                                          \
const sgn##bits * srcBase, ptrdiff_t srcStride                   \
FILL_LINES##n(FARG, sgn##bits) )                                 \
{                                                                                                    \
internal::assertSupportedConfiguration();                                                        \
(void)size;                                                                                      \
(void)srcBase;                                                                                   \
(void)srcStride;                                                                                 \
FILL_LINES##n(VOID, sgn##bits)                                                                   \
}
#define SPLIT64(sgn,n) SPLIT(sgn,64,n)
#define SPLIT4ALPHA(sgn,bits) void split4(const Size2D &size,                                        \
const sgn##bits * srcBase, ptrdiff_t srcStride,            \
sgn##bits * dst3Base, ptrdiff_t dst3Stride,                \
sgn##bits * dst1Base, ptrdiff_t dst1Stride)                \
{                                                                                                    \
internal::assertSupportedConfiguration();                                                        \
(void)size;                                                                                      \
(void)srcBase;                                                                                   \
(void)srcStride;                                                                                 \
(void)dst3Base;                                                                                  \
(void)dst3Stride;                                                                                \
(void)dst1Base;                                                                                  \
(void)dst1Stride;                                                                                \
}
#endif //CAROTENE_NEON
SPLIT(u, 8,2)
SPLIT(u, 8,3)
SPLIT(u, 8,4)
SPLIT(u,16,2)
SPLIT(u,16,3)
SPLIT(u,16,4)
SPLIT(s,32,2)
SPLIT(s,32,3)
SPLIT(s,32,4)
SPLIT64(s, 2)
SPLIT64(s, 3)
SPLIT64(s, 4)
SPLIT4ALPHA(u,8)
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
inline void vnst(u8* dst, uint8x16_t v1, uint8x16_t v2) { vst1q_u8(dst, v1); vst1q_u8(dst+16, v2); }
inline void vnst(u8* dst, uint16x8_t v1, uint16x8_t v2) { vst1q_u8(dst, vcombine_u8(vmovn_u16(v1), vmovn_u16(v2))); }
inline void vnst(u8* dst, uint32x4_t v1, uint32x4_t v2) { vst1_u8(dst, vmovn_u16(vcombine_u16(vmovn_u32(v1), vmovn_u32(v2)))); }
template <typename Op, int elsize> struct vtail
{
static inline void compare(const typename Op::type * src0, const typename Op::type * src1,
u8 * dst, const Op & op,
size_t &x, size_t width)
{
//do nothing since there couldn't be enough data
(void)src0;
(void)src1;
(void)dst;
(void)op;
(void)x;
(void)width;
}
};
template <typename Op> struct vtail<Op, 2>
{
static inline void compare(const typename Op::type * src0, const typename Op::type * src1,
u8 * dst, const Op & op,
size_t &x, size_t width)
{
typedef typename Op::type type;
typedef typename internal::VecTraits<type>::vec128 vec128;
typedef typename internal::VecTraits<type>::unsign::vec128 uvec128;
//There no more than 15 elements in the tail, so we could handle 8 element vector only once
if( x + 8 < width)
{
vec128  v_src0, v_src1;
uvec128 v_dst;
v_src0 = internal::vld1q(src0 + x);
v_src1 = internal::vld1q(src1 + x);
op(v_src0, v_src1, v_dst);
internal::vst1(dst + x, internal::vmovn(v_dst));
x+=8;
}
}
};
template <typename Op> struct vtail<Op, 1>
{
static inline void compare(const typename Op::type * src0, const typename Op::type * src1,
u8 * dst, const Op & op,
size_t &x, size_t width)
{
typedef typename Op::type type;
typedef typename internal::VecTraits<type>::vec128 vec128;
typedef typename internal::VecTraits<type>::unsign::vec128 uvec128;
typedef typename internal::VecTraits<type>::vec64 vec64;
typedef typename internal::VecTraits<type>::unsign::vec64 uvec64;
//There no more than 31 elements in the tail, so we could handle once 16+8 or 16 or 8 elements
if( x + 16 < width)
{
vec128  v_src0, v_src1;
uvec128 v_dst;
v_src0 = internal::vld1q(src0 + x);
v_src1 = internal::vld1q(src1 + x);
op(v_src0, v_src1, v_dst);
internal::vst1q(dst + x, v_dst);
x+=16;
}
if( x + 8 < width)
{
vec64  v_src0, v_src1;
uvec64 v_dst;
v_src0 = internal::vld1(src0 + x);
v_src1 = internal::vld1(src1 + x);
op(v_src0, v_src1, v_dst);
internal::vst1(dst + x, v_dst);
x+=8;
}
}
};
template <typename Op>
void vcompare(Size2D size,
const typename Op::type * src0Base, ptrdiff_t src0Stride,
const typename Op::type * src1Base, ptrdiff_t src1Stride,
u8 * dstBase, ptrdiff_t dstStride, const Op & op)
{
typedef typename Op::type type;
typedef typename internal::VecTraits<type>::vec128 vec128;
typedef typename internal::VecTraits<type>::unsign::vec128 uvec128;
if (src0Stride == src1Stride && src0Stride == dstStride &&
src0Stride == (ptrdiff_t)(size.width * sizeof(type)))
{
size.width *= size.height;
size.height = 1;
}
const u32 step_base = 32 / sizeof(type);
size_t roiw_base = size.width >= (step_base - 1) ? size.width - step_base + 1 : 0;
for (size_t y = 0; y < size.height; ++y)
{
const type * src0 = internal::getRowPtr(src0Base, src0Stride, y);
const type * src1 = internal::getRowPtr(src1Base, src1Stride, y);
u8 * dst = internal::getRowPtr(dstBase, dstStride, y);
size_t x = 0;
for( ; x < roiw_base; x += step_base )
{
internal::prefetch(src0 + x);
internal::prefetch(src1 + x);
vec128 v_src00 = internal::vld1q(src0 + x), v_src01 = internal::vld1q(src0 + x + 16 / sizeof(type));
vec128 v_src10 = internal::vld1q(src1 + x), v_src11 = internal::vld1q(src1 + x + 16 / sizeof(type));
uvec128 v_dst0;
uvec128 v_dst1;
op(v_src00, v_src10, v_dst0);
op(v_src01, v_src11, v_dst1);
vnst(dst + x, v_dst0, v_dst1);
}
vtail<Op, sizeof(type)>::compare(src0, src1, dst, op, x, size.width);
for (; x < size.width; ++x)
{
op(src0 + x, src1 + x, dst + x);
}
}
}
template<typename T>
struct OpCmpEQ
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0, const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::unsign::vec128 & v_dst) const
{
v_dst = internal::vceqq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0, const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::unsign::vec64 & v_dst) const
{
v_dst = internal::vceq(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, u8 * dst) const
{
dst[0] = src0[0] == src1[0] ? 255 : 0;
}
};
template<typename T>
struct OpCmpNE
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0, const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::unsign::vec128 & v_dst) const
{
v_dst = internal::vmvnq(internal::vceqq(v_src0, v_src1));
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0, const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::unsign::vec64 & v_dst) const
{
v_dst = internal::vmvn(internal::vceq(v_src0, v_src1));
}
void operator() (const T * src0, const T * src1, u8 * dst) const
{
dst[0] = src0[0] == src1[0] ? 0 : 255;
}
};
template<typename T>
struct OpCmpGT
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0, const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::unsign::vec128 & v_dst) const
{
v_dst = internal::vcgtq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0, const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::unsign::vec64 & v_dst) const
{
v_dst = internal::vcgt(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, u8 * dst) const
{
dst[0] = src0[0] > src1[0] ? 255 : 0;
}
};
template<typename T>
struct OpCmpGE
{
typedef T type;
void operator() (const typename internal::VecTraits<T>::vec128 & v_src0, const typename internal::VecTraits<T>::vec128 & v_src1,
typename internal::VecTraits<T>::unsign::vec128 & v_dst) const
{
v_dst = internal::vcgeq(v_src0, v_src1);
}
void operator() (const typename internal::VecTraits<T>::vec64 & v_src0, const typename internal::VecTraits<T>::vec64 & v_src1,
typename internal::VecTraits<T>::unsign::vec64 & v_dst) const
{
v_dst = internal::vcge(v_src0, v_src1);
}
void operator() (const T * src0, const T * src1, u8 * dst) const
{
dst[0] = src0[0] >= src1[0] ? 255 : 0;
}
};
}
#define IMPL_CMPOP(op, type)                              \
void cmp##op(const Size2D &size,                          \
const type * src0Base, ptrdiff_t src0Stride, \
const type * src1Base, ptrdiff_t src1Stride, \
u8 *dstBase, ptrdiff_t dstStride)  \
{                                                         \
internal::assertSupportedConfiguration();             \
vcompare(size,                                        \
src0Base, src0Stride,                        \
src1Base, src1Stride,                        \
dstBase, dstStride,                          \
OpCmp##op<type>());                          \
}
#else
#define IMPL_CMPOP(op, type)                              \
void cmp##op(const Size2D &size,                          \
const type * src0Base, ptrdiff_t src0Stride, \
const type * src1Base, ptrdiff_t src1Stride, \
u8 *dstBase, ptrdiff_t dstStride)            \
{                                                         \
internal::assertSupportedConfiguration();             \
(void)size;                                           \
(void)src0Base;                                       \
(void)src0Stride;                                     \
(void)src1Base;                                       \
(void)src1Stride;                                     \
(void)dstBase;                                        \
(void)dstStride;                                      \
}
#endif
IMPL_CMPOP(EQ, u8)
IMPL_CMPOP(EQ, s8)
IMPL_CMPOP(EQ, u16)
IMPL_CMPOP(EQ, s16)
IMPL_CMPOP(EQ, u32)
IMPL_CMPOP(EQ, s32)
IMPL_CMPOP(EQ, f32)
IMPL_CMPOP(NE, u8)
IMPL_CMPOP(NE, s8)
IMPL_CMPOP(NE, u16)
IMPL_CMPOP(NE, s16)
IMPL_CMPOP(NE, u32)
IMPL_CMPOP(NE, s32)
IMPL_CMPOP(NE, f32)
IMPL_CMPOP(GT, u8)
IMPL_CMPOP(GT, s8)
IMPL_CMPOP(GT, u16)
IMPL_CMPOP(GT, s16)
IMPL_CMPOP(GT, u32)
IMPL_CMPOP(GT, s32)
IMPL_CMPOP(GT, f32)
IMPL_CMPOP(GE, u8)
IMPL_CMPOP(GE, s8)
IMPL_CMPOP(GE, u16)
IMPL_CMPOP(GE, s16)
IMPL_CMPOP(GE, u32)
IMPL_CMPOP(GE, s32)
IMPL_CMPOP(GE, f32)
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "saturate_cast.hpp"
#include "vround_helper.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
enum
{
SHIFT = 15,
SHIFT_DELTA = 1 << (SHIFT - 1),
R2Y_BT601   = 9798,
G2Y_BT601   = 19235,
B2Y_BT601   = 3735,
R2Y_BT709   = 3483,
G2Y_BT709   = 11718,
B2Y_BT709   = 1183,
};
inline uint8x8_t convertToGray(const uint16x8_t & v_r,
const uint16x8_t & v_g,
const uint16x8_t & v_b,
const uint16x4_t & v_r2y,
const uint16x4_t & v_g2y,
const uint16x4_t & v_b2y)
{
uint32x4_t v_dst0 = vmull_u16(vget_low_u16(v_g), v_g2y);
uint32x4_t v_dst1 = vmull_u16(vget_high_u16(v_g), v_g2y);
v_dst0 = vmlal_u16(v_dst0, vget_low_u16(v_r), v_r2y);
v_dst1 = vmlal_u16(v_dst1, vget_high_u16(v_r), v_r2y);
v_dst0 = vmlal_u16(v_dst0, vget_low_u16(v_b), v_b2y);
v_dst1 = vmlal_u16(v_dst1, vget_high_u16(v_b), v_b2y);
uint8x8_t v_gray = vqmovn_u16(vcombine_u16(vrshrn_n_u32(v_dst0, SHIFT),
vrshrn_n_u32(v_dst1, SHIFT)));
return v_gray;
}
} // namespace
#endif
void rgb2gray(const Size2D &size, COLOR_SPACE color_space,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
const u32 R2Y = color_space == COLOR_SPACE_BT601 ? R2Y_BT601 : R2Y_BT709;
const u32 G2Y = color_space == COLOR_SPACE_BT601 ? G2Y_BT601 : G2Y_BT709;
const u32 B2Y = color_space == COLOR_SPACE_BT601 ? B2Y_BT601 : B2Y_BT709;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register int16x4_t v_r2y asm ("d31") = vmov_n_s16(R2Y);
register int16x4_t v_g2y asm ("d30") = vmov_n_s16(G2Y);
register int16x4_t v_b2y asm ("d29") = vmov_n_s16(B2Y);
#else
uint16x4_t v_r2y = vdup_n_u16(R2Y),
v_g2y = vdup_n_u16(G2Y),
v_b2y = vdup_n_u16(B2Y);
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
for (; dj < roiw8; sj += 24, dj += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld3.8 {d0-d2}, [%[in]] @RGB                       \n\t"
"vmovl.u8 q2, d0         @R (d4,d5)                 \n\t"
"vmovl.u8 q3, d1         @G (d6,d7)                 \n\t"
"vmovl.u8 q4, d2         @B (d8,d9)                 \n\t"
"vmull.u16 q5, d6, d30   @Y (q5,q6):  G             \n\t"
"vmull.u16 q6, d7, d30   @Y (q5,q6):  G             \n\t"
"vmlal.s16 q5, d8, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q6, d9, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q5, d4, d31   @Y (q5,q6):  GBR           \n\t"
"vmlal.s16 q6, d5, d31   @Y (q5,q6):  GBR           \n\t"
"vrshrn.s32 d8, q5, #14  @Y  -> q4                  \n\t"
"vrshrn.s32 d9, q6, #14  @Y  -> q4                  \n\t"
"vqmovn.u16 d4, q4                                  \n\t"
"vst1.8 {d4}, [%[out]]                              \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj), "w" (v_r2y), "w" (v_g2y), "w" (v_b2y)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
#else
for (; dj < roiw16; sj += 48, dj += 16)
{
internal::prefetch(src + sj);
uint8x16x3_t v_src0 = vld3q_u8(src + sj);
// 0
uint16x8_t v_r = vmovl_u8(vget_low_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_low_u8(v_src0.val[1])),
v_b = vmovl_u8(vget_low_u8(v_src0.val[2]));
uint8x8_t v_gray0 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
v_r = vmovl_u8(vget_high_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_high_u8(v_src0.val[1])),
v_b = vmovl_u8(vget_high_u8(v_src0.val[2]));
uint8x8_t v_gray1 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1q_u8(dst + dj, vcombine_u8(v_gray0, v_gray1));
}
if (dj < roiw8)
{
uint8x8x3_t v_src = vld3_u8(src + sj);
uint16x8_t v_r = vmovl_u8(v_src.val[0]),
v_g = vmovl_u8(v_src.val[1]),
v_b = vmovl_u8(v_src.val[2]);
uint8x8_t v_gray = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1_u8(dst + dj, v_gray);
sj += 24; dj += 8;
}
#endif
for (; dj < size.width; sj += 3, dj++)
{
u32 val = src[sj] * R2Y + src[sj + 1] * G2Y + src[sj + 2] * B2Y;
dst[dj] = internal::saturate_cast<u8>((val + SHIFT_DELTA) >> SHIFT);
}
}
#else
(void)size;
(void)color_space;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2gray(const Size2D &size, COLOR_SPACE color_space,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
const u32 R2Y = color_space == COLOR_SPACE_BT601 ? R2Y_BT601 : R2Y_BT709;
const u32 G2Y = color_space == COLOR_SPACE_BT601 ? G2Y_BT601 : G2Y_BT709;
const u32 B2Y = color_space == COLOR_SPACE_BT601 ? B2Y_BT601 : B2Y_BT709;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register int16x4_t v_r2y asm ("d31") = vmov_n_s16(R2Y);
register int16x4_t v_g2y asm ("d30") = vmov_n_s16(G2Y);
register int16x4_t v_b2y asm ("d29") = vmov_n_s16(B2Y);
#else
uint16x4_t v_r2y = vdup_n_u16(R2Y),
v_g2y = vdup_n_u16(G2Y),
v_b2y = vdup_n_u16(B2Y);
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
for (; dj < roiw8; sj += 32, dj += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld4.8 {d0-d3}, [%[in]] @RGBA                      \n\t"
"vmovl.u8 q2, d0         @R (d4,d5)                 \n\t"
"vmovl.u8 q3, d1         @G (d6,d7)                 \n\t"
"vmovl.u8 q4, d2         @B (d8,d9)                 \n\t"
"vmull.u16 q5, d6, d30   @Y (q5,q6):  G             \n\t"
"vmull.u16 q6, d7, d30   @Y (q5,q6):  G             \n\t"
"vmlal.s16 q5, d8, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q6, d9, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q5, d4, d31   @Y (q5,q6):  GBR           \n\t"
"vmlal.s16 q6, d5, d31   @Y (q5,q6):  GBR           \n\t"
"vrshrn.s32 d8, q5, #14  @Y  -> q4                  \n\t"
"vrshrn.s32 d9, q6, #14  @Y  -> q4                  \n\t"
"vqmovn.u16 d4, q4                                  \n\t"
"vst1.8 {d4}, [%[out]]                              \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj), "w" (v_r2y), "w" (v_g2y), "w" (v_b2y)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
#else
for (; dj < roiw16; sj += 64, dj += 16)
{
internal::prefetch(src + sj);
uint8x16x4_t v_src0 = vld4q_u8(src + sj);
// 0
uint16x8_t v_r = vmovl_u8(vget_low_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_low_u8(v_src0.val[1])),
v_b = vmovl_u8(vget_low_u8(v_src0.val[2]));
uint8x8_t v_gray0 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
v_r = vmovl_u8(vget_high_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_high_u8(v_src0.val[1])),
v_b = vmovl_u8(vget_high_u8(v_src0.val[2]));
uint8x8_t v_gray1 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1q_u8(dst + dj, vcombine_u8(v_gray0, v_gray1));
}
if (dj < roiw8)
{
uint8x8x4_t v_src = vld4_u8(src + sj);
uint16x8_t v_r = vmovl_u8(v_src.val[0]),
v_g = vmovl_u8(v_src.val[1]),
v_b = vmovl_u8(v_src.val[2]);
uint8x8_t v_gray = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1_u8(dst + dj, v_gray);
sj += 32; dj += 8;
}
#endif
for (; dj < size.width; sj += 4, dj++)
{
u32 val = src[sj] * R2Y + src[sj + 1] * G2Y + src[sj + 2] * B2Y;
dst[dj] = internal::saturate_cast<u8>((val + SHIFT_DELTA) >> SHIFT);
}
}
#else
(void)size;
(void)color_space;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void bgr2gray(const Size2D &size, COLOR_SPACE color_space,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
const u32 R2Y = color_space == COLOR_SPACE_BT601 ? R2Y_BT601 : R2Y_BT709;
const u32 G2Y = color_space == COLOR_SPACE_BT601 ? G2Y_BT601 : G2Y_BT709;
const u32 B2Y = color_space == COLOR_SPACE_BT601 ? B2Y_BT601 : B2Y_BT709;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register int16x4_t v_r2y asm ("d31") = vmov_n_s16(R2Y);
register int16x4_t v_g2y asm ("d30") = vmov_n_s16(G2Y);
register int16x4_t v_b2y asm ("d29") = vmov_n_s16(B2Y);
#else
uint16x4_t v_r2y = vdup_n_u16(R2Y),
v_g2y = vdup_n_u16(G2Y),
v_b2y = vdup_n_u16(B2Y);
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
for (; dj < roiw8; sj += 24, dj += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld3.8 {d0-d2}, [%[in]] @BGR                       \n\t"
"vmovl.u8 q2, d2         @R (d4,d5)                 \n\t"
"vmovl.u8 q3, d1         @G (d6,d7)                 \n\t"
"vmovl.u8 q4, d0         @B (d8,d9)                 \n\t"
"vmull.u16 q5, d6, d30   @Y (q5,q6):  G             \n\t"
"vmull.u16 q6, d7, d30   @Y (q5,q6):  G             \n\t"
"vmlal.s16 q5, d8, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q6, d9, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q5, d4, d31   @Y (q5,q6):  GBR           \n\t"
"vmlal.s16 q6, d5, d31   @Y (q5,q6):  GBR           \n\t"
"vrshrn.s32 d8, q5, #14  @Y  -> q4                  \n\t"
"vrshrn.s32 d9, q6, #14  @Y  -> q4                  \n\t"
"vqmovn.u16 d4, q4                                  \n\t"
"vst1.8 {d4}, [%[out]]                              \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj), "w" (v_r2y), "w" (v_g2y), "w" (v_b2y)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
#else
for (; dj < roiw16; sj += 48, dj += 16)
{
internal::prefetch(src + sj);
uint8x16x3_t v_src0 = vld3q_u8(src + sj);
// 0
uint16x8_t v_b = vmovl_u8(vget_low_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_low_u8(v_src0.val[1])),
v_r = vmovl_u8(vget_low_u8(v_src0.val[2]));
uint8x8_t v_gray0 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
v_b = vmovl_u8(vget_high_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_high_u8(v_src0.val[1])),
v_r = vmovl_u8(vget_high_u8(v_src0.val[2]));
uint8x8_t v_gray1 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1q_u8(dst + dj, vcombine_u8(v_gray0, v_gray1));
}
if (dj < roiw8)
{
uint8x8x3_t v_src = vld3_u8(src + sj);
uint16x8_t v_b = vmovl_u8(v_src.val[0]),
v_g = vmovl_u8(v_src.val[1]),
v_r = vmovl_u8(v_src.val[2]);
uint8x8_t v_gray = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1_u8(dst + dj, v_gray);
sj += 24; dj += 8;
}
#endif
for (; dj < size.width; sj += 3, dj++)
{
u32 val = src[sj] * B2Y + src[sj + 1] * G2Y + src[sj + 2] * R2Y;
dst[dj] = internal::saturate_cast<u8>((val + SHIFT_DELTA) >> SHIFT);
}
}
#else
(void)size;
(void)color_space;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void bgrx2gray(const Size2D &size, COLOR_SPACE color_space,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
const u32 R2Y = color_space == COLOR_SPACE_BT601 ? R2Y_BT601 : R2Y_BT709;
const u32 G2Y = color_space == COLOR_SPACE_BT601 ? G2Y_BT601 : G2Y_BT709;
const u32 B2Y = color_space == COLOR_SPACE_BT601 ? B2Y_BT601 : B2Y_BT709;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register int16x4_t v_r2y asm ("d31") = vmov_n_s16(R2Y);
register int16x4_t v_g2y asm ("d30") = vmov_n_s16(G2Y);
register int16x4_t v_b2y asm ("d29") = vmov_n_s16(B2Y);
#else
uint16x4_t v_r2y = vdup_n_u16(R2Y),
v_g2y = vdup_n_u16(G2Y),
v_b2y = vdup_n_u16(B2Y);
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
for (; dj < roiw8; sj += 32, dj += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld4.8 {d0-d3}, [%[in]] @BGRA                      \n\t"
"vmovl.u8 q2, d2         @R (d4,d5)                 \n\t"
"vmovl.u8 q3, d1         @G (d6,d7)                 \n\t"
"vmovl.u8 q4, d0         @B (d8,d9)                 \n\t"
"vmull.u16 q5, d6, d30   @Y (q5,q6):  G             \n\t"
"vmull.u16 q6, d7, d30   @Y (q5,q6):  G             \n\t"
"vmlal.s16 q5, d8, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q6, d9, d29   @Y (q5,q6):  GB            \n\t"
"vmlal.s16 q5, d4, d31   @Y (q5,q6):  GBR           \n\t"
"vmlal.s16 q6, d5, d31   @Y (q5,q6):  GBR           \n\t"
"vrshrn.s32 d8, q5, #14  @Y  -> q4                  \n\t"
"vrshrn.s32 d9, q6, #14  @Y  -> q4                  \n\t"
"vqmovn.u16 d4, q4                                  \n\t"
"vst1.8 {d4}, [%[out]]                              \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj), "w" (v_r2y), "w" (v_g2y), "w" (v_b2y)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
#else
for (; dj < roiw16; sj += 64, dj += 16)
{
internal::prefetch(src + sj);
uint8x16x4_t v_src0 = vld4q_u8(src + sj);
// 0
uint16x8_t v_b = vmovl_u8(vget_low_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_low_u8(v_src0.val[1])),
v_r = vmovl_u8(vget_low_u8(v_src0.val[2]));
uint8x8_t v_gray0 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
v_b = vmovl_u8(vget_high_u8(v_src0.val[0])),
v_g = vmovl_u8(vget_high_u8(v_src0.val[1])),
v_r = vmovl_u8(vget_high_u8(v_src0.val[2]));
uint8x8_t v_gray1 = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1q_u8(dst + dj, vcombine_u8(v_gray0, v_gray1));
}
if (dj < roiw8)
{
uint8x8x4_t v_src = vld4_u8(src + sj);
uint16x8_t v_b = vmovl_u8(v_src.val[0]),
v_g = vmovl_u8(v_src.val[1]),
v_r = vmovl_u8(v_src.val[2]);
uint8x8_t v_gray = convertToGray(v_r, v_g, v_b, v_r2y, v_g2y, v_b2y);
vst1_u8(dst + dj, v_gray);
sj += 32; dj += 8;
}
#endif
for (; dj < size.width; sj += 4, dj++)
{
u32 val = src[sj] * B2Y + src[sj + 1] * G2Y + src[sj + 2] * R2Y;
dst[dj] = internal::saturate_cast<u8>((val + SHIFT_DELTA) >> SHIFT);
}
}
#else
(void)size;
(void)color_space;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void gray2rgb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
for (; sj < roiw16; sj += 16, dj += 48)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld1.8 {d0-d1}, [%[in0]]            \n\t"
"vmov.8 q1, q0                       \n\t"
"vmov.8 q2, q0                       \n\t"
"vmov.8 q3, q1                       \n\t"
"vst3.8 {d2, d4, d6}, [%[out0]]      \n\t"
"vst3.8 {d3, d5, d7}, [%[out1]]      \n\t"
: /*no output*/
: [out0] "r" (dst + dj),      [out1] "r" (dst + dj + 24),
[in0] "r" (src + sj)
: "d0","d1","d2","d3","d4","d5","d6","d7"
);
#else
uint8x16x3_t vRgb1;
vRgb1.val[0] = vld1q_u8(src + sj);
vRgb1.val[1] = vRgb1.val[0];
vRgb1.val[2] = vRgb1.val[0];
vst3q_u8(dst + dj, vRgb1);
#endif
}
if (sj < roiw8)
{
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld1.8 {d0}, [%[in]]                \n\t"
"vmov.8 d1, d0                       \n\t"
"vmov.8 d2, d0                       \n\t"
"vst3.8 {d0-d2}, [%[out]]            \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj)
: "d0","d1","d2"
);
#else
uint8x8x3_t vRgb2;
vRgb2.val[0] = vld1_u8(src + sj);
vRgb2.val[1] = vRgb2.val[0];
vRgb2.val[2] = vRgb2.val[0];
vst3_u8(dst + dj, vRgb2);
#endif
sj += 8; dj += 24;
}
for (; sj < size.width; sj++, dj += 3)
{
dst[dj+0] = src[sj];
dst[dj+1] = src[sj];
dst[dj+2] = src[sj];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void gray2rgbx(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register uint8x16_t vc255   asm ("q4") = vmovq_n_u8(255);
#else
uint8x16x4_t vRgba;
uint8x8x4_t  vRgba2;
vRgba.val[3]  = vmovq_n_u8(255);
vRgba2.val[3] = vget_low_u8(vRgba.val[3]);
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u;
for (; sj < roiw16; sj += 16, dj += 64)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld1.8 {d0-d1}, [%[in0]]            \n\t"
"vmov.8 q1, q0                       \n\t"
"vmov.8 q2, q0                       \n\t"
"vmov.8 q3, q1                       \n\t"
"vst4.8 {d2, d4, d6, d8}, [%[out0]]  \n\t"
"vst4.8 {d3, d5, d7, d9}, [%[out1]]  \n\t"
: /*no output*/
: [out0] "r" (dst + dj),      [out1] "r" (dst + dj + 32),
[in0] "r" (src + sj),
"w" (vc255)
: "d0","d1","d2","d3","d4","d5","d6","d7"
);
#else
vRgba.val[0]  = vld1q_u8(src + sj);
vRgba.val[1] = vRgba.val[0];
vRgba.val[2] = vRgba.val[0];
vst4q_u8(dst + dj, vRgba);
#endif
}
if (sj < roiw8)
{
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld1.8 {d5}, [%[in]]                \n\t"
"vmov.8 d6, d5                       \n\t"
"vmov.8 d7, d5                       \n\t"
"vst4.8 {d5-d8}, [%[out]]            \n\t"
: /*no output*/
: [out] "r" (dst + dj), [in] "r" (src + sj), "w" (vc255)
: "d5","d6","d7"
);
#else
vRgba2.val[0] = vld1_u8(src + sj);
vRgba2.val[1] = vRgba2.val[0];
vRgba2.val[2] = vRgba2.val[0];
vst4_u8(dst + dj, vRgba2);
#endif
sj += 8; dj += 32;
}
for (; sj < size.width; sj++, dj += 4)
{
dst[dj+0] = src[sj];
dst[dj+1] = src[sj];
dst[dj+2] = src[sj];
dst[dj+3] = 255;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2rgbx(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
register uint8x8_t vc255_0  asm ("d3") = vmov_n_u8(255);
#else
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
union { uint8x16x4_t v4; uint8x16x3_t v3; } v_dst0;
v_dst0.v4.val[3] = vdupq_n_u8(255);
union { uint8x8x4_t v4; uint8x8x3_t v3; } v_dst;
v_dst.v4.val[3] = vdup_n_u8(255);
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 24, dj += 32, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld3.8 {d0, d1, d2}, [%[in0]]             \n\t"
"vst4.8 {d0, d1, d2, d3}, [%[out0]]        \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj),
"w" (vc255_0)
: "d0","d1","d2"
);
}
#else
for (; j < roiw16; sj += 48, dj += 64, j += 16)
{
internal::prefetch(src + sj);
v_dst0.v3 = vld3q_u8(src + sj);
vst4q_u8(dst + dj, v_dst0.v4);
}
if (j < roiw8)
{
v_dst.v3 = vld3_u8(src + sj);
vst4_u8(dst + dj, v_dst.v4);
sj += 24; dj += 32; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 3, dj += 4)
{
dst[dj] = src[sj];
dst[dj + 1] = src[sj + 1];
dst[dj + 2] = src[sj + 2];
dst[dj + 3] = 255;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2rgb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
#if !(!defined(__aarch64__) && defined(__GNUC__) && defined(__arm__))
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
union { uint8x16x4_t v4; uint8x16x3_t v3; } v_dst0;
union { uint8x8x4_t v4; uint8x8x3_t v3; } v_dst;
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld4.8 {d0, d1, d2, d3}, [%[in0]]         \n\t"
"vst3.8 {d0, d1, d2}, [%[out0]]            \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj)
: "d0","d1","d2","d3"
);
}
#else
for (; j < roiw16; sj += 64, dj += 48, j += 16)
{
internal::prefetch(src + sj);
v_dst0.v4 = vld4q_u8(src + sj);
vst3q_u8(dst + dj, v_dst0.v3);
}
if (j < roiw8)
{
v_dst.v4 = vld4_u8(src + sj);
vst3_u8(dst + dj, v_dst.v3);
sj += 32; dj += 24; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 4, dj += 3)
{
dst[dj] = src[sj];
dst[dj + 1] = src[sj + 1];
dst[dj + 2] = src[sj + 2];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2bgr(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#if !(!defined(__aarch64__) && defined(__GNUC__) && defined(__arm__))
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;

#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 24, dj += 24, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld3.8 {d0, d1, d2}, [%[in0]]             \n\t"
"vswp d0, d2                               \n\t"
"vst3.8 {d0, d1, d2}, [%[out0]]            \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj)
: "d0","d1","d2"
);
}
#else
for (; j < roiw16; sj += 48, dj += 48, j += 16)
{
internal::prefetch(src + sj);
uint8x16x3_t vals0 = vld3q_u8(src + sj);
std::swap(vals0.val[0], vals0.val[2]);
vst3q_u8(dst + dj, vals0);
}
if (j < roiw8)
{
uint8x8x3_t vals = vld3_u8(src + sj);
std::swap(vals.val[0], vals.val[2]);
vst3_u8(dst + dj, vals);
sj += 24; dj += 24; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 3, dj += 3)
{
u8 b = src[sj + 2];//Handle src == dst case
dst[dj + 2] = src[sj    ];
dst[dj + 1] = src[sj + 1];
dst[dj    ] = b;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2bgrx(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#if !(!defined(__aarch64__) && defined(__GNUC__) && defined(__arm__))
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 32, dj += 32, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld4.8 {d0, d1, d2, d3}, [%[in0]]         \n\t"
"vswp d0, d2                               \n\t"
"vst4.8 {d0, d1, d2, d3}, [%[out0]]        \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj)
: "d0","d1","d2","d3"
);
}
#else
for (; j < roiw16; sj += 64, dj += 64, j += 16)
{
internal::prefetch(src + sj);
uint8x16x4_t vals0 = vld4q_u8(src + sj);
std::swap(vals0.val[0], vals0.val[2]);
vst4q_u8(dst + dj, vals0);
}
if (j < roiw8)
{
uint8x8x4_t vals = vld4_u8(src + sj);
std::swap(vals.val[0], vals.val[2]);
vst4_u8(dst + dj, vals);
sj += 32; dj += 32; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 4, dj += 4)
{
u8 b = src[sj + 2];//Handle src == dst case
dst[dj + 2] = src[sj    ];
dst[dj + 1] = src[sj + 1];
dst[dj    ] = b;
dst[dj + 3] = src[sj + 3];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2bgr(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#if !(!defined(__aarch64__) && defined(__GNUC__) && defined(__arm__))
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld4.8 {d0, d1, d2, d3}, [%[in0]]         \n\t"
"vswp d0, d2                               \n\t"
"vst3.8 {d0, d1, d2}, [%[out0]]            \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj)
: "d0","d1","d2","d3"
);
}
#else
for (; j < roiw16; sj += 64, dj += 48, j += 16)
{
internal::prefetch(src + sj);
union { uint8x16x4_t v4; uint8x16x3_t v3; } vals0;
vals0.v4 = vld4q_u8(src + sj);
std::swap(vals0.v3.val[0], vals0.v3.val[2]);
vst3q_u8(dst + dj, vals0.v3);
}
if (j < roiw8)
{
union { uint8x8x4_t v4; uint8x8x3_t v3; } vals;
vals.v4 = vld4_u8(src + sj);
std::swap(vals.v3.val[0], vals.v3.val[2]);
vst3_u8(dst + dj, vals.v3);
sj += 32; dj += 24; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 4, dj += 3)
{
dst[dj + 2] = src[sj    ];
dst[dj + 1] = src[sj + 1];
dst[dj    ] = src[sj + 2];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2bgrx(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
register uint8x8_t vc255  asm ("d3") = vmov_n_u8(255);
#else
union { uint8x16x4_t v4; uint8x16x3_t v3; } vals0;
vals0.v4.val[3] = vmovq_n_u8(255);
union { uint8x8x4_t v4; uint8x8x3_t v3; } vals8;
vals8.v4.val[3] = vmov_n_u8(255);
#endif
#if !(!defined(__aarch64__) && defined(__GNUC__) && defined(__arm__))
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
#endif
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
for (; j < roiw8; sj += 24, dj += 32, j += 8)
{
internal::prefetch(src + sj);
__asm__ (
"vld3.8 {d0, d1, d2}, [%[in0]]             \n\t"
"vswp d0, d2                               \n\t"
"vst4.8 {d0, d1, d2, d3}, [%[out0]]        \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[in0]  "r" (src + sj),
"w" (vc255)
: "d0","d1","d2"
);
}
#else
for (; j < roiw16; sj += 48, dj += 64, j += 16)
{
internal::prefetch(src + sj);
vals0.v3 = vld3q_u8(src + sj);
std::swap(vals0.v4.val[0], vals0.v4.val[2]);
vst4q_u8(dst + dj, vals0.v4);
}
if (j < roiw8)
{
vals8.v3 = vld3_u8(src + sj);
std::swap(vals8.v4.val[0], vals8.v4.val[2]);
vst4_u8(dst + dj, vals8.v4);
sj += 24; dj += 32; j += 8;
}
#endif
for (; j < size.width; ++j, sj += 3, dj += 4)
{
dst[dj + 3] = 255;
dst[dj + 2] = src[sj    ];
dst[dj + 1] = src[sj + 1];
dst[dj    ] = src[sj + 2];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
namespace {
#ifdef CAROTENE_NEON
inline uint8x8x3_t convertToHSV(const uint8x8_t vR, const uint8x8_t vG, const uint8x8_t vB,
const s32 hrange )
{
const s32 hsv_shift = 12;
const f32 vsdiv_table = f32(255 << hsv_shift);
f32 vhdiv_table = f32(hrange << hsv_shift);
const s32 vhrange = hrange;
const s32 v0 = s32(0);
const s32 vshift = s32(1 << (hsv_shift-1));
const s32 v6 = s32(6);
uint8x8_t vMin = vmin_u8(vR, vG);
uint8x8_t vMax = vmax_u8(vR, vG);
uint16x8_t vR_u16 = vmovl_u8(vR);
uint16x8_t vG_u16 = vmovl_u8(vG);
vMax = vmax_u8(vMax, vB);
vMin = vmin_u8(vMin, vB);
uint16x8_t vB_u16 = vmovl_u8(vB);
uint16x8_t vDiff = vsubl_u8(vMax, vMin);
uint16x8_t vV = vmovl_u8(vMax);
uint16x8_t vDiffx2 = vaddq_u16(vDiff, vDiff);
uint32x4_t vDiffL = vmovl_u16(vget_low_u16(vDiff));
uint32x4_t vDiffH = vmovl_u16(vget_high_u16(vDiff));
uint16x8_t vVEqR = vceqq_u16(vR_u16, vV);
uint16x8_t vVEqG = vceqq_u16(vG_u16, vV);
int16x8_t vG_B = vsubq_s16(vreinterpretq_s16_u16(vG_u16), vreinterpretq_s16_u16(vB_u16));
uint16x8_t vInvR = vmvnq_u16(vVEqR);
int16x8_t vB_R = vsubq_s16(vreinterpretq_s16_u16(vB_u16), vreinterpretq_s16_u16(vR_u16));
int16x8_t vR_G = vsubq_s16(vreinterpretq_s16_u16(vR_u16), vreinterpretq_s16_u16(vG_u16));
uint16x8_t vMask2 = vandq_u16(vVEqG, vInvR);
vR_u16 = vandq_u16(vreinterpretq_u16_s16(vG_B), vVEqR);
int16x8_t vH2 = vaddq_s16(vB_R, vreinterpretq_s16_u16(vDiffx2));
vVEqR = vmvnq_u16(vVEqG);
vB_R = vaddq_s16(vreinterpretq_s16_u16(vDiffx2), vreinterpretq_s16_u16(vDiffx2));
vG_B = vandq_s16(vreinterpretq_s16_u16(vInvR), vreinterpretq_s16_u16(vVEqR));
vInvR = vandq_u16(vreinterpretq_u16_s16(vH2), vMask2);
vR_G = vaddq_s16(vR_G, vB_R);
int16x8_t vH = vaddq_s16(vreinterpretq_s16_u16(vR_u16), vreinterpretq_s16_u16(vInvR));
uint32x4_t vV_L = vmovl_u16(vget_low_u16(vV));
vR_G = vandq_s16(vR_G, vG_B);
uint32x4_t vV_H = vmovl_u16(vget_high_u16(vV));
int16x8_t vDiff4 = vaddq_s16(vH, vR_G);
int32x4_t vc6 = vdupq_n_s32(v6);
uint32x4_t vLine1 = vmulq_u32(vDiffL, vreinterpretq_u32_s32(vc6));
uint32x4_t vLine2 = vmulq_u32(vDiffH, vreinterpretq_u32_s32(vc6));
float32x4_t vF1 = vcvtq_f32_u32(vV_L);
float32x4_t vF2 = vcvtq_f32_u32(vV_H);
float32x4_t vHF1 = vcvtq_f32_u32(vLine1);
float32x4_t vHF2 = vcvtq_f32_u32(vLine2);
float32x4_t vXInv1 = vrecpeq_f32(vF1);
float32x4_t vXInv2 = vrecpeq_f32(vF2);
float32x4_t vXInv3 = vrecpeq_f32(vHF1);
float32x4_t vXInv4 = vrecpeq_f32(vHF2);
float32x4_t vSt1 = vrecpsq_f32(vXInv1, vF1);
float32x4_t vSt2 = vrecpsq_f32(vXInv2, vF2);
float32x4_t vSt3 = vrecpsq_f32(vXInv3, vHF1);
float32x4_t vSt4 = vrecpsq_f32(vXInv4, vHF2);
vF1 = vmulq_f32(vXInv1, vSt1);
vF2 = vmulq_f32(vXInv2, vSt2);
vHF1 = vmulq_f32(vXInv3, vSt3);
vHF2 = vmulq_f32(vXInv4, vSt4);
float32x4_t vDivTab = vdupq_n_f32(vsdiv_table);
vSt1 = vmulq_f32(vF1, vDivTab);
vSt2 = vmulq_f32(vF2, vDivTab);
vDivTab = vdupq_n_f32(vhdiv_table);
vSt3 = vmulq_f32(vHF1, vDivTab);
vSt4 = vmulq_f32(vHF2, vDivTab);
uint32x4_t vRes1 = internal::vroundq_u32_f32(vSt1);
uint32x4_t vRes2 = internal::vroundq_u32_f32(vSt2);
uint32x4_t vRes3 = internal::vroundq_u32_f32(vSt3);
uint32x4_t vRes4 = internal::vroundq_u32_f32(vSt4);
int32x4_t vH_L = vmovl_s16(vget_low_s16(vDiff4));
int32x4_t vH_H = vmovl_s16(vget_high_s16(vDiff4));
uint32x4_t vDiff_Res1 = vmulq_u32(vDiffL, vRes1);
uint32x4_t vDiff_Res2 = vmulq_u32(vDiffH, vRes2);
uint32x4_t vDiff_Res3 = vmulq_u32(vreinterpretq_u32_s32(vH_L), vRes3);
uint32x4_t vDiff_Res4 = vmulq_u32(vreinterpretq_u32_s32(vH_H), vRes4);
int32x4_t vShift = vdupq_n_s32(vshift);
uint32x4_t vAddRes1 = vaddq_u32(vDiff_Res1, vreinterpretq_u32_s32(vShift));
uint32x4_t vAddRes2 = vaddq_u32(vDiff_Res2, vreinterpretq_u32_s32(vShift));
uint32x4_t vAddRes3 = vaddq_u32(vDiff_Res3, vreinterpretq_u32_s32(vShift));
uint32x4_t vAddRes4 = vaddq_u32(vDiff_Res4, vreinterpretq_u32_s32(vShift));
int16x4_t vShrRes1 = vshrn_n_s32(vreinterpretq_s32_u32(vAddRes1), 8);
int16x4_t vShrRes2 = vshrn_n_s32(vreinterpretq_s32_u32(vAddRes2), 8);
int16x4_t vShrRes3 = vshrn_n_s32(vreinterpretq_s32_u32(vAddRes3), 8);
int16x4_t vShrRes4 = vshrn_n_s32(vreinterpretq_s32_u32(vAddRes4), 8);
int16x8_t vc0 = vdupq_n_s16((s16)v0);
int8x8_t vShrRes1_s8 = vshrn_n_s16(vcombine_s16(vShrRes1, vShrRes2), 4);
uint16x8_t vCltRes_u16 = vcltq_s16(vcombine_s16(vShrRes3, vShrRes4), vc0);
int8x8_t vShrRes2_s8 = vshrn_n_s16(vcombine_s16(vShrRes3, vShrRes4), 4);
int8x8_t vCltRes_s8 = vmovn_s16(vreinterpretq_s16_u16(vCltRes_u16));
int8x8_t vcHRange = vdup_n_s8((s8)vhrange);
uint8x8_t vHResAdd = vand_u8(vreinterpret_u8_s8(vCltRes_s8), vreinterpret_u8_s8(vcHRange));
int8x8_t vHRes = vadd_s8(vShrRes2_s8, vreinterpret_s8_u8(vHResAdd));
uint8x8x3_t vHsv;
vHsv.val[0] = vreinterpret_u8_s8(vHRes);
vHsv.val[1] = vreinterpret_u8_s8(vShrRes1_s8);
vHsv.val[2] = vMax;
return vHsv;
}
const u8 fastSaturate8u[] =
{
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,  14,  15,
16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  28,  29,  30,  31,
32,  33,  34,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  45,  46,  47,
48,  49,  50,  51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,
64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,  78,  79,
80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,
96,  97,  98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,
112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127,
128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143,
144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159,
160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175,
176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191,
192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,
208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,
224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,
240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255, 255,
255
};
inline void convertToHSV(const s32 r, const s32 g, const s32 b,
const s32 &hrange, const s32 &hsv_shift,
u8* dst)
{
s32 h, s, v = b;
s32 vmin = b, diff;
s32 vr, vg;
v += fastSaturate8u[g-v+256];
v += fastSaturate8u[r-v+256];
vmin -= fastSaturate8u[vmin-g+256];
vmin -= fastSaturate8u[vmin-r+256];
diff = v - vmin;
vr = v == r ? -1 : 0;
vg = v == g ? -1 : 0;
s = (s32(diff * (255 << hsv_shift) * (1.0f/(f32)v)) + (1 << (hsv_shift-1))) >> hsv_shift;
h = (vr & (g - b)) + (~vr & ((vg & (b - r + 2 * diff)) + ((~vg) & (r - g + 4 * diff))));
h = ((h * s32((hrange << hsv_shift)/(6.f*diff) + 0.5)) + (1 << (hsv_shift-1))) >> hsv_shift;
h += h < 0 ? hrange : 0;
dst[0] = internal::saturate_cast<u8>(h);
dst[1] = (u8)s;
dst[2] = (u8)v;
}
#define CONVERT_TO_HSV_ASM(loadop, rreg, breg)                        \
__asm__ (                                                 \
#loadop    ", [%[in]] @RGB                       \n\t" \
"vmin.u8 d3, d0, d1      @VMin (d3)                 \n\t" \
"vmax.u8 d6, d0, d1      @V (d6)                    \n\t" \
"vmovl.u8 q2, " #rreg "  @V16_R (d4,d5)             \n\t" \
"vmovl.u8 q4, d1         @V16_G (d8,d9)             \n\t" \
"vmax.u8 d6, d6, d2                                 \n\t" \
"vmin.u8 d3, d3, d2                                 \n\t" \
"vmovl.u8 q0, " #breg "  @V16_B (d0,d1)             \n\t" \
"vsubl.u8 q8, d6, d3     @V16_Diff (d16,d17)        \n\t" \
\
"vmovl.u8 q5, d6         @V16_V (d10,d11)           \n\t" \
"vadd.s16 q10, q8, q8    @V16_Diff_2 (d20,d21)      \n\t" \
"vmovl.u16 q9, d16       @V32_Diff_L (d18,d19)      \n\t" \
"vmovl.u16 q11, d17      @V32_Diff_H (d22,d23)      \n\t" \
"vceq.u16 q12, q2, q5    @V==R(d24,d25)             \n\t" \
"vceq.u16 q13, q4, q5    @V==G(d26,d27)             \n\t" \
\
"vsub.s16 q8, q4, q0     @V16_G-B (d16,d17)         \n\t" \
"vmvn.u16 q15, q12       @V16~R                     \n\t" \
"vsub.s16 q6, q0, q2     @V16_B-R (d12,d13)         \n\t" \
"vsub.s16 q7, q2, q4     @V16_R-G (d14,d15)         \n\t" \
"vand.u16 q1, q13, q15   @VMask2                    \n\t" \
"vand.u16 q2, q8, q12    @V16_H(d4,d5)              \n\t" \
"vadd.s16 q4, q6, q10    @V16_H2                    \n\t" \
"vmvn.u16 q12, q13       @V16~G                     \n\t" \
"vadd.s16 q6, q10, q10   @VDiff16_4 (d12,d13)       \n\t" \
"vand.u16 q8, q15, q12   @VMask3                    \n\t" \
"vand.u16 q15, q4, q1    @vH2(d30,d31)              \n\t" \
"vadd.s16 q7, q7, q6     @V16_H3 (d14,d15)          \n\t" \
"vadd.s16 q14, q2, q15   @vH16                      \n\t" \
"vmovl.u16 q12, d10      @V32_V_L                   \n\t" \
"vand.s16 q7, q7, q8     @vH16                      \n\t" \
"vmovl.u16 q13, d11      @V32_V_H                   \n\t" \
"vadd.s16 q2, q14, q7    @V16_Diff_4                \n\t" \
\
"vdup.32 q4, %[v6]                                  \n\t" \
"vmul.u32 q14, q9, q4                               \n\t" \
"vmul.u32 q15, q11, q4                              \n\t" \
"vcvt.f32.u32 q4, q12     @VF1 (d8,d9)              \n\t" \
"vcvt.f32.u32 q8, q13     @VF2                      \n\t" \
"vcvt.f32.u32 q0, q14     @HF1                      \n\t" \
"vcvt.f32.u32 q1, q15     @HF2                      \n\t" \
"vrecpe.f32 q12, q4       @Vxinv                    \n\t" \
"vrecpe.f32 q13, q8       @Vxinv                    \n\t" \
"vrecpe.f32 q5, q0        @Vxinv                    \n\t" \
"vrecpe.f32 q7, q1        @Vxinv                    \n\t" \
"vrecps.f32 q14, q12, q4  @Vst1                     \n\t" \
"vrecps.f32 q15, q13, q8  @Vst1                     \n\t" \
"vrecps.f32 q10, q5, q0   @Vst1                     \n\t" \
"vrecps.f32 q6, q7, q1    @Vst1                     \n\t" \
"vmul.f32 q4, q12, q14                              \n\t" \
"vmul.f32 q8, q13, q15                              \n\t" \
"vmul.f32 q0, q5, q10                               \n\t" \
"vmul.f32 q1, q7, q6                                \n\t" \
"vdup.32 q12, %[vsdiv_table]                        \n\t" \
"vmul.f32 q14, q4, q12                              \n\t" \
"vmul.f32 q15, q8, q12                              \n\t" \
"vdup.32 q12, %[vhdiv_table]                        \n\t" \
"vmul.f32 q10, q0, q12                              \n\t" \
"vmul.f32 q6, q1, q12                               \n\t" \
\
"vdup.32 q12, %[bias]                               \n\t" \
\
"vadd.f32 q7, q14, q12                              \n\t" \
"vadd.f32 q13, q15, q12                             \n\t" \
"vcvt.u32.f32 q4, q7                                \n\t" \
"vcvt.u32.f32 q8, q13                               \n\t" \
\
"vadd.f32 q14, q10, q12                             \n\t" \
"vadd.f32 q7, q6, q12                               \n\t" \
"vcvt.u32.f32 q0, q14                               \n\t" \
"vcvt.u32.f32 q1, q7      @Vres                     \n\t" \
\
"vmovl.s16 q7, d4         @V32_H_L (d14,d15)        \n\t" \
"vmovl.s16 q5, d5         @V32_H_H (d10,d11)        \n\t" \
"vmul.u32 q14, q9, q4                               \n\t" \
"vmul.u32 q15, q11, q8                              \n\t" \
"vmul.u32 q10, q7, q0                               \n\t" \
"vmul.u32 q6, q5, q1                                \n\t" \
\
"vdup.32 q12, %[vshift]                             \n\t" \
"vadd.u32 q13, q14, q12                             \n\t" \
"vadd.u32 q8, q15, q12                              \n\t" \
"vadd.u32 q0, q10, q12                              \n\t" \
"vadd.u32 q1, q6, q12                               \n\t" \
"vshrn.s32 d8, q13, #8                              \n\t" \
"vshrn.s32 d9, q8, #8                               \n\t" \
"vshrn.s32 d10, q0, #8                              \n\t" \
"vshrn.s32 d11, q1, #8                              \n\t" \
\
"vdup.16 q8, %[v0]                                  \n\t" \
"vshrn.s16 d5, q4, #4                               \n\t" \
"vclt.s16 q9, q5, q8                                \n\t" \
"vshrn.s16 d4, q5, #4                               \n\t" \
\
"vmovn.s16 d9, q9                                   \n\t" \
"vdup.8 d7, %[vhrange]                              \n\t" \
"vand.u8 d10, d9, d7                                \n\t" \
"vadd.s8 d4, d4, d10                                \n\t" \
"vst3.8 {d4-d6}, [%[out]] @HSV                      \n\t" \
: /*no output*/                                           \
: [out] "r" (dst + dj), [in] "r" (src + sj),              \
[vsdiv_table] "r" (vsdiv_table),              \
[vshift] "r" (vshift),                        \
[vhdiv_table] "r" (vhdiv_table),              \
[v6] "r" (v6), [vhrange] "r" (vhrange),       \
[v0] "r" (v0), [bias] "r" (bias)              \
: "d0","d1","d2","d3","d4","d5","d6","d7",                \
"d8","d9","d10","d11","d12","d13","d14","d15",          \
"d16","d17","d18","d19","d20","d21","d22","d23",        \
"d24","d25","d26","d27","d28","d29","d30","d31"         \
);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
#define YCRCB_CONSTS                                                        \
register int16x4_t vcYR  asm ("d31") = vmov_n_s16(4899);                \
register int16x4_t vcYG  asm ("d30") = vmov_n_s16(9617);                \
register int16x4_t vcYB  asm ("d29") = vmov_n_s16(1868);                \
register int16x4_t vcCrG asm ("d28") = vmov_n_s16(6860);                \
register int16x4_t vcCrB asm ("d27") = vmov_n_s16(1332);                \
register int16x4_t vcCbR asm ("d26") = vmov_n_s16(2765);                \
register int16x4_t vcCbG asm ("d25") = vmov_n_s16(5427);
#else
#define YCRCB_CONSTS                                                        \
const s16       convertCoeffs[] = { 4899, 4899, 4899, 4899,             \
9617, 9617, 9617, 9617,             \
1868, 1868, 1868, 1868,             \
6860, 6860, 6860, 6860,             \
1332, 1332, 1332, 1332,             \
2765, 2765, 2765, 2765,             \
5427, 5427, 5427, 5427  };          \
const int16x8_t vcYRG  = vld1q_s16(convertCoeffs);      /*YR and YG*/   \
const int16x4_t vcYB   = vld1_s16(convertCoeffs + 8);   /*YB*/          \
const int16x8_t vcCrGB = vld1q_s16(convertCoeffs + 12); /*CrG and CrB*/ \
const int16x8_t vcCbRG = vld1q_s16(convertCoeffs + 20); /*CbR and CbG*/
#endif
#define CONVERTTOYCRCB(loadcmd, rreg, greg, breg)                           \
__asm__ (                                                               \
#loadcmd   ", [%[in]] @RGB                       \n\t"               \
"vmovl.u8 q2, " #rreg "  @R (d4,d5)                 \n\t"               \
"vmovl.u8 q3, " #greg "  @G (d6,d7)                 \n\t"               \
"vmovl.u8 q4, " #breg "  @B (d8,d9)                 \n\t"               \
\
"vshll.u16 q7, d4, #13   @Cr(q7,q8):  R             \n\t"               \
"vmull.u16 q5, d6, d30   @Y (q5,q6):  G             \n\t"               \
"vshll.u16 q9, d8, #13   @Cb(q9,q10): B             \n\t"               \
"vshll.u16 q8, d5, #13   @Cr(q7,q8):  R             \n\t"               \
"vmull.u16 q6, d7, d30   @Y (q5,q6):  G             \n\t"               \
"vshll.u16 q10, d9, #13  @Cb(q9,q10): B             \n\t"               \
\
"vmlsl.s16 q7, d6, d28   @Cr(q7,q8):  RG            \n\t"               \
"vmlal.s16 q5, d8, d29   @Y (q5,q6):  GB            \n\t"               \
"vmlsl.s16 q9, d4, d26   @Cb(q9,q10): BR            \n\t"               \
"vmlsl.s16 q8, d7, d28   @Cr(q7,q8):  RG            \n\t"               \
"vmlal.s16 q6, d9, d29   @Y (q5,q6):  GB            \n\t"               \
"vmlsl.s16 q10, d5, d26  @Cb(q9,q10): BR            \n\t"               \
\
"vmlsl.s16 q7, d8, d27   @Cr(q7,q8):  RGB           \n\t"               \
"vmlal.s16 q5, d4, d31   @Y (q5,q6):  GBR           \n\t"               \
"vmlsl.s16 q9, d6, d25   @Cb(q9,q10): BRG           \n\t"               \
"vmlsl.s16 q8, d9, d27   @Cr(q7,q8):  RGB           \n\t"               \
"vmlal.s16 q6, d5, d31   @Y (q5,q6):  GBR           \n\t"               \
"vmlsl.s16 q10, d7, d25  @Cb(q9,q10): BRG           \n\t"               \
\
"vrshrn.s32 d4, q7, #14  @Cr -> q2                  \n\t"               \
"vrshrn.s32 d8, q5, #14  @Y  -> q4                  \n\t"               \
"vrshrn.s32 d6, q9, #14  @Cb -> q3                  \n\t"               \
"vrshrn.s32 d5, q8, #14  @Cr -> q2                  \n\t"               \
"vrshrn.s32 d9, q6, #14  @Y  -> q4                  \n\t"               \
"vrshrn.s32 d7, q10, #14 @Cb -> q3                  \n\t"               \
\
"vmov.s16 q5, #128                                  \n\t"               \
"vmov.s16 q6, #128                                  \n\t"               \
"vadd.i16 q5, q2         @Cr -> q5                  \n\t"               \
"vadd.i16 q6, q3         @Cb -> q6                  \n\t"               \
\
"vqmovn.u16 d4, q4                                  \n\t"               \
"vqmovun.s16 d5, q5                                 \n\t"               \
"vqmovun.s16 d6, q6                                 \n\t"               \
\
"vst3.8 {d4-d6}, [%[out]]                           \n\t"               \
: /*no output*/                                                         \
: [out] "r" (dst + dj), [in] "r" (src + sj),                            \
"w" (vcYR), "w" (vcYG), "w" (vcYB),                                   \
"w" (vcCrB), "w" (vcCrG), "w" (vcCbG), "w" (vcCbR)                    \
: "d0","d1","d2","d3","d4","d5","d6","d7",                              \
"d8","d9","d10","d11","d12","d13","d14","d15",                        \
"d16","d17","d18","d19","d20","d21"                                   \
);

inline uint8x8x3_t convertToYCrCb( const int16x8_t& vR, const int16x8_t& vG, const int16x8_t& vB,
const int16x8_t& vcYRG, const int16x4_t& vcYB,
const int16x8_t& vcCrGB, const int16x8_t& vcCbRG )
{
int32x4_t vCrL = vshll_n_s16(vget_low_s16(vR), 13);                  // R
int32x4_t vCrH = vshll_n_s16(vget_high_s16(vR), 13);                 // R
int32x4_t vYL  = vmull_s16(vget_low_s16(vG), vget_high_s16(vcYRG));  // G
int32x4_t vYH  = vmull_s16(vget_high_s16(vG), vget_high_s16(vcYRG)); // G
int32x4_t vCbL = vshll_n_s16(vget_low_s16(vB), 13);                  // B
int32x4_t vCbH = vshll_n_s16(vget_high_s16(vB), 13);                 // B
vCrL = vmlsl_s16(vCrL, vget_low_s16(vG), vget_low_s16(vcCrGB));      // RG
vCrH = vmlsl_s16(vCrH, vget_high_s16(vG), vget_low_s16(vcCrGB));     // RG
vYL  = vmlal_s16(vYL, vget_low_s16(vB), vcYB);                       // GB
vYH  = vmlal_s16(vYH, vget_high_s16(vB), vcYB);                      // GB
vCbL = vmlsl_s16(vCbL, vget_low_s16(vR), vget_low_s16(vcCbRG));      // BR
vCbH = vmlsl_s16(vCbH, vget_high_s16(vR), vget_low_s16(vcCbRG));     // BR
vCrL = vmlsl_s16(vCrL, vget_low_s16(vB), vget_high_s16(vcCrGB));     // RGB
vCrH = vmlsl_s16(vCrH, vget_high_s16(vB), vget_high_s16(vcCrGB));    // RGB
vYL  = vmlal_s16(vYL, vget_low_s16(vR), vget_low_s16(vcYRG));        // GBR
vYH  = vmlal_s16(vYH, vget_high_s16(vR), vget_low_s16(vcYRG));       // GBR
vCbL = vmlsl_s16(vCbL, vget_low_s16(vG), vget_high_s16(vcCbRG));     // BRG
vCbH = vmlsl_s16(vCbH, vget_high_s16(vG), vget_high_s16(vcCbRG));    // BRG
int16x4_t vCrL_ = vrshrn_n_s32(vCrL, 14);
int16x4_t vCrH_ = vrshrn_n_s32(vCrH, 14);
int16x4_t vYL_  = vrshrn_n_s32(vYL, 14);
int16x4_t vYH_  = vrshrn_n_s32(vYH, 14);
int16x4_t vCbL_ = vrshrn_n_s32(vCbL, 14);
int16x4_t vCbH_ = vrshrn_n_s32(vCbH, 14);
int16x8_t vCr = vmovq_n_s16(128);
int16x8_t vCb = vmovq_n_s16(128);
vCr = vaddq_s16(vCr, vcombine_s16(vCrL_, vCrH_));
vCb = vaddq_s16(vCb, vcombine_s16(vCbL_, vCbH_));
uint8x8x3_t vYCrCb;
vYCrCb.val[0] = vqmovn_u16(vreinterpretq_u16_s16(vcombine_s16(vYL_, vYH_)));
vYCrCb.val[1] = vqmovun_s16(vCr);
vYCrCb.val[2] = vqmovun_s16(vCb);
return vYCrCb;
}
#define S_CONVERTTOYCRCB(R, G, B)                                           \
s32 Y =         (R * 4899    + G * 9617 + B * 1868 + (1 << 13)) >> 14;  \
s32 Cr = 128 + ((R * 8192    - G * 6860 - B * 1332 + (1 << 13)) >> 14); \
s32 Cb = 128 + ((R * (-2765) - G * 5427 + B * 8192 + (1 << 13)) >> 14); \
dst[dj + 0] = internal::saturate_cast<u8>(Y);                           \
dst[dj + 1] = internal::saturate_cast<u8>(Cr);                          \
dst[dj + 2] = internal::saturate_cast<u8>(Cb);
#define COEFF_Y   (   149)
#define COEFF_BU  (   129)
#define COEFF_RV  (   102)
#define COEFF_GU  (    25)
#define COEFF_GV  (    52)
#define COEFF_R   (-14248)
#define COEFF_G   (  8663)
#define COEFF_B   (-17705)
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
#define YUV420ALPHA3_CONST
#define YUV420ALPHA4_CONST register uint8x16_t c255  asm ("q13") = vmovq_n_u8(255);
#define YUV420ALPHA3_CONVERT
#define YUV420ALPHA4_CONVERT , "w" (c255)
#define YUV420STORE1CMD3 "vst3.8 {d20, d22, d24}"
#define YUV420STORE2CMD3 "vst3.8 {d21, d23, d25}"
#define YUV420STORE1CMD4 "vst4.8 {d20, d22, d24, d26}"
#define YUV420STORE2CMD4 "vst4.8 {d21, d23, d25, d27}"
#define YUV420_CONSTS(cn, bIdx, vIdx)                            \
register const s32 cR = s16(COEFF_R);                        \
register const s32 cG = s16(COEFF_G);                        \
register const s32 cB = s16(COEFF_B);                        \
\
register uint8x16_t vc16  asm ("q15") = vmovq_n_u8(16);      \
register uint8x8_t cGU    asm ("d14") = vmov_n_u8(COEFF_GU); \
register uint8x8_t cGV    asm ("d15") = vmov_n_u8(COEFF_GV); \
register uint8x8_t cRV    asm ("d16") = vmov_n_u8(COEFF_RV); \
register uint8x8_t cBU    asm ("d17") = vmov_n_u8(COEFF_BU); \
register uint8x16_t cRGBY asm ("q3")  = vmovq_n_u8(COEFF_Y); \
YUV420ALPHA##cn##_CONST
#define CONVERTYUV420TORGB(cn, ureg, vreg, rreg, breg)                                                    \
__asm__ (                                                                                             \
"vld2.8 {d0-d1}, [%[inUV]]                      @UV                                         \n\t" \
"vdup.16 q4, %[cG]                              @cG                                         \n\t" \
"vld2.8 {d2-d3}, [%[inY1]]                      @YY                                         \n\t" \
"vdup.16 "#rreg", %[cR]                         @cR                                         \n\t" \
"vld2.8 {d4-d5}, [%[inY2]]                      @YY                                         \n\t" \
"vdup.16 "#breg", %[cB]                         @cB                                         \n\t" \
"vmlsl.u8 q4, "#ureg", d14                      @cG-25u                                     \n\t" \
"vmax.u8 q1, q15                                @max(Y,16)                                  \n\t" \
"vmlal.u8 "#rreg", "#vreg", d16                 @cR+102*v                                   \n\t" \
"vmlal.u8 "#breg", "#ureg", d17                 @cB+129*u                                   \n\t" \
"vmax.u8 q2, q15                                @max(Y,16)                                  \n\t" \
"vmlsl.u8 q4, "#vreg", d15                      @cG-25u-52v                                 \n\t" \
/*q10,q11,q12,q13 - for output*/ \
"vmull.u8 q9, d3, d6                            @h 149*y                                    \n\t" \
"vmull.u8 q10, d2, d7                           @l 149*y                                    \n\t" \
"vshr.u16 q9, #1                                @h (149*y)/2                                \n\t" \
"vshr.u16 q10, #1                               @l (149*y)/2                                \n\t" \
\
"vhadd.s16 q0, q9, q4                           @hG ((149*y)/2 + cG - 25*u - 52*v)/2        \n\t" \
"vhadd.s16 q12, q10, q6                         @lB ((149*y)/2 + cB + 129*u)/2              \n\t" \
"vhadd.s16 q1, q9, q5                           @hR ((149*y)/2 + cR + 102*v)/2              \n\t" \
"vhadd.s16 q11, q10, q4                         @lG ((149*y)/2 + cG - 25*u - 52*v)/2        \n\t" \
"vhadd.s16 q9, q6                               @hB ((149*y)/2 + cB + 129*u)/2              \n\t" \
"vhadd.s16 q10, q5                              @lR ((149*y)/2 + cR + 102*v)/2              \n\t" \
\
"vqrshrun.s16 d24, q12, #5                      @lB ((149*y)/2 + cB + 129*u)/2/32           \n\t" \
"vqrshrun.s16 d22, q11, #5                      @lG ((149*y)/2 + cG - 25*u - 52*v)/2/32     \n\t" \
"vqrshrun.s16 d20, q10, #5                      @lR ((149*y)/2 + cR + 102*v)/2/32           \n\t" \
"vqrshrun.s16 d23, q0, #5                       @hG ((149*y)/2 + cG - 25*u - 52*v)/2/32     \n\t" \
"vqrshrun.s16 d21, q1, #5                       @hR ((149*y)/2 + cR + 102*v)/2/32           \n\t" \
"vqrshrun.s16 d25, q9, #5                       @hB ((149*y)/2 + cB + 129*u)/2/32           \n\t" \
\
"vzip.8 d22, d23                                @G                \n\t"                           \
"vzip.8 d20, d21                                @R                \n\t"                           \
"vzip.8 d24, d25                                @B                \n\t"                           \
\
YUV420STORE1CMD##cn", [%[out1]]                                \n\t"                              \
YUV420STORE2CMD##cn", [%[out1x]]                               \n\t"                              \
\
"vmull.u8 q9, d5, d6                            @h 149*y                \n\t"                     \
"vmull.u8 q10, d4, d7                           @l 149*y                \n\t"                     \
"vshr.u16 q9, #1                                @h (149*y)/2            \n\t"                     \
"vshr.u16 q10, #1                               @l (149*y)/2            \n\t"                     \
\
"vhadd.s16 q0, q9, q4                           @hG ((149*y)/2 + cG - 25*u - 52*v)/2        \n\t" \
"vhadd.s16 q12, q10, q6                         @lB ((149*y)/2 + cB + 129*u)/2              \n\t" \
"vhadd.s16 q1, q9, q5                           @hR ((149*y)/2 + cR + 102*v)/2              \n\t" \
"vhadd.s16 q11, q10, q4                         @lG ((149*y)/2 + cG - 25*u - 52*v)/2        \n\t" \
"vhadd.s16 q9, q6                               @hB ((149*y)/2 + cB + 129*u)/2              \n\t" \
"vhadd.s16 q10, q5                              @lR ((149*y)/2 + cR + 102*v)/2              \n\t" \
\
"vqrshrun.s16 d24, q12, #5                      @lB ((149*y)/2 + cB + 129*u)/2/32           \n\t" \
"vqrshrun.s16 d22, q11, #5                      @lG ((149*y)/2 + cG - 25*u - 52*v)/2/32     \n\t" \
"vqrshrun.s16 d20, q10, #5                      @lR ((149*y)/2 + cR + 102*v)/2/32           \n\t" \
"vqrshrun.s16 d23, q0, #5                       @hG ((149*y)/2 + cG - 25*u - 52*v)/2/32     \n\t" \
"vqrshrun.s16 d21, q1, #5                       @hR ((149*y)/2 + cR + 102*v)/2/32           \n\t" \
"vqrshrun.s16 d25, q9, #5                       @hB ((149*y)/2 + cB + 129*u)/2/32           \n\t" \
\
"vzip.8 d22, d23                                @G                \n\t"                           \
"vzip.8 d20, d21                                @R                \n\t"                           \
"vzip.8 d24, d25                                @B                \n\t"                           \
\
YUV420STORE1CMD##cn", [%[out2]]                                \n\t"                              \
YUV420STORE2CMD##cn", [%[out2x]]                               \n\t"                              \
\
: /*no output*/                                                                                   \
: [out1] "r" (dst1 + dj), [out2] "r" (dst2 + dj),                                                 \
[out1x] "r" (dst1 + dj+cn*8), [out2x] "r" (dst2 + dj+cn*8),                                     \
[inUV] "r" (uv+j), [inY1] "r" (y1+j), [inY2] "r" (y2+j),                                        \
[cR] "r" (cR), [cG] "r" (cG), [cB] "r" (cB),                                                    \
"w" (vc16), "w" (cGU), "w" (cGV), "w" (cBU), "w" (cRV), "w" (cRGBY) YUV420ALPHA##cn##_CONVERT   \
: "d0","d1","d2","d3","d4","d5","d8","d9","d10","d11","d12",                                      \
"d13","d18","d19","d20","d21","d22","d23","d24","d25"                                           \
);
#else
template<int bIdx>
struct _convertYUV420Internals
{
uint16x8_t vc14216;
uint16x8_t vc17672;
uint16x8_t vc8696;
uint8x8_t  vc102;
uint8x8_t  vc25;
uint8x8_t  vc129;
uint8x8_t  vc52;
uint16x8_t vc_1;
uint8x8_t  vc149;
uint8x8_t  vc16;
_convertYUV420Internals()
{
vc14216 = vdupq_n_u16(-COEFF_R);
vc17672 = vdupq_n_u16(-COEFF_B);
vc8696  = vdupq_n_u16(COEFF_G);
vc102   = vdup_n_u8(COEFF_RV);
vc25    = vdup_n_u8(COEFF_GU);
vc129   = vdup_n_u8(COEFF_BU);
vc52    = vdup_n_u8(COEFF_GV);
vc_1    = vdupq_n_u16((uint16_t)-1);
vc149   = vdup_n_u8(COEFF_Y);
vc16    = vdup_n_u8(16);
}
inline void UVrgbToRGB( const int16x8_t &ruv, const int16x8_t &guv, const int16x8_t &buv,
const u8 *y, uint8x16x3_t &rgbl )
{
//y get line
uint8x8x2_t yl = vld2_u8(y);
yl.val[0] = vmax_u8(yl.val[0], vc16);
yl.val[1] = vmax_u8(yl.val[1], vc16);
//y part line
uint16x8_t yodd1 = vmlal_u8(vc_1, yl.val[0], vc149); //(-1+149*y)
uint16x8_t yevn1 = vmlal_u8(vc_1, yl.val[1], vc149); //(-1+149*y)
int16x8_t yodd1h = (int16x8_t)vshrq_n_u16(yodd1, 1);  //(-1+149*y)/2
int16x8_t yevn1h = (int16x8_t)vshrq_n_u16(yevn1, 1);  //(-1+149*y)/2
//y line calc rgb
int16x8_t rodd1w = vhsubq_s16(yodd1h, ruv); //((-1+149*y)/2 - (14216-102*v))/2
int16x8_t gevn1w = vhaddq_s16(yevn1h, guv); //((-1+149*y)/2 + ((8696-25*u)-52*v))/2
int16x8_t bodd1w = vhsubq_s16(yodd1h, buv); //((-1+149*y)/2 - (17672-129*u))/2
int16x8_t revn1w = vhsubq_s16(yevn1h, ruv); //((-1+149*y)/2 - (14216-102*v))/2
int16x8_t godd1w = vhaddq_s16(yodd1h, guv); //((-1+149*y)/2 + ((8696-25*u)-52*v))/2
int16x8_t bevn1w = vhsubq_s16(yevn1h, buv); //((-1+149*y)/2 - (17672-129*u))/2
//y line clamp + narrow
uint8x8_t rodd1n = vqshrun_n_s16(rodd1w, 5);
uint8x8_t revn1n = vqshrun_n_s16(revn1w, 5);
uint8x8_t godd1n = vqshrun_n_s16(godd1w, 5);
uint8x8x2_t r1 = vzip_u8 (rodd1n, revn1n);
uint8x8_t gevn1n = vqshrun_n_s16(gevn1w, 5);
uint8x8_t bodd1n = vqshrun_n_s16(bodd1w, 5);
uint8x8x2_t g1 = vzip_u8 (godd1n, gevn1n);
uint8x8_t bevn1n = vqshrun_n_s16(bevn1w, 5);
uint8x8x2_t b1 = vzip_u8 (bodd1n, bevn1n);
rgbl.val[2 - bIdx] = vcombine_u8(r1.val[0], r1.val[1]);
rgbl.val[1]        = vcombine_u8(g1.val[0], g1.val[1]);
rgbl.val[0 + bIdx] = vcombine_u8(b1.val[0], b1.val[1]);
}
};
template<int cn, int bIdx, int vIdx>
struct _convertYUV420
{
_convertYUV420Internals<bIdx> convertYUV420Internals;
inline void ToRGB( const u8 *y1, const u8 *y2, const u8 *uv,
u8 *dst1, u8 *dst2 )
{
uint8x8x2_t raw_uv = vld2_u8(uv);
uint16x8_t gu =            vmlsl_u8(convertYUV420Internals.vc8696,  raw_uv.val[1-vIdx], convertYUV420Internals.vc25);  //(8696-25*u)
int16x8_t ruv = (int16x8_t)vmlsl_u8(convertYUV420Internals.vc14216, raw_uv.val[vIdx], convertYUV420Internals.vc102); //(14216-102*v)
int16x8_t buv = (int16x8_t)vmlsl_u8(convertYUV420Internals.vc17672, raw_uv.val[1-vIdx], convertYUV420Internals.vc129); //(17672-129*u)
int16x8_t guv = (int16x8_t)vmlsl_u8(gu,      raw_uv.val[vIdx], convertYUV420Internals.vc52);  //((8696-25*u)-52*v))
uint8x16x3_t rgbl;
//y line1
convertYUV420Internals.UVrgbToRGB(ruv, guv, buv, y1, rgbl);
vst3q_u8(dst1, rgbl);
//y line2
convertYUV420Internals.UVrgbToRGB(ruv, guv, buv, y2, rgbl);
vst3q_u8(dst2, rgbl);
}
};
template<int bIdx, int vIdx>
struct _convertYUV420<4, bIdx, vIdx>
{
_convertYUV420Internals<bIdx> convertYUV420Internals;
inline void ToRGB( const u8 *y1, const u8 *y2, const u8 *uv,
u8 *dst1, u8 *dst2 )
{
uint8x8x2_t raw_uv = vld2_u8(uv);
uint16x8_t gu =            vmlsl_u8(convertYUV420Internals.vc8696,  raw_uv.val[1-vIdx], convertYUV420Internals.vc25);  //(8696-25*u)
int16x8_t ruv = (int16x8_t)vmlsl_u8(convertYUV420Internals.vc14216, raw_uv.val[vIdx], convertYUV420Internals.vc102); //(14216-102*v)
int16x8_t buv = (int16x8_t)vmlsl_u8(convertYUV420Internals.vc17672, raw_uv.val[1-vIdx], convertYUV420Internals.vc129); //(17672-129*u)
int16x8_t guv = (int16x8_t)vmlsl_u8(gu,      raw_uv.val[vIdx], convertYUV420Internals.vc52);  //((8696-25*u)-52*v))
union { uint8x16x4_t v4; uint8x16x3_t v3; } rgbl;
rgbl.v4.val[3] = vdupq_n_u8(0xff);
//y line1
convertYUV420Internals.UVrgbToRGB(ruv, guv, buv, y1, rgbl.v3);
vst4q_u8(dst1, rgbl.v4);
//y line2
convertYUV420Internals.UVrgbToRGB(ruv, guv, buv, y2, rgbl.v3);
vst4q_u8(dst2, rgbl.v4);
}
};
#define YUV420_CONSTS(cn, bIdx, vIdx) _convertYUV420<cn, bIdx, vIdx> convertYUV420;
#endif
template <int cn> inline void fillAlpha(u8 *, u8 *){}
template <> inline void fillAlpha<4>(u8 *dst1, u8 *dst2)
{
dst1[3] = 255;
dst1[7] = 255;
dst2[3] = 255;
dst2[7] = 255;
}
template <int cn, int bIdx, int vIdx>
inline void convertYUV420ToRGB(const u8 *y1, const u8 *y2, const u8 *uv, u8* dst1, u8 *dst2)
{
int Y11 = y1[0];
int Y12 = y1[1];
int Y21 = y2[0];
int Y22 = y2[1];
int U = uv[1 - vIdx];
int V = uv[vIdx];
int y11 = (COEFF_Y * std::max(16, Y11)) >> 1;
int y12 = (COEFF_Y * std::max(16, Y12)) >> 1;
int y21 = (COEFF_Y * std::max(16, Y21)) >> 1;
int y22 = (COEFF_Y * std::max(16, Y22)) >> 1;
int uvR = COEFF_R +                COEFF_RV * V;
int uvG = COEFF_G - COEFF_GU * U - COEFF_GV * V;
int uvB = COEFF_B + COEFF_BU * U;
dst1[2-bIdx] = internal::saturate_cast<u8>((((y11 + uvR) >> 1) + (1 << 4)) >> 5);
dst1[1] = internal::saturate_cast<u8>((((y11 + uvG) >> 1) + (1 << 4)) >> 5);
dst1[bIdx] = internal::saturate_cast<u8>((((y11 + uvB) >> 1) + (1 << 4)) >> 5);
dst1[cn+2-bIdx] = internal::saturate_cast<u8>((((y12 + uvR) >> 1) + (1 << 4)) >> 5);
dst1[cn+1] = internal::saturate_cast<u8>((((y12 + uvG) >> 1) + (1 << 4)) >> 5);
dst1[cn+bIdx] = internal::saturate_cast<u8>((((y12 + uvB) >> 1) + (1 << 4)) >> 5);
dst2[2-bIdx] = internal::saturate_cast<u8>((((y21 + uvR) >> 1) + (1 << 4)) >> 5);
dst2[1] = internal::saturate_cast<u8>((((y21 + uvG) >> 1) + (1 << 4)) >> 5);
dst2[bIdx] = internal::saturate_cast<u8>((((y21 + uvB) >> 1) + (1 << 4)) >> 5);
dst2[cn+2-bIdx] = internal::saturate_cast<u8>((((y22 + uvR) >> 1) + (1 << 4)) >> 5);
dst2[cn+1] = internal::saturate_cast<u8>((((y22 + uvG) >> 1) + (1 << 4)) >> 5);
dst2[cn+bIdx] = internal::saturate_cast<u8>((((y22 + uvB) >> 1) + (1 << 4)) >> 5);
fillAlpha<cn>(dst1, dst2);
}
// converts R, G, B (B, G, R) pixels to  RGB(BGR)565 format respectively
inline uint8x16x2_t convertTo565( const uint8x16_t& vR, const uint8x16_t& vG, const uint8x16_t& vB )
{
uint8x16x2_t vRgb565;                               // rrrrRRRR ggggGGGG bbbbBBBB
vRgb565.val[1] = vsriq_n_u8(vB, vG, 5);             // xxxxxxxx bbbbBggg
vRgb565.val[0] = vshlq_n_u8(vG, 3);                 // gGGGG000 bbbbBggg
vRgb565.val[0] = vsriq_n_u8(vRgb565.val[0], vR, 3); // gGGrrrrR bbbbBggg
return vRgb565;
}
inline void convertTo565( const u16 R, const u16 G, const u16 B, u8 * dst )
{
*((u16*)dst) = (R >> 3)|((G&~3) << 3)|((B&~7) << 8);
}
#endif
} //namespace
void rgb2hsv(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
s32 hrange)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
const s32 hsv_shift = 12;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register const f32 vsdiv_table = f32(255 << hsv_shift);
register f32 vhdiv_table = f32(hrange << hsv_shift);
register const s32 vhrange = hrange;
register const s32 v0 = s32(0);
register const s32 vshift = s32(1 << (hsv_shift-1));
register const s32 v6 = s32(6);
register const f32 bias = 0.5f;
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 24, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERT_TO_HSV_ASM(vld3.8 {d0-d2}, d0, d2)
#else
uint8x8x3_t vRgb = vld3_u8(src + sj);
uint8x8x3_t vHsv = convertToHSV(vRgb.val[0], vRgb.val[1], vRgb.val[2], hrange);
vst3_u8(dst + dj, vHsv);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 3)
{
convertToHSV(src[sj], src[sj+1], src[sj+2], hrange, hsv_shift, dst+dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)hrange;
#endif
}
void rgbx2hsv(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
s32 hrange)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
const s32 hsv_shift = 12;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register const f32 vsdiv_table = f32(255 << hsv_shift);
register f32 vhdiv_table = f32(hrange << hsv_shift);
register const s32 vhrange = hrange;
register const s32 v0 = s32(0);
register const s32 vshift = s32(1 << (hsv_shift-1));
register const s32 v6 = s32(6);
register const f32 bias = 0.5f;
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERT_TO_HSV_ASM(vld4.8 {d0-d3}, d0, d2)
#else
uint8x8x4_t vRgb = vld4_u8(src + sj);
uint8x8x3_t vHsv = convertToHSV(vRgb.val[0], vRgb.val[1], vRgb.val[2], hrange);
vst3_u8(dst + dj, vHsv);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 3)
{
convertToHSV(src[sj], src[sj+1], src[sj+2], hrange, hsv_shift, dst+dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)hrange;
#endif
}
void bgr2hsv(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
s32 hrange)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
const s32 hsv_shift = 12;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register const f32 vsdiv_table = f32(255 << hsv_shift);
register f32 vhdiv_table = f32(hrange << hsv_shift);
register const s32 vhrange = hrange;
register const s32 v0 = s32(0);
register const s32 vshift = s32(1 << (hsv_shift-1));
register const s32 v6 = s32(6);
register const f32 bias = 0.5f;
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 24, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERT_TO_HSV_ASM(vld3.8 {d0-d2}, d2, d0)
#else
uint8x8x3_t vRgb = vld3_u8(src + sj);
uint8x8x3_t vHsv = convertToHSV(vRgb.val[2], vRgb.val[1], vRgb.val[0], hrange);
vst3_u8(dst + dj, vHsv);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 3)
{
convertToHSV(src[sj+2], src[sj+1], src[sj], hrange, hsv_shift, dst+dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)hrange;
#endif
}
void bgrx2hsv(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
s32 hrange)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
const s32 hsv_shift = 12;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
register const f32 vsdiv_table = f32(255 << hsv_shift);
register f32 vhdiv_table = f32(hrange << hsv_shift);
register const s32 vhrange = hrange;
register const s32 v0 = s32(0);
register const s32 vshift = s32(1 << (hsv_shift-1));
register const s32 v6 = s32(6);
register const f32 bias = 0.5f;
#endif
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERT_TO_HSV_ASM(vld4.8 {d0-d3}, d2, d0)
#else
uint8x8x4_t vRgb = vld4_u8(src + sj);
uint8x8x3_t vHsv = convertToHSV(vRgb.val[2], vRgb.val[1], vRgb.val[0], hrange);
vst3_u8(dst + dj, vHsv);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 3)
{
convertToHSV(src[sj+2], src[sj+1], src[sj], hrange, hsv_shift, dst+dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)hrange;
#endif
}
void rgbx2bgr565(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw16; sj += 64, dj += 32, j += 16)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld4.8 {d2, d4, d6, d8}, [%[in0]]        @  q0       q1       q2       q3       q4       \n\t"
"vld4.8 {d3, d5, d7, d9}, [%[in1]]        @  xxxxxxxx rrrrRRRR ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vsri.8 q1, q2, #5                        @  xxxxxxxx rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vshl.u8 q0, q2, #3                       @  gGGGG000 rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vsri.8 q0, q3, #3                        @  gGGbbbbB rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vst2.8 {d0, d2}, [%[out0]]                                                               \n\t"
"vst2.8 {d1, d3}, [%[out1]]                                                               \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[out1] "r" (dst + dj + 16),
[in0]  "r" (src + sj),
[in1]  "r" (src + sj + 32)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9"
);
#else
uint8x16x4_t vRgba = vld4q_u8(src + sj);
uint8x16x2_t vVal565 = convertTo565(vRgba.val[2], vRgba.val[1], vRgba.val[0]);
vst2q_u8(dst + dj, vVal565);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 2)
{
convertTo565(src[sj + 2], src[sj + 1], src[sj], dst + dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2bgr565(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw16; sj += 48, dj += 32, j += 16)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld3.8 {d2, d4, d6}, [%[in0]]       @  q0       q1       q2       q3       q4       \n\t"
"vld3.8 {d3, d5, d7}, [%[in1]]       @  xxxxxxxx rrrrRRRR ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vsri.8 q1, q2, #5                   @  xxxxxxxx rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vshl.u8 q0, q2, #3                  @  gGGGG000 rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vsri.8 q0, q3, #3                   @  gGGbbbbB rrrrRggg ggggGGGG bbbbBBBB xxxxxxxx \n\t"
"vst2.8 {d0, d2}, [%[out0]]                                                          \n\t"
"vst2.8 {d1, d3}, [%[out1]]                                                          \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[out1] "r" (dst + dj + 16),
[in0]  "r" (src + sj),
[in1]  "r" (src + sj + 24)
: "d0","d1","d2","d3","d4","d5","d6","d7"
);
#else
uint8x16x3_t vRgba = vld3q_u8(src + sj);
uint8x16x2_t vVal565 = convertTo565(vRgba.val[2], vRgba.val[1], vRgba.val[0]);
vst2q_u8(dst + dj, vVal565);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 2)
{
convertTo565(src[sj + 2], src[sj + 1], src[sj], dst + dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2rgb565(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw16; sj += 64, dj += 32, j += 16)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld4.8 {d0, d2, d4, d6}, [%[in0]]    @  q0       q1       q2       q3         \n\t"
"vld4.8 {d1, d3, d5, d7}, [%[in1]]    @  rrrrRRRR ggggGGGG bbbbBBBB aaaaAAAA   \n\t"
"vsri.8 q2, q1, #5                    @  rrrrRRRR ggggGGGG bbbbBggg aaaaAAAA   \n\t"
"vshl.u8 q1, #3                       @  rrrrRRRR gGGGG000 bbbbBggg aaaaAAAA   \n\t"
"vsri.8 q1, q0, #3                    @  rrrrRRRR gGGrrrrR bbbbBggg aaaaAAAA   \n\t"
"vst2.8 {d2, d4}, [%[out0]]                                                    \n\t"
"vst2.8 {d3, d5}, [%[out1]]                                                    \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[out1] "r" (dst + dj + 16),
[in0]  "r" (src + sj),
[in1]  "r" (src + sj + 32)
: "d0","d1","d2","d3","d4","d5","d6","d7"
);
#else
uint8x16x4_t vRgba = vld4q_u8(src + sj);
uint8x16x2_t vVal565 = convertTo565(vRgba.val[0], vRgba.val[1], vRgba.val[2]);
vst2q_u8(dst + dj, vVal565);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 2)
{
convertTo565(src[sj], src[sj + 1], src[sj + 2], dst + dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2rgb565(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw16; sj += 48, dj += 32, j += 16)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ (
"vld3.8 {d0, d2, d4}, [%[in0]]        @  q0       q1       q2       q3         \n\t"
"vld3.8 {d1, d3, d5}, [%[in1]]        @  rrrrRRRR ggggGGGG bbbbBBBB xxxxxxxx   \n\t"
"vsri.8 q2, q1, #5                    @  rrrrRRRR ggggGGGG bbbbBggg xxxxxxxx   \n\t"
"vshl.u8 q1, #3                       @  rrrrRRRR gGGGG000 bbbbBggg xxxxxxxx   \n\t"
"vsri.8 q1, q0, #3                    @  rrrrRRRR gGGrrrrR bbbbBggg xxxxxxxx   \n\t"
"vst2.8 {d2, d4}, [%[out0]]                                                    \n\t"
"vst2.8 {d3, d5}, [%[out1]]                                                    \n\t"
: /*no output*/
: [out0] "r" (dst + dj),
[out1] "r" (dst + dj + 16),
[in0]  "r" (src + sj),
[in1]  "r" (src + sj + 24)
: "d0","d1","d2","d3","d4","d5"
);
#else
uint8x16x3_t vRgba = vld3q_u8(src + sj);
uint8x16x2_t vVal565 = convertTo565(vRgba.val[0], vRgba.val[1], vRgba.val[2]);
vst2q_u8(dst + dj, vVal565);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 2)
{
convertTo565(src[sj], src[sj + 1], src[sj + 2], dst + dj);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgb2ycrcb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YCRCB_CONSTS
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 24, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTTOYCRCB(vld3.8 {d0-d2}, d0, d1, d2)
#else
uint8x8x3_t vRgb = vld3_u8(src + sj);
int16x8_t vR = vreinterpretq_s16_u16(vmovl_u8(vRgb.val[0]));
int16x8_t vG = vreinterpretq_s16_u16(vmovl_u8(vRgb.val[1]));
int16x8_t vB = vreinterpretq_s16_u16(vmovl_u8(vRgb.val[2]));
uint8x8x3_t vYCrCb = convertToYCrCb(vR, vG, vB, vcYRG, vcYB, vcCrGB, vcCbRG);
vst3_u8(dst + dj, vYCrCb);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 3)
{
S_CONVERTTOYCRCB(src[sj], src[sj + 1], src[sj + 2]);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void rgbx2ycrcb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YCRCB_CONSTS
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTTOYCRCB(vld4.8 {d0-d3}, d0, d1, d2)
#else
uint8x8x4_t vRgba = vld4_u8(src + sj);
int16x8_t vR = vreinterpretq_s16_u16(vmovl_u8(vRgba.val[0]));
int16x8_t vG = vreinterpretq_s16_u16(vmovl_u8(vRgba.val[1]));
int16x8_t vB = vreinterpretq_s16_u16(vmovl_u8(vRgba.val[2]));
uint8x8x3_t vYCrCb = convertToYCrCb(vR, vG, vB, vcYRG, vcYB, vcCrGB, vcCbRG);
vst3_u8(dst + dj, vYCrCb);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 3)
{
S_CONVERTTOYCRCB(src[sj], src[sj + 1], src[sj + 2]);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void bgr2ycrcb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YCRCB_CONSTS
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 24, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTTOYCRCB(vld3.8 {d0-d2}, d2, d1, d0)
#else
uint8x8x3_t vBgr = vld3_u8(src + sj);
int16x8_t vB = vreinterpretq_s16_u16(vmovl_u8(vBgr.val[0]));
int16x8_t vG = vreinterpretq_s16_u16(vmovl_u8(vBgr.val[1]));
int16x8_t vR = vreinterpretq_s16_u16(vmovl_u8(vBgr.val[2]));
uint8x8x3_t vYCrCb = convertToYCrCb(vR, vG, vB, vcYRG, vcYB, vcCrGB, vcCbRG);
vst3_u8(dst + dj, vYCrCb);
#endif
}
for (; j < size.width; ++j, sj += 3, dj += 3)
{
S_CONVERTTOYCRCB(src[sj + 2], src[sj + 1], src[sj]);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void bgrx2ycrcb(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YCRCB_CONSTS
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0u; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t sj = 0u, dj = 0u, j = 0u;
for (; j < roiw8; sj += 32, dj += 24, j += 8)
{
internal::prefetch(src + sj);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTTOYCRCB(vld4.8 {d0-d3}, d2, d1, d0)
#else
uint8x8x4_t vBgra = vld4_u8(src + sj);
int16x8_t vB = vreinterpretq_s16_u16(vmovl_u8(vBgra.val[0]));
int16x8_t vG = vreinterpretq_s16_u16(vmovl_u8(vBgra.val[1]));
int16x8_t vR = vreinterpretq_s16_u16(vmovl_u8(vBgra.val[2]));
uint8x8x3_t vYCrCb = convertToYCrCb(vR, vG, vB, vcYRG, vcYB, vcCrGB, vcCbRG);
vst3_u8(dst + dj, vYCrCb);
#endif
}
for (; j < size.width; ++j, sj += 4, dj += 3)
{
S_CONVERTTOYCRCB(src[sj + 2], src[sj + 1], src[sj]);
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420sp2rgb(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
// input data:
////////////// Y matrix:
// {y1, y2,   y3, y4,   y5, y6,   y7, y8,   y9, y10, y11, y12, y13, y14, y15, y16}
// {Y1, Y2,   Y3, Y4,   Y5, Y6,   Y7, Y8,   Y9, Y10, Y11, Y12, Y13, Y14, Y15, Y16}
////////////// UV matrix:
// {v12, u12, v34, u34, v56, u56, v78, u78, v90 u90, V12, U12, V34, U34, V56, U56}
// fp version
// R = 1.164(Y - 16) + 1.596(V - 128)
// G = 1.164(Y - 16) - 0.813(V - 128) - 0.391(U - 128)
// B = 1.164(Y - 16)                  + 2.018(U - 128)
// integer version
// R = [((149*y)/2 + (-14248+102*v)      )/2]/32
// G = [((149*y)/2 + ((8663- 25*u)-52*v))/2]/32
// B = [((149*y)/2 + (-17705+129*u)      )/2]/32
// error estimation:
//Rerr = 0.0000625 * y - 0.00225 * v                - 0.287
//Gerr = 0.0000625 * y + 0.0005  * v + 0.000375 * u + 0.128625
//Berr = 0.0000625 * y               - 0.002375 * u - 0.287375
//real error test:
//=================
//R: 1 less: 520960       ==  3.11% of full space
//G: 1 less: 251425       ==  1.50% of full space
//B: 1 less: 455424       ==  2.71% of full space
//=================
//R: 1 more: 642048       ==  3.83% of full space
//G: 1 more: 192458       ==  1.15% of full space
//B: 1 more: 445184       ==  2.65% of full space
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(3, 2, 0)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 48, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(3, d1, d0, q5, q6)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 6)
{
convertYUV420ToRGB<3, 2, 0>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420sp2rgbx(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(4, 2, 0)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 64, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(4, d1, d0, q5, q6)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 8)
{
convertYUV420ToRGB<4, 2, 0>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420i2rgb(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(3, 2, 1)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 48, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(3, d0, d1, q5, q6)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 6)
{
convertYUV420ToRGB<3, 2, 1>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420i2rgbx(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(4, 2, 1)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 64, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(4, d0, d1, q5, q6)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 8)
{
convertYUV420ToRGB<4, 2, 1>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420sp2bgr(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(3, 0, 0)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 48, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(3, d1, d0, q6, q5)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 6)
{
convertYUV420ToRGB<3, 0, 0>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420sp2bgrx(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(4, 0, 0)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 64, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(4, d1, d0, q6, q5)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 8)
{
convertYUV420ToRGB<4, 0, 0>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420i2bgr(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(3, 0, 1)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 48, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(3, d0, d1, q6, q5)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 6)
{
convertYUV420ToRGB<3, 0, 1>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
void yuv420i2bgrx(const Size2D &size,
const u8 *  yBase, ptrdiff_t  yStride,
const u8 * uvBase, ptrdiff_t uvStride,
u8 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
YUV420_CONSTS(4, 0, 1)
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
for (size_t i = 0u; i < size.height; i+=2)
{
const u8 * uv = internal::getRowPtr(uvBase, uvStride, i>>1);
const u8 * y1 = internal::getRowPtr(yBase, yStride, i);
const u8 * y2 = internal::getRowPtr(yBase, yStride, i+1);
u8 * dst1 = internal::getRowPtr(dstBase, dstStride, i);
u8 * dst2 = internal::getRowPtr(dstBase, dstStride, i+1);
size_t dj = 0u, j = 0u;
for (; j < roiw16; dj += 64, j += 16)
{
internal::prefetch(uv + j);
internal::prefetch(y1 + j);
internal::prefetch(y2 + j);
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CONVERTYUV420TORGB(4, d0, d1, q6, q5)
#else
convertYUV420.ToRGB(y1 + j, y2 + j, uv + j, dst1 + dj, dst2 + dj);
#endif
}
for (; j + 2 <= size.width; j+=2, dj += 8)
{
convertYUV420ToRGB<4, 0, 1>(y1+j, y2+j, uv+j, dst1 + dj, dst2 + dj);
}
}
#else
(void)size;
(void)yBase;
(void)yStride;
(void)uvBase;
(void)uvStride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include <cstdlib>
#include <iostream>
#include "common.hpp"
namespace CAROTENE_NS {
bool isSupportedConfiguration()
{
#ifdef CAROTENE_NEON
return true;
#else
return false;
#endif
}
namespace internal {
void assertSupportedConfiguration(bool parametersSupported)
{
if (!isSupportedConfiguration()) {
std::cerr << "internal error: attempted to use an unavailable function" << std::endl;
std::abort();
}
if (!parametersSupported) {
std::cerr << "internal error: attempted to use a function with unsupported parameters" << std::endl;
std::abort();
}
}
ptrdiff_t borderInterpolate(ptrdiff_t _p, size_t _len, BORDER_MODE borderType, size_t startMargin, size_t endMargin)
{
ptrdiff_t p = _p + (ptrdiff_t)startMargin;
size_t len = _len + startMargin + endMargin;
if( (size_t)p < len )
return _p;
else if( borderType == BORDER_MODE_REPLICATE )
p = p < 0 ? 0 : (ptrdiff_t)len - 1;
else if( borderType == BORDER_MODE_REFLECT || borderType == BORDER_MODE_REFLECT101 )
{
s32 delta = borderType == BORDER_MODE_REFLECT101;
if( len == 1 )
return 0;
do
{
if( p < 0 )
p = -p - 1 + delta;
else
p = (ptrdiff_t)len - 1 - (p - (ptrdiff_t)len) - delta;
}
while( (size_t)p >= len );
}
else if( borderType == BORDER_MODE_WRAP )
{
if( p < 0 )
p -= ((p-(ptrdiff_t)len+1)/(ptrdiff_t)len)*(ptrdiff_t)len;
if( p >= (ptrdiff_t)len )
p %= (ptrdiff_t)len;
}
else if( borderType == BORDER_MODE_CONSTANT )
p = -1;
else
internal::assertSupportedConfiguration(false);
return p - (ptrdiff_t)startMargin;
}
} // namespace internal
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
#define CVT_FUNC(T1, T2, SIMD_SIZE, CVTINIT, CVTROW)                            \
void convert(const Size2D &_size,                                           \
const T1 * srcBase, ptrdiff_t srcStride,                       \
T2 * dstBase, ptrdiff_t dstStride)                             \
{                                                                           \
internal::assertSupportedConfiguration();                               \
Size2D size(_size);                                                     \
if (srcStride == dstStride &&                                           \
srcStride == (ptrdiff_t)(size.width))                               \
{                                                                       \
size.width *= size.height;                                          \
size.height = 1;                                                    \
}                                                                       \
const ptrdiff_t sstep = srcStride / sizeof(T1);                         \
const ptrdiff_t dstep = dstStride / sizeof(T2);                         \
const size_t w = size.width & ~(SIMD_SIZE-1);                           \
if (size.width >= SIMD_SIZE)                                            \
{                                                                       \
const T1* _src = srcBase;                                           \
T2* _dst = dstBase;                                                 \
CVTINIT                                                             \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep ) \
CVTROW                                                          \
}                                                                       \
if(w < size.width)                                                      \
{                                                                       \
const T1* _src = srcBase;                                           \
T2* _dst = dstBase;                                                 \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep ) \
for(size_t i = w; i < size.width; i++ )                         \
_dst[i] = internal::saturate_cast<T2>(_src[i]);             \
}                                                                       \
}
#else
#define CVT_FUNC(T1, T2, SIMD_SIZE, CVTINIT, CVTROW)                            \
void convert(const Size2D &,                                                \
const T1 *, ptrdiff_t,                                         \
T2 *, ptrdiff_t)                                               \
{                                                                           \
internal::assertSupportedConfiguration();                               \
}
#endif
CVT_FUNC(u8, s8, 16,
uint8x16_t v127 = vdupq_n_u8(127);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vu8 = vld1q_u8(_src + i);
int8x16_t vu1 = vreinterpretq_s8_u8(vminq_u8(vu8, v127));
vst1q_s8(_dst + i, vu1);
}
})
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(u8, u16, 16,
register uint8x16_t zero0 asm ("q1") = vmovq_n_u8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vst2.8 {d0,d2}, [%[dst1]]                             \n\t"
"vst2.8 {d1,d3}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w" (zero0)
: "d0","d1"
);
}
})
#else
CVT_FUNC(u8, u16, 16,
uint8x16x2_t vline;
vline.val[1] = vmovq_n_u8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
vline.val[0] = vld1q_u8(_src + i);
vst2q_u8((uint8_t*)(_dst + i), vline);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(u8, s32, 16,
register uint8x16_t zero0 asm ("q1") = vmovq_n_u8(0);
register uint8x16_t zero1 asm ("q2") = vmovq_n_u8(0);
register uint8x16_t zero2 asm ("q3") = vmovq_n_u8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vst4.8 {d0,d2,d4,d6}, [%[dst1]]                       \n\t"
"vst4.8 {d1,d3,d5,d7}, [%[dst2]]                       \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w" (zero0), "w" (zero1), "w" (zero2)
: "d0","d1"
);
}
})
#else
CVT_FUNC(u8, s32, 16,
uint8x16x4_t vline;
vline.val[1] = vmovq_n_u8(0);
vline.val[2] = vmovq_n_u8(0);
vline.val[3] = vmovq_n_u8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
vline.val[0] = vld1q_u8(_src + i);
vst4q_u8((uint8_t*)(_dst + i), vline);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(u8, f32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vmovl.u8 q1, d0                                       \n\t"
"vmovl.u8 q2, d1                                       \n\t"
"vmovl.u16 q3, d2                                      \n\t"
"vmovl.u16 q4, d3                                      \n\t"
"vmovl.u16 q5, d4                                      \n\t"
"vmovl.u16 q6, d5                                      \n\t"
"vcvt.f32.u32 q7, q3                                   \n\t"
"vcvt.f32.u32 q8, q4                                   \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vst1.32 {d14-d15}, [%[dst1]]                          \n\t"
"vst1.32 {d16-d17}, [%[dst2]]                          \n\t"
"vst1.32 {d18-d19}, [%[dst3]]                          \n\t"
"vst1.32 {d20-d21}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21"
);
}
})
#else
CVT_FUNC(u8, f32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline_u8 = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8(vline_u8));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline_u8));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16(vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16(vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vst1q_f32(_dst + i, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
vst1q_f32(_dst + i + 8, vline3_f32);
vst1q_f32(_dst + i + 12, vline4_f32);
}
})
#endif
CVT_FUNC(s8, u8, 16,
int8x16_t vZero = vdupq_n_s8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vu8 = vld1q_s8(_src + i);
uint8x16_t vu1 = vreinterpretq_u8_s8(vmaxq_s8(vu8, vZero));
vst1q_u8(_dst + i, vu1);
}
})
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(s8, u16, 16,
register uint8x16_t zero0 asm ("q1") = vmovq_n_u8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vmax.s8 q0, q1                                        \n\t"
"vst2.8 {d0,d2}, [%[dst1]]                             \n\t"
"vst2.8 {d1,d3}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w" (zero0)
: "d0","d1"
);
}
})
#else
CVT_FUNC(s8, u16, 16,
int8x16x2_t vline_s8;
vline_s8.val[1] = vmovq_n_s8(0);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
vline_s8.val[0] = vld1q_s8(_src + i);
vline_s8.val[0] = vmaxq_s8(vline_s8.val[0], vline_s8.val[1]);
vst2q_s8((int8_t*)(_dst + i), vline_s8);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s8, s16, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vmovl.s8 q1, d0                                       \n\t"
"vmovl.s8 q2, d1                                       \n\t"
"vst1.16 {d2-d3}, [%[dst1]]                            \n\t"
"vst1.16 {d4-d5}, [%[dst2]]                            \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s8, s16, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline_s8 = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8(vline_s8));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline_s8));
vst1q_s16(_dst + i, vline1_s16);
vst1q_s16(_dst + i + 8, vline2_s16);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(s8, s32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vmovl.s8 q1, d0                                       \n\t"
"vmovl.s8 q2, d1                                       \n\t"
"vmovl.s16 q3, d2                                      \n\t"
"vmovl.s16 q4, d3                                      \n\t"
"vmovl.s16 q5, d4                                      \n\t"
"vmovl.s16 q6, d5                                      \n\t"
"vst1.32 {d6-d7}, [%[dst1]]                            \n\t"
"vst1.32 {d8-d9}, [%[dst2]]                            \n\t"
"vst1.32 {d10-d11}, [%[dst3]]                          \n\t"
"vst1.32 {d12-d13}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
})
#else
CVT_FUNC(s8, s32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline_s8 = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8(vline_s8));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline_s8));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16(vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16(vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
vst1q_s32(_dst + i, vline1_s32);
vst1q_s32(_dst + i + 4, vline2_s32);
vst1q_s32(_dst + i + 8, vline3_s32);
vst1q_s32(_dst + i + 12, vline4_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s8, f32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src]]                              \n\t"
"vmovl.s8 q1, d0                                       \n\t"
"vmovl.s8 q2, d1                                       \n\t"
"vmovl.s16 q3, d2                                      \n\t"
"vmovl.s16 q4, d3                                      \n\t"
"vmovl.s16 q5, d4                                      \n\t"
"vmovl.s16 q6, d5                                      \n\t"
"vcvt.f32.s32 q7, q3                                   \n\t"
"vcvt.f32.s32 q8, q4                                   \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vst1.32 {d14-d15}, [%[dst1]]                          \n\t"
"vst1.32 {d16-d17}, [%[dst2]]                          \n\t"
"vst1.32 {d18-d19}, [%[dst3]]                          \n\t"
"vst1.32 {d20-d21}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21"
);
}
})
#else
CVT_FUNC(s8, f32, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline_s8 = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8(vline_s8));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline_s8));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16(vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16(vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vst1q_f32(_dst + i, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
vst1q_f32(_dst + i + 8, vline3_f32);
vst1q_f32(_dst + i + 12, vline4_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(u16, u8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src1]]                             \n\t"
"vqmovn.u16 d4, q0                                     \n\t"
"vld1.8 {d2-d3}, [%[src2]]                             \n\t"
"vqmovn.u16 d5, q1                                     \n\t"
"vst1.8 {d4-d5}, [%[dst]]                              \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 8),
[dst] "r" (_dst + i + 0)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(u16, u8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint16x8_t vline1_u16 = vld1q_u16(_src + i);
uint16x8_t vline2_u16 = vld1q_u16(_src + i + 8);
uint8x8_t vline1_u8 = vqmovn_u16(vline1_u16);
uint8x8_t vline2_u8 = vqmovn_u16(vline2_u16);
vst1q_u8(_dst + i, vcombine_u8(vline1_u8, vline2_u8));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(u16, s8, 16,
register uint8x16_t v127 asm ("q4") = vmovq_n_u8(127);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src1]]                             \n\t"
"vqmovn.u16 d4, q0                                     \n\t"
"vld1.8 {d2-d3}, [%[src2]]                             \n\t"
"vqmovn.u16 d5, q1                                     \n\t"
"vmin.u8 q3, q2, q4                                    \n\t"
"vst1.8 {d6-d7}, [%[dst]]                              \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 8),
[dst] "r" (_dst + i + 0),
"w" (v127)
: "d0","d1","d2","d3","d4","d5","d6","d7"
);
}
})
#else
CVT_FUNC(u16, s8, 16,
uint8x8_t v127 = vmov_n_u8(127);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint16x8_t vline1_u16 = vld1q_u16(_src + i);
uint16x8_t vline2_u16 = vld1q_u16(_src + i + 8);
uint8x8_t vline1_u8 = vqmovn_u16(vline1_u16);
uint8x8_t vline2_u8 = vqmovn_u16(vline2_u16);
vline1_u8 = vmin_u8(vline1_u8, v127);
vline2_u8 = vmin_u8(vline2_u8, v127);
vst1q_s8(_dst + i, vcombine_s8(vreinterpret_s8_u8(vline1_u8), vreinterpret_s8_u8(vline2_u8)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(u16, s16, 8,
register uint16x8_t v32767 asm ("q4") = vmovq_n_u16(0x7FFF);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                              \n\t"
"vmin.u16 q1, q0, q4                                    \n\t"
"vst1.16 {d2-d3}, [%[dst]]                              \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w" (v32767)
: "d0","d1","d2","d3"
);
}
})
#else
CVT_FUNC(u16, s16, 8,
uint16x8_t v32767 = vmovq_n_u16(0x7FFF);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline_u16 = vld1q_u16(_src + i);
vline_u16 = vminq_u16(vline_u16, v32767);
vst1q_s16((_dst + i), vreinterpretq_s16_u16(vline_u16));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(u16, s32, 8,
register uint16x8_t zero0 asm ("q1") = vmovq_n_u16(0);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                        \n\t"
"vst2.16 {d0,d2}, [%[dst1]]                       \n\t"
"vst2.16 {d1,d3}, [%[dst2]]                       \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i),
[dst2] "r" (_dst + i + 4),
"w" (zero0)
: "d0","d1"//,"d2","d3"//,"d4","d5","d6","d7"
);
}
})
#else
CVT_FUNC(u16, s32, 8,
uint16x8x2_t vline;
vline.val[1] = vmovq_n_u16(0);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
vline.val[0] = vld1q_u16(_src + i);
vst2q_u16((uint16_t*)(_dst + i), vline);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(u16, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                              \n\t"
"vmovl.u16 q1, d0                                       \n\t"
"vmovl.u16 q2, d1                                       \n\t"
"vcvt.f32.u32 q3, q1                                    \n\t"
"vcvt.f32.u32 q4, q2                                    \n\t"
"vst1.32 {d6-d7}, [%[dst1]]                             \n\t"
"vst1.32 {d8-d9}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9"
);
}
})
#else
CVT_FUNC(u16, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline_u16 = vld1q_u16(_src + i);
uint32x4_t vline_u32_lo = vmovl_u16(vget_low_u16(vline_u16));
uint32x4_t vline_u32_hi = vmovl_u16(vget_high_u16(vline_u16));
float32x4_t vline_f32_lo = vcvtq_f32_u32(vline_u32_lo);
float32x4_t vline_f32_hi = vcvtq_f32_u32(vline_u32_hi);
vst1q_f32(_dst + i, vline_f32_lo);
vst1q_f32(_dst + i + 4, vline_f32_hi);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s16, u8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src1]]                             \n\t"
"vld1.8 {d2-d3}, [%[src2]]                             \n\t"
"vqmovun.s16 d4, q0                                    \n\t"
"vqmovun.s16 d5, q1                                    \n\t"
"vst1.8 {d4-d5}, [%[dst]]                              \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 8),
[dst] "r" (_dst + i + 0)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s16, u8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int16x8_t vline1_s16 = vld1q_s16(_src + i);
int16x8_t vline2_s16 = vld1q_s16(_src + i + 8);
uint8x8_t vline1_u8 = vqmovun_s16(vline1_s16);
uint8x8_t vline2_u8 = vqmovun_s16(vline2_s16);
vst1q_u8(_dst + i, vcombine_u8(vline1_u8, vline2_u8));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s16, s8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d0-d1}, [%[src1]]                             \n\t"
"vld1.8 {d2-d3}, [%[src2]]                             \n\t"
"vqmovn.s16 d4, q0                                     \n\t"
"vqmovn.s16 d5, q1                                     \n\t"
"vst1.8 {d4-d5}, [%[dst]]                              \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 8),
[dst] "r" (_dst + i + 0)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s16, s8, 16,
,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int16x8_t vline1_s16 = vld1q_s16(_src + i);
int16x8_t vline2_s16 = vld1q_s16(_src + i + 8);
int8x8_t vline1_s8 = vqmovn_s16(vline1_s16);
int8x8_t vline2_s8 = vqmovn_s16(vline2_s16);
vst1q_s8(_dst + i, vcombine_s8(vline1_s8, vline2_s8));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVT_FUNC(s16, u16, 8,
register int16x8_t vZero asm ("q4") = vmovq_n_s16(0);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                              \n\t"
"vmax.s16 q1, q0, q4                                    \n\t"
"vst1.16 {d2-d3}, [%[dst]]                              \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w" (vZero)
: "d0","d1","d2","d3"
);
}
})
#else
CVT_FUNC(s16, u16, 8,
int16x4_t vZero = vmov_n_s16(0);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline_s16 = vld1q_s16(_src + i);
int16x4_t vline_s16_lo = vmax_s16(vget_low_s16(vline_s16), vZero);
int16x4_t vline_s16_hi = vmax_s16(vget_high_s16(vline_s16), vZero);
vst1q_u16(_dst + i, vcombine_u16(vreinterpret_u16_s16(vline_s16_lo), vreinterpret_u16_s16(vline_s16_hi)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s16, s32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                              \n\t"
"vmovl.s16 q1, d0                                       \n\t"
"vmovl.s16 q2, d1                                       \n\t"
"vst1.32 {d2-d3}, [%[dst1]]                             \n\t"
"vst1.32 {d4-d5}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s16, s32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline_s16 = vld1q_s16(_src + i);
int32x4_t vline_s32_lo = vmovl_s16(vget_low_s16(vline_s16));
int32x4_t vline_s32_hi = vmovl_s16(vget_high_s16(vline_s16));
vst1q_s32(_dst + i, vline_s32_lo);
vst1q_s32(_dst + i + 4, vline_s32_hi);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s16, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d0-d1}, [%[src]]                              \n\t"
"vmovl.s16 q1, d0                                       \n\t"
"vmovl.s16 q2, d1                                       \n\t"
"vcvt.f32.s32 q3, q1                                    \n\t"
"vcvt.f32.s32 q4, q2                                    \n\t"
"vst1.32 {d6-d7}, [%[dst1]]                             \n\t"
"vst1.32 {d8-d9}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9"
);
}
})
#else
CVT_FUNC(s16, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline_s16 = vld1q_s16(_src + i);
int32x4_t vline_s32_lo = vmovl_s16(vget_low_s16(vline_s16));
int32x4_t vline_s32_hi = vmovl_s16(vget_high_s16(vline_s16));
float32x4_t vline_f32_lo = vcvtq_f32_s32(vline_s32_lo);
float32x4_t vline_f32_hi = vcvtq_f32_s32(vline_s32_hi);
vst1q_f32(_dst + i, vline_f32_lo);
vst1q_f32(_dst + i + 4, vline_f32_hi);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s32, u8, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d0-d1}, [%[src1]]                              \n\t"
"vld1.32 {d2-d3}, [%[src2]]                              \n\t"
"vqmovun.s32 d4, q0                                      \n\t"
"vqmovun.s32 d5, q1                                      \n\t"
"vqmovn.u16  d6, q2                                      \n\t"
"vst1.8 {d6}, [%[dst]]                                   \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i)
: "d0","d1","d2","d3","d4","d5","d6"
);
}
})
#else
CVT_FUNC(s32, u8, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
uint16x4_t vline1_u16 = vqmovun_s32(vline1_s32);
uint16x4_t vline2_u16 = vqmovun_s32(vline2_s32);
uint8x8_t vline_u8 = vqmovn_u16(vcombine_u16(vline1_u16, vline2_u16));
vst1_u8(_dst + i, vline_u8);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s32, s8, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d0-d1}, [%[src1]]                              \n\t"
"vld1.32 {d2-d3}, [%[src2]]                              \n\t"
"vqmovn.s32 d4, q0                                       \n\t"
"vqmovn.s32 d5, q1                                       \n\t"
"vqmovn.s16  d6, q2                                      \n\t"
"vst1.8 {d6}, [%[dst]]                                   \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i)
: "d0","d1","d2","d3","d4","d5","d6"
);
}
})
#else
CVT_FUNC(s32, s8, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
int16x4_t vline1_s16 = vqmovn_s32(vline1_s32);
int16x4_t vline2_s16 = vqmovn_s32(vline2_s32);
int8x8_t vline_s8 = vqmovn_s16(vcombine_s16(vline1_s16, vline2_s16));
vst1_s8(_dst + i, vline_s8);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s32, u16, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d0-d1}, [%[src1]]                              \n\t"
"vld1.32 {d2-d3}, [%[src2]]                              \n\t"
"vqmovun.s32 d4, q0                                      \n\t"
"vqmovun.s32 d5, q1                                      \n\t"
"vst1.16 {d4-d5}, [%[dst]]                               \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s32, u16, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
uint16x4_t vline1_u16 = vqmovun_s32(vline1_s32);
uint16x4_t vline2_u16 = vqmovun_s32(vline2_s32);
vst1q_u16(_dst + i, vcombine_u16(vline1_u16, vline2_u16));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s32, s16, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d0-d1}, [%[src1]]                              \n\t"
"vld1.32 {d2-d3}, [%[src2]]                              \n\t"
"vqmovn.s32 d4, q0                                       \n\t"
"vqmovn.s32 d5, q1                                       \n\t"
"vst1.8 {d4-d5}, [%[dst]]                                \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i)
: "d0","d1","d2","d3","d4","d5"
);
}
})
#else
CVT_FUNC(s32, s16, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
int16x4_t vline1_s16 = vqmovn_s32(vline1_s32);
int16x4_t vline2_s16 = vqmovn_s32(vline2_s32);
vst1q_s16(_dst + i, vcombine_s16(vline1_s16, vline2_s16));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(s32, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d0-d1}, [%[src]]                              \n\t"
"vcvt.f32.s32 q1, q0                                    \n\t"
"vst1.32 {d2-d3}, [%[dst]]                              \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i)
: "d0","d1","d2","d3"//,"d4","d5"
);
__asm__ (
"vld1.32 {d0-d1}, [%[src]]                              \n\t"
"vcvt.f32.s32 q1, q0                                    \n\t"
"vst1.32 {d2-d3}, [%[dst]]                              \n\t"
: /*no output*/
: [src] "r" (_src + i + 4),
[dst] "r" (_dst + i + 4)
: "d0","d1","d2","d3"//,"d4","d5"
);
}
})
#else
CVT_FUNC(s32, f32, 8,
,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline_s32 = vld1q_s32(_src + i);
float32x4_t vline_f32 = vcvtq_f32_s32(vline_s32);
vst1q_f32(_dst + i, vline_f32);
vline_s32 = vld1q_s32(_src + i + 4);
vline_f32 = vcvtq_f32_s32(vline_s32);
vst1q_f32(_dst + i + 4, vline_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(f32, u8, 8,
register float32x4_t vmult asm ("q0") = vdupq_n_f32((float)(1 << 16));
register uint32x4_t  vmask asm ("q1") = vdupq_n_u32(1<<16);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vcvt.u32.f32 q6, q4                                     \n\t"
"vcvt.u32.f32 q7, q5                                     \n\t"
"vbic q8, q1, q6                                         \n\t"
"vbic q9, q1, q7                                         \n\t"
"vshr.u32 q10, q8, #16                                   \n\t"
"vshr.u32 q11, q9, #16                                   \n\t"
"vqsub.u32 q12, q6, q10                                  \n\t"
"vqsub.u32 q13, q7, q11                                  \n\t"
"vqrshrn.u32 d28, q12, #16                               \n\t"
"vqrshrn.u32 d29, q13, #16                               \n\t"
"vqmovn.u16 d30, q14                                     \n\t"
"vst1.8 {d30}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vmult), "w" (vmask)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30"
);
}
})
#else
CVT_FUNC(f32, u8, 8,
float32x4_t vmult = vdupq_n_f32((float)(1 << 16));
uint32x4_t  vmask = vdupq_n_u32(1<<16);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
float32x4_t vline1w_f32 = vmulq_f32(vline1_f32, vmult);
float32x4_t vline2w_f32 = vmulq_f32(vline2_f32, vmult);
uint32x4_t vline1_u32 = vcvtq_u32_f32(vline1w_f32);
uint32x4_t vline2_u32 = vcvtq_u32_f32(vline2w_f32);
uint32x4_t vl1_masked = vbicq_u32(vmask, vline1_u32);
uint32x4_t vl2_masked = vbicq_u32(vmask, vline2_u32);
uint32x4_t vl1_masked2 = vshrq_n_u32(vl1_masked, 16);
uint32x4_t vl2_masked2 = vshrq_n_u32(vl2_masked, 16);
uint32x4_t vline1r_u32 = vqsubq_u32(vline1_u32, vl1_masked2);
uint32x4_t vline2r_u32 = vqsubq_u32(vline2_u32, vl2_masked2);
uint16x4_t vline1_u16 = vqrshrn_n_u32(vline1r_u32, 16);
uint16x4_t vline2_u16 = vqrshrn_n_u32(vline2r_u32, 16);
uint8x8_t vline_u8 = vqmovn_u16(vcombine_u16(vline1_u16, vline2_u16));
vst1_u8(_dst + i, vline_u8);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(f32, s8, 8,
register float32x4_t vhalf asm ("q0") = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d2-d3}, [%[src1]]                              \n\t"
"vld1.32 {d4-d5}, [%[src2]]                              \n\t"
"vadd.f32 q3, q1, q0                                     \n\t"
"vadd.f32 q4, q2, q0                                     \n\t"
"vcvt.s32.f32 q5, q3                                     \n\t"
"vcvt.s32.f32 q6, q4                                     \n\t"
"vqmovn.s32 d14, q5                                      \n\t"
"vqmovn.s32 d15, q6                                      \n\t"
"vqmovn.s16 d16, q7                                      \n\t"
"vst1.8 {d16}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17"
);
}
})
#else
CVT_FUNC(f32, s8, 8,
float32x4_t vhalf = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vaddq_f32(vline1_f32, vhalf);
vline2_f32 = vaddq_f32(vline2_f32, vhalf);
int32x4_t vline1_s32 = vcvtq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = vcvtq_s32_f32(vline2_f32);
int16x4_t vline1_s16 = vqmovn_s32(vline1_s32);
int16x4_t vline2_s16 = vqmovn_s32(vline2_s32);
int8x8_t vline_s8 = vqmovn_s16(vcombine_s16(vline1_s16, vline2_s16));
vst1_s8(_dst + i, vline_s8);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(f32, u16, 8,
register float32x4_t vhalf asm ("q0") = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d2-d3}, [%[src]]                               \n\t"
"vadd.f32 q2, q1, q0                                     \n\t"
"vcvt.u32.f32 q3, q2                                     \n\t"
"vqmovn.u32 d8, q3                                       \n\t"
"vst1.16 {d8}, [%[dst]]                                  \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8"
);
__asm__ (
"vld1.32 {d2-d3}, [%[src]]                               \n\t"
"vadd.f32 q2, q1, q0                                     \n\t"
"vcvt.u32.f32 q3, q2                                     \n\t"
"vqmovn.u32 d8, q3                                       \n\t"
"vst1.16 {d8}, [%[dst]]                                  \n\t"
: /*no output*/
: [src] "r" (_src + i + 4),
[dst] "r" (_dst + i + 4),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8"
);
}
})
#else
CVT_FUNC(f32, u16, 8,
float32x4_t vhalf = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline_f32 = vld1q_f32(_src + i);
vline_f32 = vaddq_f32(vline_f32, vhalf);
uint32x4_t vline_u32 = vcvtq_u32_f32(vline_f32);
uint16x4_t vline_u16 = vqmovn_u32(vline_u32);
vst1_u16(_dst + i, vline_u16);
vline_f32 = vld1q_f32(_src + i + 4);
vline_f32 = vaddq_f32(vline_f32, vhalf);
vline_u32 = vcvtq_u32_f32(vline_f32);
vline_u16 = vqmovn_u32(vline_u32);
vst1_u16(_dst + i + 4, vline_u16);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(f32, s16, 8,
register float32x4_t vhalf asm ("q0") = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d2-d3}, [%[src]]                               \n\t"
"vadd.f32 q2, q1, q0                                     \n\t"
"vcvt.s32.f32 q3, q2                                     \n\t"
"vqmovn.s32 d8, q3                                       \n\t"
"vst1.16 {d8}, [%[dst]]                                  \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8"
);
__asm__ (
"vld1.32 {d2-d3}, [%[src]]                               \n\t"
"vadd.f32 q2, q1, q0                                     \n\t"
"vcvt.s32.f32 q3, q2                                     \n\t"
"vqmovn.s32 d8, q3                                       \n\t"
"vst1.16 {d8}, [%[dst]]                                  \n\t"
: /*no output*/
: [src] "r" (_src + i + 4),
[dst] "r" (_dst + i + 4),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8"
);
}
})
#else
CVT_FUNC(f32, s16, 8,
float32x4_t vhalf = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline_f32 = vld1q_f32(_src + i);
vline_f32 = vaddq_f32(vline_f32, vhalf);
int32x4_t vline_s32 = vcvtq_s32_f32(vline_f32);
int16x4_t vline_s16 = vqmovn_s32(vline_s32);
vst1_s16(_dst + i, vline_s16);
vline_f32 = vld1q_f32(_src + i + 4);
vline_f32 = vaddq_f32(vline_f32, vhalf);
vline_s32 = vcvtq_s32_f32(vline_f32);
vline_s16 = vqmovn_s32(vline_s32);
vst1_s16(_dst + i + 4, vline_s16);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 6 && !defined(__clang__)
CVT_FUNC(f32, s32, 8,
register float32x4_t vhalf asm ("q0") = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d2-d3}, [%[src1]]                              \n\t"
"vld1.32 {d4-d5}, [%[src2]]                              \n\t"
"vadd.f32 q3, q1, q0                                     \n\t"
"vadd.f32 q4, q2, q0                                     \n\t"
"vcvt.s32.f32 q5, q3                                     \n\t"
"vcvt.s32.f32 q6, q4                                     \n\t"
"vst1.32 {q5}, [%[dst1]]                                 \n\t"
"vst1.32 {q6}, [%[dst2]]                                 \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 4),
[dst1] "r" (_dst + i),
[dst2] "r" (_dst + i + 4),
"w" (vhalf)
: "d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13"
);
}
})
#else
CVT_FUNC(f32, s32, 8,
float32x4_t vhalf = vdupq_n_f32(0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline_f32 = vld1q_f32(_src + i);
vline_f32 = vaddq_f32(vline_f32, vhalf);
int32x4_t vline_s32 = vcvtq_s32_f32(vline_f32);
vst1q_s32(_dst + i, vline_s32);
vline_f32 = vld1q_f32(_src + i + 4);
vline_f32 = vaddq_f32(vline_f32, vhalf);
vline_s32 = vcvtq_s32_f32(vline_f32);
vst1q_s32(_dst + i + 4, vline_s32);
}
})
#endif
void convert(const Size2D &_size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride)
{
convert(_size, srcBase, srcStride, (u16*)dstBase, dstStride);
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include <cstring>
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
template <int shift>
void lshiftConst(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
s16 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
uint8x16_t v_src = vld1q_u8(src + j);
int16x8_t v_dst0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src)));
int16x8_t v_dst1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src)));
vst1q_s16(dst + j, vshlq_n_s16(v_dst0, shift));
vst1q_s16(dst + j + 8, vshlq_n_s16(v_dst1, shift));
}
for (; j < roiw8; j += 8)
{
int16x8_t v_dst = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src + j)));
vst1q_s16(dst + j, vshlq_n_s16(v_dst, shift));
}
for (; j < size.width; j++)
{
dst[j] = ((s16)src[j] << shift);
}
}
}
template <>
void lshiftConst<0>(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
s16 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
uint8x16_t v_src = vld1q_u8(src + j);
int16x8_t v_dst0 = vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(v_src)));
int16x8_t v_dst1 = vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(v_src)));
vst1q_s16(dst + j, v_dst0);
vst1q_s16(dst + j + 8, v_dst1);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_dst = vreinterpretq_s16_u16(vmovl_u8(vld1_u8(src + j)));
vst1q_s16(dst + j, v_dst);
}
for (; j < size.width; j++)
{
dst[j] = (s16)src[j];
}
}
}
template <int shift>
void rshiftConst(const Size2D &size,
const s16 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
CONVERT_POLICY cpolicy)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const s16 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
if (cpolicy == CONVERT_POLICY_SATURATE)
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vshrq_n_s16(vld1q_s16(src + j), shift),
v_src1 = vshrq_n_s16(vld1q_s16(src + j + 8), shift);
uint8x16_t v_dst = vcombine_u8(vqmovun_s16(v_src0),
vqmovun_s16(v_src1));
vst1q_u8(dst + j, v_dst);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vshrq_n_s16(vld1q_s16(src + j), shift);
vst1_u8(dst + j, vqmovun_s16(v_src));
}
for (; j < size.width; j++)
{
dst[j] = internal::saturate_cast<u8>((src[j] >> shift));
}
}
else // CONVERT_POLICY_WRAP
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vshrq_n_s16(vld1q_s16(src + j), shift),
v_src1 = vshrq_n_s16(vld1q_s16(src + j + 8), shift);
int8x16_t v_dst = vcombine_s8(vmovn_s16(v_src0),
vmovn_s16(v_src1));
vst1q_u8(dst + j, vreinterpretq_u8_s8(v_dst));
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vshrq_n_s16(vld1q_s16(src + j), shift);
vst1_u8(dst + j, vreinterpret_u8_s8(vmovn_s16(v_src)));
}
for (; j < size.width; j++)
{
dst[j] = (u8)((src[j] >> shift));
}
}
}
}
template <>
void rshiftConst<0>(const Size2D &size,
const s16 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
CONVERT_POLICY cpolicy)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const s16 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
if (cpolicy == CONVERT_POLICY_SATURATE)
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vld1q_s16(src + j), v_src1 = vld1q_s16(src + j + 8);
uint8x16_t v_dst = vcombine_u8(vqmovun_s16(v_src0), vqmovun_s16(v_src1));
vst1q_u8(dst + j, v_dst);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vld1q_s16(src + j);
vst1_u8(dst + j, vqmovun_s16(v_src));
}
for (; j < size.width; j++)
{
dst[j] = internal::saturate_cast<u8>(src[j]);
}
}
else // CONVERT_POLICY_WRAP
{
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vld1q_s16(src + j), v_src1 = vld1q_s16(src + j + 8);
int8x16_t v_dst = vcombine_s8(vmovn_s16(v_src0), vmovn_s16(v_src1));
vst1q_u8(dst + j, vreinterpretq_u8_s8(v_dst));
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vld1q_s16(src + j);
vst1_u8(dst + j, vreinterpret_u8_s8(vmovn_s16(v_src)));
}
for (; j < size.width; j++)
{
dst[j] = (u8)src[j];
}
}
}
}
typedef void (* lshiftConstFunc)(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride);
typedef void (* rshiftConstFunc)(const Size2D &size,
const s16 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
CONVERT_POLICY cpolicy);
} // namespace
#endif
void lshift(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
u32 shift)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (shift >= 16u)
{
for (size_t i = 0; i < size.height; ++i)
{
s16 * dst = internal::getRowPtr(dstBase, dstStride, i);
std::memset(dst, 0, sizeof(s16) * size.width);
}
return;
}
// this ugly contruction is needed to avoid:
// /usr/lib/gcc/arm-linux-gnueabihf/4.8/include/arm_neon.h:3581:59: error: argument must be a constant
// return (int16x8_t)__builtin_neon_vshl_nv8hi (__a, __b, 1);
lshiftConstFunc funcs[16] =
{
lshiftConst<0>,
lshiftConst<1>,
lshiftConst<2>,
lshiftConst<3>,
lshiftConst<4>,
lshiftConst<5>,
lshiftConst<6>,
lshiftConst<7>,
lshiftConst<8>,
lshiftConst<9>,
lshiftConst<10>,
lshiftConst<11>,
lshiftConst<12>,
lshiftConst<13>,
lshiftConst<14>,
lshiftConst<15>
}, func = funcs[shift];
func(size, srcBase, srcStride, dstBase, dstStride);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)shift;
#endif
}
void rshift(const Size2D &size,
const s16 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
u32 shift, CONVERT_POLICY cpolicy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (shift >= 16)
{
if (cpolicy == CONVERT_POLICY_WRAP)
{
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
int16x8_t v_zero = vdupq_n_s16(0);
for (size_t i = 0; i < size.height; ++i)
{
const s16 * src = internal::getRowPtr(srcBase, srcStride, i);
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vld1q_s16(src + j), v_src1 = vld1q_s16(src + j + 8);
uint8x16_t v_dst = vcombine_u8(vmovn_u16(vcltq_s16(v_src0, v_zero)),
vmovn_u16(vcltq_s16(v_src1, v_zero)));
vst1q_u8(dst + j, v_dst);
}
for (; j < roiw8; j += 8)
{
int16x8_t v_src = vld1q_s16(src + j);
vst1_u8(dst + j, vmovn_u16(vcltq_s16(v_src, v_zero)));
}
for (; j < size.width; j++)
{
dst[j] = src[j] >= 0 ? 0 : 255;
}
}
}
else
{
for (size_t i = 0; i < size.height; ++i)
{
u8 * dst = internal::getRowPtr(dstBase, dstStride, i);
std::memset(dst, 0, sizeof(u8) * size.width);
}
}
return;
}
// this ugly contruction is needed to avoid:
// /usr/lib/gcc/arm-linux-gnueabihf/4.8/include/arm_neon.h:3581:59: error: argument must be a constant
// return (int16x8_t)__builtin_neon_vshr_nv8hi (__a, __b, 1);
rshiftConstFunc funcs[16] =
{
rshiftConst<0>,
rshiftConst<1>,
rshiftConst<2>,
rshiftConst<3>,
rshiftConst<4>,
rshiftConst<5>,
rshiftConst<6>,
rshiftConst<7>,
rshiftConst<8>,
rshiftConst<9>,
rshiftConst<10>,
rshiftConst<11>,
rshiftConst<12>,
rshiftConst<13>,
rshiftConst<14>,
rshiftConst<15>
}, func = funcs[shift];
func(size, srcBase, srcStride, dstBase, dstStride, cpolicy);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)shift;
(void)cpolicy;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vround_helper.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
#define CVTS_FUNC(T1, T2, SIMD_SIZE, CVTINIT, CVTROW)                            \
void convertScale(const Size2D &_size,                                       \
const T1 * srcBase, ptrdiff_t srcStride,                   \
T2 * dstBase, ptrdiff_t dstStride,                         \
f64 alpha, f64 beta)                                       \
{                                                                            \
internal::assertSupportedConfiguration();                                \
Size2D size(_size);                                                      \
if (srcStride == dstStride &&                                            \
srcStride == (ptrdiff_t)(size.width))                                \
{                                                                        \
size.width *= size.height;                                           \
size.height = 1;                                                     \
}                                                                        \
const ptrdiff_t sstep = srcStride / sizeof(T1);                          \
const ptrdiff_t dstep = dstStride / sizeof(T2);                          \
const size_t w = size.width & ~(SIMD_SIZE-1);                            \
if (size.width >= SIMD_SIZE)                                             \
{                                                                        \
const T1* _src = srcBase;                                            \
T2* _dst = dstBase;                                                  \
CVTINIT                                                              \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep )  \
CVTROW                                                           \
}                                                                        \
if(w < size.width)                                                       \
{                                                                        \
const T1* _src = srcBase;                                            \
T2* _dst = dstBase;                                                  \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep )  \
for(size_t i = w; i < size.width; i++ )                          \
_dst[i] = internal::saturate_cast<T2>(_src[i]*alpha + beta); \
}                                                                        \
}
#define CVTS_FUNC1(T1, SIMD_SIZE, CVTSINIT, CVTSROW)                             \
void convertScale(const Size2D &_size,                                       \
const T1 * srcBase, ptrdiff_t srcStride,                   \
T1 * dstBase, ptrdiff_t dstStride,                         \
f64 alpha, f64 beta)                                       \
{                                                                            \
internal::assertSupportedConfiguration();                                \
Size2D size(_size);                                                      \
if (srcStride == dstStride &&                                            \
srcStride == (ptrdiff_t)(size.width))                                \
{                                                                        \
size.width *= size.height;                                           \
size.height = 1;                                                     \
}                                                                        \
const ptrdiff_t sstep = srcStride / sizeof(T1);                          \
const ptrdiff_t dstep = dstStride / sizeof(T1);                          \
const size_t w = size.width & ~(SIMD_SIZE-1);                            \
if (size.width >= SIMD_SIZE)                                             \
{                                                                        \
const T1* _src = srcBase;                                            \
T1* _dst = dstBase;                                                  \
CVTSINIT                                                             \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep )  \
CVTSROW                                                          \
}                                                                        \
if(w < size.width)                                                       \
{                                                                        \
const T1* _src = srcBase;                                            \
T1* _dst = dstBase;                                                  \
for (ptrdiff_t h = size.height; h--; _src += sstep, _dst += dstep )  \
for(size_t i = w; i < size.width; i++ )                          \
_dst[i] = internal::saturate_cast<T1>(_src[i]*alpha + beta); \
}                                                                        \
}
#else
#define CVTS_FUNC(T1, T2, SIMD_SIZE, CVTINIT, CVTROW)                            \
void convertScale(const Size2D &,                                            \
const T1 *, ptrdiff_t,                                     \
T2 *, ptrdiff_t,                                           \
f64, f64)                                                  \
{                                                                            \
internal::assertSupportedConfiguration();                                \
}
#define CVTS_FUNC1(T1, SIMD_SIZE, CVTSINIT, CVTSROW)                             \
void convertScale(const Size2D &,                                            \
const T1 *, ptrdiff_t,                                     \
T1 *, ptrdiff_t,                                           \
f64, f64)                                                  \
{                                                                            \
internal::assertSupportedConfiguration();                                \
}
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC1(u8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovun.s32 d22, q7                                   \n\t"
"vqmovun.s32 d23, q8                                   \n\t"
"vqmovun.s32 d24, q9                                   \n\t"
"vqmovun.s32 d25, q10                                  \n\t"
"vqmovn.u16 d26, q11                                   \n\t"
"vqmovn.u16 d27, q12                                   \n\t"
"vst1.8 {d26-d27}, [%[dst1]]                           \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC1(u8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int32x4_t vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
int32x4_t vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
uint16x8_t vRes1_u16 = vcombine_u16(vqmovun_s32(vline1_s32), vqmovun_s32(vline2_s32));
uint16x8_t vRes2_u16 = vcombine_u16(vqmovun_s32(vline3_s32), vqmovun_s32(vline4_s32));
vst1q_u8(_dst + i, vcombine_u8(vqmovn_u16(vRes1_u16), vqmovn_u16(vRes2_u16)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(u8, s8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovn.s32 d22, q7                                    \n\t"
"vqmovn.s32 d23, q8                                    \n\t"
"vqmovn.s32 d24, q9                                    \n\t"
"vqmovn.s32 d25, q10                                   \n\t"
"vqmovn.s16 d26, q11                                   \n\t"
"vqmovn.s16 d27, q12                                   \n\t"
"vst1.8 {d26-d27}, [%[dst1]]                           \n\t"
: //no output
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(u8, s8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int32x4_t vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
int32x4_t vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
int16x8_t vRes1_u16 = vcombine_s16(vqmovn_s32(vline1_s32), vqmovn_s32(vline2_s32));
int16x8_t vRes2_u16 = vcombine_s16(vqmovn_s32(vline3_s32), vqmovn_s32(vline4_s32));
vst1q_s8(_dst + i, vcombine_s8(vqmovn_s16(vRes1_u16), vqmovn_s16(vRes2_u16)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(u8, u16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovun.s32 d22, q7                                   \n\t"
"vqmovun.s32 d23, q8                                   \n\t"
"vqmovun.s32 d24, q9                                   \n\t"
"vqmovun.s32 d25, q10                                  \n\t"
"vst1.16 {d22-d23}, [%[dst1]]                          \n\t"
"vst1.16 {d24-d25}, [%[dst2]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(u8, u16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int32x4_t vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
int32x4_t vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
vst1q_u16(_dst + i + 0, vcombine_u16(vqmovun_s32(vline1_s32), vqmovun_s32(vline2_s32)));
vst1q_u16(_dst + i + 8, vcombine_u16(vqmovun_s32(vline3_s32), vqmovun_s32(vline4_s32)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(u8, s16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovn.s32 d22, q7                                    \n\t"
"vqmovn.s32 d23, q8                                    \n\t"
"vqmovn.s32 d24, q9                                    \n\t"
"vqmovn.s32 d25, q10                                   \n\t"
"vst1.16 {d22-d23}, [%[dst1]]                          \n\t"
"vst1.16 {d24-d25}, [%[dst2]]                          \n\t"
: //no output
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(u8, s16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int32x4_t vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
int32x4_t vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
vst1q_s16(_dst + i + 0, vcombine_s16(vqmovn_s32(vline1_s32), vqmovn_s32(vline2_s32)));
vst1q_s16(_dst + i + 8, vcombine_s16(vqmovn_s32(vline3_s32), vqmovn_s32(vline4_s32)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u8, s32, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vst1.32 {d14-d15}, [%[dst1]]                          \n\t"
"vst1.32 {d16-d17}, [%[dst2]]                          \n\t"
"vst1.32 {d18-d19}, [%[dst3]]                          \n\t"
"vst1.32 {d20-d21}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10",
"d11","d12","d13","d14","d15","d16","d17",
"d18","d19","d20","d21","d22","d23","d24",
"d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(u8, s32, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int32x4_t vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
int32x4_t vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
vst1q_s32(_dst + i + 0,  vline1_s32);
vst1q_s32(_dst + i + 4,  vline2_s32);
vst1q_s32(_dst + i + 8,  vline3_s32);
vst1q_s32(_dst + i + 12, vline4_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u8, f32, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u8 q3, d4                                       \n\t"
"vmovl.u8 q4, d5                                       \n\t"
"vmovl.u16 q5, d6                                      \n\t"
"vmovl.u16 q6, d7                                      \n\t"
"vmovl.u16 q7, d8                                      \n\t"
"vmovl.u16 q8, d9                                      \n\t"
"vcvt.f32.u32 q9, q5                                   \n\t"
"vcvt.f32.u32 q10, q6                                  \n\t"
"vcvt.f32.u32 q11, q7                                  \n\t"
"vcvt.f32.u32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vst1.32 {d6-d7}, [%[dst1]]                            \n\t"
"vst1.32 {d8-d9}, [%[dst2]]                            \n\t"
"vst1.32 {d10-d11}, [%[dst3]]                          \n\t"
"vst1.32 {d12-d13}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10",
"d11","d12","d13","d14","d15","d16","d17",
"d18","d19","d20","d21","d22","d23","d24",
"d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(u8, f32, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
uint8x16_t vline = vld1q_u8(_src + i);
uint16x8_t vline1_u16 = vmovl_u8(vget_low_u8 (vline));
uint16x8_t vline2_u16 = vmovl_u8(vget_high_u8(vline));
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline1_u16));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline1_u16));
uint32x4_t vline3_u32 = vmovl_u16(vget_low_u16 (vline2_u16));
uint32x4_t vline4_u32 = vmovl_u16(vget_high_u16(vline2_u16));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
float32x4_t vline3_f32 = vcvtq_f32_u32(vline3_u32);
float32x4_t vline4_f32 = vcvtq_f32_u32(vline4_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vst1q_f32(_dst + i + 0,  vline1_f32);
vst1q_f32(_dst + i + 4,  vline2_f32);
vst1q_f32(_dst + i + 8,  vline3_f32);
vst1q_f32(_dst + i + 12, vline4_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(s8, u8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovun.s32 d22, q7                                   \n\t"
"vqmovun.s32 d23, q8                                   \n\t"
"vqmovun.s32 d24, q9                                   \n\t"
"vqmovun.s32 d25, q10                                  \n\t"
"vqmovn.u16 d26, q11                                   \n\t"
"vqmovn.u16 d27, q12                                   \n\t"
"vst1.8 {d26-d27}, [%[dst1]]                           \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(s8, u8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
uint16x8_t vRes1_u16 = vcombine_u16(vqmovun_s32(vline1_s32), vqmovun_s32(vline2_s32));
uint16x8_t vRes2_u16 = vcombine_u16(vqmovun_s32(vline3_s32), vqmovun_s32(vline4_s32));
vst1q_u8(_dst + i, vcombine_u8(vqmovn_u16(vRes1_u16), vqmovn_u16(vRes2_u16)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC1(s8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovn.s32 d22, q7                                    \n\t"
"vqmovn.s32 d23, q8                                    \n\t"
"vqmovn.s32 d24, q9                                    \n\t"
"vqmovn.s32 d25, q10                                   \n\t"
"vqmovn.s16 d26, q11                                   \n\t"
"vqmovn.s16 d27, q12                                   \n\t"
"vst1.8 {d26-d27}, [%[dst1]]                           \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC1(s8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
int16x8_t vRes1_s16 = vcombine_s16(vqmovn_s32(vline1_s32), vqmovn_s32(vline2_s32));
int16x8_t vRes2_s16 = vcombine_s16(vqmovn_s32(vline3_s32), vqmovn_s32(vline4_s32));
vst1q_s8(_dst + i, vcombine_s8(vqmovn_s16(vRes1_s16), vqmovn_s16(vRes2_s16)));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(s8, u16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovun.s32 d22, q7                                   \n\t"
"vqmovun.s32 d23, q8                                   \n\t"
"vqmovun.s32 d24, q9                                   \n\t"
"vqmovun.s32 d25, q10                                  \n\t"
"vst1.16 {d22-d23}, [%[dst1]]                          \n\t"
"vst1.16 {d24-d25}, [%[dst2]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(s8, u16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
uint16x8_t vRes1_u16 = vcombine_u16(vqmovun_s32(vline1_s32), vqmovun_s32(vline2_s32));
uint16x8_t vRes2_u16 = vcombine_u16(vqmovun_s32(vline3_s32), vqmovun_s32(vline4_s32));
vst1q_u16(_dst + i + 0, vRes1_u16);
vst1q_u16(_dst + i + 8, vRes2_u16);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
CVTS_FUNC(s8, s16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vqmovn.s32 d22, q7                                    \n\t"
"vqmovn.s32 d23, q8                                    \n\t"
"vqmovn.s32 d24, q9                                    \n\t"
"vqmovn.s32 d25, q10                                   \n\t"
"vst1.16 {d22-d23}, [%[dst1]]                          \n\t"
"vst1.16 {d24-d25}, [%[dst2]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 8),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(s8, s16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
int16x8_t vRes1_s16 = vcombine_s16(vqmovn_s32(vline1_s32), vqmovn_s32(vline2_s32));
int16x8_t vRes2_s16 = vcombine_s16(vqmovn_s32(vline3_s32), vqmovn_s32(vline4_s32));
vst1q_s16(_dst + i + 0, vRes1_s16);
vst1q_s16(_dst + i + 8, vRes2_s16);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s8, s32, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vcvt.s32.f32 q7, q3                                   \n\t"
"vcvt.s32.f32 q8, q4                                   \n\t"
"vcvt.s32.f32 q9, q5                                   \n\t"
"vcvt.s32.f32 q10, q6                                  \n\t"
"vst1.32 {d14-d15}, [%[dst1]]                          \n\t"
"vst1.32 {d16-d17}, [%[dst2]]                          \n\t"
"vst1.32 {d18-d19}, [%[dst3]]                          \n\t"
"vst1.32 {d20-d21}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10",
"d11","d12","d13","d14","d15","d16","d17",
"d18","d19","d20","d21","d22","d23","d24",
"d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(s8, s32, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vline3_s32 = internal::vroundq_s32_f32(vline3_f32);
vline4_s32 = internal::vroundq_s32_f32(vline4_f32);
vst1q_s32(_dst + i + 0,  vline1_s32);
vst1q_s32(_dst + i + 4,  vline2_s32);
vst1q_s32(_dst + i + 8,  vline3_s32);
vst1q_s32(_dst + i + 12, vline4_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s8, f32, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s8 q3, d4                                       \n\t"
"vmovl.s8 q4, d5                                       \n\t"
"vmovl.s16 q5, d6                                      \n\t"
"vmovl.s16 q6, d7                                      \n\t"
"vmovl.s16 q7, d8                                      \n\t"
"vmovl.s16 q8, d9                                      \n\t"
"vcvt.f32.s32 q9, q5                                   \n\t"
"vcvt.f32.s32 q10, q6                                  \n\t"
"vcvt.f32.s32 q11, q7                                  \n\t"
"vcvt.f32.s32 q12, q8                                  \n\t"
"vmul.f32 q13, q9, q0                                  \n\t"
"vmul.f32 q14, q10, q0                                 \n\t"
"vmul.f32 q15, q11, q0                                 \n\t"
"vmul.f32 q2, q12, q0                                  \n\t"
"vadd.f32 q3, q13, q1                                  \n\t"
"vadd.f32 q4, q14, q1                                  \n\t"
"vadd.f32 q5, q15, q1                                  \n\t"
"vadd.f32 q6, q2, q1                                   \n\t"
"vst1.32 {d6-d7}, [%[dst1]]                            \n\t"
"vst1.32 {d8-d9}, [%[dst2]]                            \n\t"
"vst1.32 {d10-d11}, [%[dst3]]                          \n\t"
"vst1.32 {d12-d13}, [%[dst4]]                          \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
[dst3] "r" (_dst + i + 8),
[dst4] "r" (_dst + i + 12),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10",
"d11","d12","d13","d14","d15","d16","d17",
"d18","d19","d20","d21","d22","d23","d24",
"d25","d26","d27","d28","d29","d30","d31"
);
}
})
#else
CVTS_FUNC(s8, f32, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 16)
{
internal::prefetch(_src + i);
int8x16_t vline = vld1q_s8(_src + i);
int16x8_t vline1_s16 = vmovl_s8(vget_low_s8 (vline));
int16x8_t vline2_s16 = vmovl_s8(vget_high_s8(vline));
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline1_s16));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline1_s16));
int32x4_t vline3_s32 = vmovl_s16(vget_low_s16 (vline2_s16));
int32x4_t vline4_s32 = vmovl_s16(vget_high_s16(vline2_s16));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
float32x4_t vline3_f32 = vcvtq_f32_s32(vline3_s32);
float32x4_t vline4_f32 = vcvtq_f32_s32(vline4_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline3_f32 = vmulq_f32(vline3_f32, vscale);
vline4_f32 = vmulq_f32(vline4_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline3_f32 = vaddq_f32(vline3_f32, vshift);
vline4_f32 = vaddq_f32(vline4_f32, vshift);
vst1q_f32(_dst + i + 0,  vline1_f32);
vst1q_f32(_dst + i + 4,  vline2_f32);
vst1q_f32(_dst + i + 8,  vline3_f32);
vst1q_f32(_dst + i + 12, vline4_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u16, u8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src1]]                             \n\t"
"vmovl.u16 q3, d4                                      \n\t"
"vmovl.u16 q4, d5                                      \n\t"
"vcvt.f32.u32 q5, q3                                   \n\t"
"vcvt.f32.u32 q6, q4                                   \n\t"
"vmul.f32 q7, q5, q0                                   \n\t"
"vmul.f32 q8, q6, q0                                   \n\t"
"vadd.f32 q9, q7, q1                                   \n\t"
"vadd.f32 q10, q8, q1                                  \n\t"
"vcvt.s32.f32 q11, q9                                  \n\t"
"vcvt.s32.f32 q12, q10                                 \n\t"
"vqmovn.s32 d26, q11                                   \n\t"
"vqmovn.s32 d27, q12                                   \n\t"
"vqmovun.s16 d28, q13                                  \n\t"
"vst1.8 {d28}, [%[dst]]                               \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28"
);
}
})
#else
CVTS_FUNC(u16, u8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
uint8x8_t vRes = vqmovun_s16(vcombine_s16(vRes1, vRes2));
vst1_u8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u16, s8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src1]]                             \n\t"
"vmovl.u16 q3, d4                                      \n\t"
"vmovl.u16 q4, d5                                      \n\t"
"vcvt.f32.u32 q5, q3                                   \n\t"
"vcvt.f32.u32 q6, q4                                   \n\t"
"vmul.f32 q7, q5, q0                                   \n\t"
"vmul.f32 q8, q6, q0                                   \n\t"
"vadd.f32 q9, q7, q1                                   \n\t"
"vadd.f32 q10, q8, q1                                  \n\t"
"vcvt.s32.f32 q11, q9                                  \n\t"
"vcvt.s32.f32 q12, q10                                 \n\t"
"vqmovn.s32 d26, q11                                   \n\t"
"vqmovn.s32 d27, q12                                   \n\t"
"vqmovn.s16 d28, q13                                   \n\t"
"vst1.8 {d28}, [%[dst]]                                \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28"
);
}
})
#else
CVTS_FUNC(u16, s8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
int8x8_t vRes = vqmovn_s16(vcombine_s16(vRes1, vRes2));
vst1_s8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC1(u16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u16 q3, d4                                       \n\t"
"vmovl.u16 q4, d5                                       \n\t"
"vcvt.f32.u32 q5, q3                                    \n\t"
"vcvt.f32.u32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vcvt.s32.f32 q12, q10                                  \n\t"
"vqmovun.s32 d26, q11                                   \n\t"
"vqmovun.s32 d27, q12                                   \n\t"
"vst1.16 {d26-d27}, [%[dst]]                            \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w" (vshift), "w" (vscale)
: "d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27"
);
}
})
#else
CVTS_FUNC1(u16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
uint16x4_t vRes1 = vqmovun_s32(vline1_s32);
uint16x4_t vRes2 = vqmovun_s32(vline2_s32);
vst1q_u16(_dst + i, vcombine_u16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u16, s16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u16 q3, d4                                       \n\t"
"vmovl.u16 q4, d5                                       \n\t"
"vcvt.f32.u32 q5, q3                                    \n\t"
"vcvt.f32.u32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vcvt.s32.f32 q12, q10                                  \n\t"
"vqmovn.s32 d26, q11                                    \n\t"
"vqmovn.s32 d27, q12                                    \n\t"
"vst1.16 {d26-d27}, [%[dst]]                            \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w" (vshift), "w" (vscale)
: "d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27"
);
}
})
#else
CVTS_FUNC(u16, s16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
vst1q_s16(_dst + i, vcombine_s16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u16, s32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                        \n\t"
"vmovl.u16 q3, d4                                 \n\t"
"vmovl.u16 q4, d5                                 \n\t"
"vcvt.f32.u32 q5, q3                              \n\t"
"vcvt.f32.u32 q6, q4                              \n\t"
"vmul.f32 q7, q5, q0                              \n\t"
"vmul.f32 q8, q6, q0                              \n\t"
"vadd.f32 q9, q7, q1                              \n\t"
"vadd.f32 q10, q8, q1                             \n\t"
"vcvt.s32.f32 q11, q9                             \n\t"
"vcvt.s32.f32 q12, q10                            \n\t"
"vst1.32 {d22-d23}, [%[dst1]]                     \n\t"
"vst1.32 {d24-d25}, [%[dst2]]                     \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i),
[dst2] "r" (_dst + i + 4),
"w" (vshift), "w" (vscale)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25"
);
}
})
#else
CVTS_FUNC(u16, s32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vst1q_s32(_dst + i + 0, vline1_s32);
vst1q_s32(_dst + i + 4, vline2_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(u16, f32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.u16 q3, d4                                       \n\t"
"vmovl.u16 q4, d5                                       \n\t"
"vcvt.f32.u32 q5, q3                                    \n\t"
"vcvt.f32.u32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vst1.32 {d18-d19}, [%[dst1]]                           \n\t"
"vst1.32 {d20-d21}, [%[dst2]]                           \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21"
);
}
})
#else
CVTS_FUNC(u16, f32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
uint16x8_t vline = vld1q_u16(_src + i);
uint32x4_t vline1_u32 = vmovl_u16(vget_low_u16 (vline));
uint32x4_t vline2_u32 = vmovl_u16(vget_high_u16(vline));
float32x4_t vline1_f32 = vcvtq_f32_u32(vline1_u32);
float32x4_t vline2_f32 = vcvtq_f32_u32(vline2_u32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vst1q_f32(_dst + i + 0, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s16, u8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src1]]                             \n\t"
"vmovl.s16 q3, d4                                      \n\t"
"vmovl.s16 q4, d5                                      \n\t"
"vcvt.f32.s32 q5, q3                                   \n\t"
"vcvt.f32.s32 q6, q4                                   \n\t"
"vmul.f32 q7, q5, q0                                   \n\t"
"vmul.f32 q8, q6, q0                                   \n\t"
"vadd.f32 q9, q7, q1                                   \n\t"
"vadd.f32 q10, q8, q1                                  \n\t"
"vcvt.s32.f32 q11, q9                                  \n\t"
"vcvt.s32.f32 q12, q10                                 \n\t"
"vqmovn.s32 d26, q11                                   \n\t"
"vqmovn.s32 d27, q12                                   \n\t"
"vqmovun.s16 d28, q13                                  \n\t"
"vst1.8 {d28}, [%[dst]]                                \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28"
);
}
})
#else
CVTS_FUNC(s16, u8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
uint8x8_t vRes = vqmovun_s16(vcombine_s16(vRes1, vRes2));
vst1_u8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s16, s8, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.8 {d4-d5}, [%[src1]]                             \n\t"
"vmovl.s16 q3, d4                                      \n\t"
"vmovl.s16 q4, d5                                      \n\t"
"vcvt.f32.s32 q5, q3                                   \n\t"
"vcvt.f32.s32 q6, q4                                   \n\t"
"vmul.f32 q7, q5, q0                                   \n\t"
"vmul.f32 q8, q6, q0                                   \n\t"
"vadd.f32 q9, q7, q1                                   \n\t"
"vadd.f32 q10, q8, q1                                  \n\t"
"vcvt.s32.f32 q11, q9                                  \n\t"
"vcvt.s32.f32 q12, q10                                 \n\t"
"vqmovn.s32 d26, q11                                   \n\t"
"vqmovn.s32 d27, q12                                   \n\t"
"vqmovn.s16 d28, q13                                   \n\t"
"vst1.8 {d28}, [%[dst]]                                \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28"
);
}
})
#else
CVTS_FUNC(s16, s8, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
int8x8_t vRes = vqmovn_s16(vcombine_s16(vRes1, vRes2));
vst1_s8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s16, u16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s16 q3, d4                                       \n\t"
"vmovl.s16 q4, d5                                       \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vcvt.f32.s32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vcvt.s32.f32 q12, q10                                  \n\t"
"vqmovun.s32 d26, q11                                   \n\t"
"vqmovun.s32 d27, q12                                   \n\t"
"vst1.16 {d26-d27}, [%[dst]]                            \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27"
);
}
})
#else
CVTS_FUNC(s16, u16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
uint16x4_t vRes1 = vqmovun_s32(vline1_s32);
uint16x4_t vRes2 = vqmovun_s32(vline2_s32);
vst1q_u16(_dst + i, vcombine_u16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC1(s16, 16,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s16 q3, d4                                       \n\t"
"vmovl.s16 q4, d5                                       \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vcvt.f32.s32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vcvt.s32.f32 q12, q10                                  \n\t"
"vqmovn.s32 d26, q11                                    \n\t"
"vqmovn.s32 d27, q12                                    \n\t"
"vst1.16 {d26-d27}, [%[dst]]                            \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst] "r" (_dst + i + 0),
"w" (vshift), "w" (vscale)
: "d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27"
);
}
})
#else
CVTS_FUNC1(s16, 16,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
vst1q_s16(_dst + i, vcombine_s16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s16, s32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s16 q3, d4                                       \n\t"
"vmovl.s16 q4, d5                                       \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vcvt.f32.s32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vadd.f32 q10, q8, q1                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vcvt.s32.f32 q12, q10                                  \n\t"
"vst1.32 {d22-d23}, [%[dst1]]                           \n\t"
"vst1.32 {d24-d25}, [%[dst2]]                           \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25"
);
}
})
#else
CVTS_FUNC(s16, s32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vst1q_s32(_dst + i + 0, vline1_s32);
vst1q_s32(_dst + i + 4, vline2_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s16, f32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.16 {d4-d5}, [%[src]]                              \n\t"
"vmovl.s16 q3, d4                                       \n\t"
"vmovl.s16 q4, d5                                       \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vcvt.f32.s32 q6, q4                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vmul.f32 q8, q6, q0                                    \n\t"
"vadd.f32 q9, q7, q1                                     \n\t"
"vadd.f32 q10, q8, q1                                     \n\t"
"vst1.32 {d18-d19}, [%[dst1]]                             \n\t"
"vst1.32 {d20-d21}, [%[dst2]]                             \n\t"
: /*no output*/
: [src] "r" (_src + i),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21"
);
}
})
#else
CVTS_FUNC(s16, f32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int16x8_t vline = vld1q_s16(_src + i);
int32x4_t vline1_s32 = vmovl_s16(vget_low_s16 (vline));
int32x4_t vline2_s32 = vmovl_s16(vget_high_s16(vline));
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vst1q_f32(_dst + i + 0, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s32, u8, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vcvt.f32.s32 q4, q2                                     \n\t"
"vcvt.f32.s32 q5, q3                                     \n\t"
"vmul.f32 q6, q4, q0                                     \n\t"
"vmul.f32 q7, q5, q0                                     \n\t"
"vadd.f32 q8, q6, q1                                     \n\t"
"vadd.f32 q9, q7, q1                                     \n\t"
"vcvt.s32.f32 q10, q8                                    \n\t"
"vcvt.s32.f32 q11, q9                                    \n\t"
"vqmovun.s32 d24, q10                                    \n\t"
"vqmovun.s32 d25, q11                                    \n\t"
"vqmovn.u16  d26, q12                                    \n\t"
"vst1.8 {d26}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26"
);
}
})
#else
CVTS_FUNC(s32, u8, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
uint16x4_t vRes1 = vqmovun_s32(vline1_s32);
uint16x4_t vRes2 = vqmovun_s32(vline2_s32);
uint8x8_t vRes = vqmovn_u16(vcombine_u16(vRes1, vRes2));
vst1_u8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s32, s8, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vcvt.f32.s32 q4, q2                                     \n\t"
"vcvt.f32.s32 q5, q3                                     \n\t"
"vmul.f32 q6, q4, q0                                     \n\t"
"vmul.f32 q7, q5, q0                                     \n\t"
"vadd.f32 q8, q6, q1                                     \n\t"
"vadd.f32 q9, q7, q1                                     \n\t"
"vcvt.s32.f32 q10, q8                                    \n\t"
"vcvt.s32.f32 q11, q9                                    \n\t"
"vqmovn.s32 d24, q10                                     \n\t"
"vqmovn.s32 d25, q11                                     \n\t"
"vqmovn.s16  d26, q12                                    \n\t"
"vst1.8 {d26}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26"
);
}
})
#else
CVTS_FUNC(s32, s8, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
int8x8_t vRes = vqmovn_s16(vcombine_s16(vRes1, vRes2));
vst1_s8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s32, u16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                             \n\t"
"vld1.32 {d6-d7}, [%[src2]]                             \n\t"
"vcvt.f32.s32 q4, q2                                    \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vmul.f32 q6, q4, q0                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vadd.f32 q8, q6, q1                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vcvt.s32.f32 q10, q8                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vqmovun.s32 d24, q10                                   \n\t"
"vqmovun.s32 d25, q11                                   \n\t"
"vst1.16 {d24-d25}, [%[dst]]                            \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25"
);
}
})
#else
CVTS_FUNC(s32, u16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
uint16x4_t vRes1 = vqmovun_s32(vline1_s32);
uint16x4_t vRes2 = vqmovun_s32(vline2_s32);
vst1q_u16(_dst + i, vcombine_u16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s32, s16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                             \n\t"
"vld1.32 {d6-d7}, [%[src2]]                             \n\t"
"vcvt.f32.s32 q4, q2                                    \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vmul.f32 q6, q4, q0                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vadd.f32 q8, q6, q1                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vcvt.s32.f32 q10, q8                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vqmovn.s32 d24, q10                                    \n\t"
"vqmovn.s32 d25, q11                                    \n\t"
"vst1.8 {d24-d25}, [%[dst]]                             \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25"
);
}
})
#else
CVTS_FUNC(s32, s16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
vst1q_s16(_dst + i, vcombine_s16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC1(s32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                             \n\t"
"vld1.32 {d6-d7}, [%[src2]]                             \n\t"
"vcvt.f32.s32 q4, q2                                    \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vmul.f32 q6, q4, q0                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vadd.f32 q8, q6, q1                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vcvt.s32.f32 q10, q8                                   \n\t"
"vcvt.s32.f32 q11, q9                                   \n\t"
"vst1.32 {d20-d21}, [%[dst1]]                           \n\t"
"vst1.32 {d22-d23}, [%[dst2]]                           \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23"
);
}
})
#else
CVTS_FUNC1(s32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vst1q_s32(_dst + i + 0, vline1_s32);
vst1q_s32(_dst + i + 4, vline2_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(s32, f32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                             \n\t"
"vld1.32 {d6-d7}, [%[src2]]                             \n\t"
"vcvt.f32.s32 q4, q2                                    \n\t"
"vcvt.f32.s32 q5, q3                                    \n\t"
"vmul.f32 q6, q4, q0                                    \n\t"
"vmul.f32 q7, q5, q0                                    \n\t"
"vadd.f32 q8, q6, q1                                    \n\t"
"vadd.f32 q9, q7, q1                                    \n\t"
"vst1.32 {d16-d17}, [%[dst1]]                           \n\t"
"vst1.32 {d18-d19}, [%[dst2]]                           \n\t"
: /*no output*/
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 4),
[dst1] "r" (_dst + i),
[dst2] "r" (_dst + i + 4),
"w"  (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19"
);
}
})
#else
CVTS_FUNC(s32, f32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
int32x4_t vline1_s32 = vld1q_s32(_src + i + 0);
int32x4_t vline2_s32 = vld1q_s32(_src + i + 4);
float32x4_t vline1_f32 = vcvtq_f32_s32(vline1_s32);
float32x4_t vline2_f32 = vcvtq_f32_s32(vline2_s32);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vst1q_f32(_dst + i + 0, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(f32, u8, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)((1 << 16)*alpha));
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)((1 << 16)*beta));
register uint32x4_t  vmask  asm ("q2") = vdupq_n_u32(1<<16);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d6-d7}, [%[src1]]                              \n\t"
"vld1.32 {d8-d9}, [%[src2]]                              \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vmul.f32 q6, q4, q0                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vadd.f32 q8, q6, q1                                     \n\t"
"vcvt.u32.f32 q9, q7                                     \n\t"
"vcvt.u32.f32 q10, q8                                    \n\t"
"vbic q11, q2, q6                                        \n\t"
"vbic q12, q2, q7                                        \n\t"
"vshr.u32 q13, q11, #16                                  \n\t"
"vshr.u32 q14, q12, #16                                  \n\t"
"vqsub.u32 q7, q9, q13                                   \n\t"
"vqsub.u32 q8, q10, q14                                  \n\t"
"vqrshrn.u32 d22, q7, #16                                \n\t"
"vqrshrn.u32 d23, q8, #16                                \n\t"
"vqmovn.u16 d30, q11                                     \n\t"
"vst1.8 {d30}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vscale), "w" (vshift), "w" (vmask)
: "d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23","d24","d25","d26","d27","d28","d29","d30"
);
}
})
#else
CVTS_FUNC(f32, u8, 8,
float32x4_t vscale = vdupq_n_f32((f32)((1 << 16)*alpha));
float32x4_t vshift = vdupq_n_f32((f32)((1 << 16)*beta));
uint32x4_t  vmask  = vdupq_n_u32(1<<16);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
float32x4_t vline1Shifted_f32 = vaddq_f32(vline1_f32, vshift);
float32x4_t vline2Shifted_f32 = vaddq_f32(vline2_f32, vshift);
uint32x4_t vline1_u32 = vcvtq_u32_f32(vline1Shifted_f32);
uint32x4_t vline2_u32 = vcvtq_u32_f32(vline2Shifted_f32);
uint32x4_t vline1Mask = vbicq_u32(vmask, vreinterpretq_u32_f32(vline2_f32));
uint32x4_t vline2Mask = vbicq_u32(vmask, vreinterpretq_u32_f32(vline1Shifted_f32));
vline1Mask = vshrq_n_u32(vline1Mask, 16);
vline2Mask = vshrq_n_u32(vline2Mask, 16);
vline1_u32 = vqsubq_u32(vline1_u32, vline1Mask);
vline2_u32 = vqsubq_u32(vline2_u32, vline2Mask);
uint16x4_t vRes1 = vqrshrn_n_u32(vline1_u32, 16);
uint16x4_t vRes2 = vqrshrn_n_u32(vline2_u32, 16);
uint8x8_t vRes = vqmovn_u16(vcombine_u16(vRes1, vRes2));
vst1_u8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(f32, s8, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vadd.f32 q6, q4, q1                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vcvt.s32.f32 q8, q6                                     \n\t"
"vcvt.s32.f32 q9, q7                                     \n\t"
"vqmovn.s32 d14, q8                                      \n\t"
"vqmovn.s32 d15, q9                                      \n\t"
"vqmovn.s16 d16, q7                                      \n\t"
"vst1.8 {d16}, [%[dst]]                                  \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19"
);
}
})
#else
CVTS_FUNC(f32, s8, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
int8x8_t vRes = vqmovn_s16(vcombine_s16(vRes1, vRes2));
vst1_s8(_dst + i, vRes);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(f32, u16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vadd.f32 q6, q4, q1                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vcvt.u32.f32 q8, q6                                     \n\t"
"vcvt.u32.f32 q9, q7                                     \n\t"
"vqmovn.u32 d8, q8                                       \n\t"
"vqmovn.u32 d9, q9                                       \n\t"
"vst1.16 {d8-d9}, [%[dst]]                               \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19"
);
}
})
#else
CVTS_FUNC(f32, u16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
uint32x4_t vline1_u32 = internal::vroundq_u32_f32(vline1_f32);
uint32x4_t vline2_u32 = internal::vroundq_u32_f32(vline2_f32);
uint16x4_t vRes1 = vqmovn_u32(vline1_u32);
uint16x4_t vRes2 = vqmovn_u32(vline2_u32);
vst1q_u16(_dst + i, vcombine_u16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(f32, s16, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vadd.f32 q6, q4, q1                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vcvt.s32.f32 q8, q6                                     \n\t"
"vcvt.s32.f32 q9, q7                                     \n\t"
"vqmovn.s32 d8, q8                                       \n\t"
"vqmovn.s32 d9, q9                                       \n\t"
"vst1.16 {d8-d9}, [%[dst]]                               \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst] "r" (_dst + i),
"w" (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19"
);
}
})
#else
CVTS_FUNC(f32, s16, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
int16x4_t vRes1 = vqmovn_s32(vline1_s32);
int16x4_t vRes2 = vqmovn_s32(vline2_s32);
vst1q_s16(_dst + i, vcombine_s16(vRes1, vRes2));
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC(f32, s32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta + 0.5f);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vadd.f32 q6, q4, q1                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vcvt.s32.f32 q4, q6                                     \n\t"
"vcvt.s32.f32 q5, q7                                     \n\t"
"vst1.32 {d8-d9},   [%[dst1]]                            \n\t"
"vst1.32 {d10-d11}, [%[dst2]]                            \n\t"
: //no output
: [src1] "r" (_src + i),
[src2] "r" (_src + i + 4),
[dst1] "r" (_dst + i),
[dst2] "r" (_dst + i + 4),
"w" (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15"
);
}
})
#else
CVTS_FUNC(f32, s32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
int32x4_t vline1_s32 = internal::vroundq_s32_f32(vline1_f32);
int32x4_t vline2_s32 = internal::vroundq_s32_f32(vline2_f32);
vst1q_s32(_dst + i + 0, vline1_s32);
vst1q_s32(_dst + i + 4, vline2_s32);
}
})
#endif
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
CVTS_FUNC1(f32, 8,
register float32x4_t vscale asm ("q0") = vdupq_n_f32((f32)alpha);
register float32x4_t vshift asm ("q1") = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
__asm__ (
"vld1.32 {d4-d5}, [%[src1]]                              \n\t"
"vld1.32 {d6-d7}, [%[src2]]                              \n\t"
"vmul.f32 q4, q2, q0                                     \n\t"
"vmul.f32 q5, q3, q0                                     \n\t"
"vadd.f32 q6, q4, q1                                     \n\t"
"vadd.f32 q7, q5, q1                                     \n\t"
"vst1.32 {d12-d13}, [%[dst1]]                            \n\t"
"vst1.32 {d14-d15}, [%[dst2]]                            \n\t"
: /*no output*/
: [src1] "r" (_src + i + 0),
[src2] "r" (_src + i + 4),
[dst1] "r" (_dst + i + 0),
[dst2] "r" (_dst + i + 4),
"w" (vscale), "w" (vshift)
: "d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19"
);
}
})
#else
CVTS_FUNC1(f32, 8,
float32x4_t vscale = vdupq_n_f32((f32)alpha);
float32x4_t vshift = vdupq_n_f32((f32)beta);,
{
for (size_t i = 0; i < w; i += 8)
{
internal::prefetch(_src + i);
float32x4_t vline1_f32 = vld1q_f32(_src + i + 0);
float32x4_t vline2_f32 = vld1q_f32(_src + i + 4);
vline1_f32 = vmulq_f32(vline1_f32, vscale);
vline2_f32 = vmulq_f32(vline2_f32, vscale);
vline1_f32 = vaddq_f32(vline1_f32, vshift);
vline2_f32 = vaddq_f32(vline2_f32, vshift);
vst1q_f32(_dst + i + 0, vline1_f32);
vst1q_f32(_dst + i + 4, vline2_f32);
}
})
#endif
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "saturate_cast.hpp"
namespace CAROTENE_NS {
bool isConvolutionSupported(const Size2D &size, const Size2D &ksize,
BORDER_MODE border)
{
return isSupportedConfiguration() && size.width >= 8 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REPLICATE) &&
(ksize.width == 3) && (ksize.height == 3);
}
#ifdef CAROTENE_NEON
namespace {
template <int shift>
int32x4_t vshrq_s32(int32x4_t value)
{
return vshrq_n_s32(value, shift);
}
template <>
int32x4_t vshrq_s32<0>(int32x4_t value)
{
return value;
}
} // namespace
typedef int32x4_t (* vshrq_s32_func)(int32x4_t value);
#endif
void convolution(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue,
const Size2D & ksize, s16 * kernelBase, u32 scale)
{
internal::assertSupportedConfiguration(isConvolutionSupported(size, ksize, border));
#ifdef CAROTENE_NEON
const uint8x8_t v_zero_u8 = vdup_n_u8(0);
const uint8x8_t v_border = vdup_n_u8(borderValue);
const int32x4_t v_zero_s32 = vdupq_n_s32(0);
uint8x8_t tprev[3] = { v_zero_u8, v_zero_u8, v_zero_u8 },
tcurr[3] = { v_zero_u8, v_zero_u8, v_zero_u8 },
tnext[3] = { v_zero_u8, v_zero_u8, v_zero_u8 };
uint8x8_t t0 = v_zero_u8, t1 = v_zero_u8, t2 = v_zero_u8;
ptrdiff_t width = (ptrdiff_t)size.width, height = (ptrdiff_t)size.height;
static const vshrq_s32_func vshrq_s32_a[33] =
{
vshrq_s32<0>,
vshrq_s32<1>,
vshrq_s32<2>,
vshrq_s32<3>,
vshrq_s32<4>,
vshrq_s32<5>,
vshrq_s32<6>,
vshrq_s32<7>,
vshrq_s32<8>,
vshrq_s32<9>,
vshrq_s32<10>,
vshrq_s32<11>,
vshrq_s32<12>,
vshrq_s32<13>,
vshrq_s32<14>,
vshrq_s32<15>,
vshrq_s32<16>,
vshrq_s32<17>,
vshrq_s32<18>,
vshrq_s32<19>,
vshrq_s32<20>,
vshrq_s32<21>,
vshrq_s32<22>,
vshrq_s32<23>,
vshrq_s32<24>,
vshrq_s32<25>,
vshrq_s32<26>,
vshrq_s32<27>,
vshrq_s32<28>,
vshrq_s32<29>,
vshrq_s32<30>,
vshrq_s32<31>,
vshrq_s32<32>
};
vshrq_s32_func vshrq_s32_p = vshrq_s32_a[scale];
for (ptrdiff_t y = 0; y < height; ++y)
{
const u8 * srow0 = y == 0 && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::max<ptrdiff_t>(y - 1, 0));
const u8 * srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8 * srow2 = y + 1 == height && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::min(y + 1, height - 1));
u8 * drow = internal::getRowPtr(dstBase, dstStride, y);
u8 prevx[3] = { 0, 0, 0 },
currx[3] = { 0, 0, 0 },
nextx[3] = { 0, 0, 0 };
ptrdiff_t x = 0;
const ptrdiff_t bwidth = y + 2 < height ? width : (width - 8);
// perform vertical convolution
for ( ; x <= bwidth; x += 8)
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = !srow0 ? v_border : vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = !srow2 ? v_border : vld1_u8(srow2 + x);
// calculate values for plain CPU part below if needed
if (x + 8 >= bwidth)
{
ptrdiff_t x3 = x == width ? width - 1 : x;
ptrdiff_t x4 = border == BORDER_MODE_CONSTANT ? x3 - 1 : std::max<ptrdiff_t>(x3 - 1, 0);
if (border == BORDER_MODE_CONSTANT && x4 < 0)
prevx[0] = prevx[1] = prevx[2] = borderValue;
else
{
prevx[0] = srow0 ? srow0[x4] : borderValue;
prevx[1] =         srow1[x4]              ;
prevx[2] = srow2 ? srow2[x4] : borderValue;
}
currx[0] = srow0 ? srow0[x3] : borderValue;
currx[1] =         srow1[x3]              ;
currx[2] = srow2 ? srow2[x3] : borderValue;
}
// make shift
if (x)
{
tprev[0] = tcurr[0];
tcurr[0] = tnext[0];
tprev[1] = tcurr[1];
tcurr[1] = tnext[1];
tprev[2] = tcurr[2];
tcurr[2] = tnext[2];
}
tnext[0] = x0;
tnext[1] = x1;
tnext[2] = x2;
// make extrapolation for the first elements
if (!x)
{
// make border
if (border == BORDER_MODE_CONSTANT)
tcurr[0] = tcurr[1] = tcurr[2] = v_border;
else if (border == BORDER_MODE_REPLICATE)
{
tcurr[0] = vdup_n_u8(vget_lane_u8(tnext[0], 0));
tcurr[1] = vdup_n_u8(vget_lane_u8(tnext[1], 0));
tcurr[2] = vdup_n_u8(vget_lane_u8(tnext[2], 0));
}
continue;
}
int32x4_t v_dst0 = v_zero_s32, v_dst1 = v_zero_s32;
{
// combine 3 "shifted" vectors
t0 = vext_u8(tprev[0], tcurr[0], 7);
t1 = tcurr[0];
t2 = vext_u8(tcurr[0], tnext[0], 1);
int16x8_t t0_16s = vreinterpretq_s16_u16(vmovl_u8(t0));
int16x8_t t1_16s = vreinterpretq_s16_u16(vmovl_u8(t1));
int16x8_t t2_16s = vreinterpretq_s16_u16(vmovl_u8(t2));
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t0_16s), kernelBase[8]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t1_16s), kernelBase[7]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t2_16s), kernelBase[6]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t0_16s), kernelBase[8]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t1_16s), kernelBase[7]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t2_16s), kernelBase[6]);
}
{
// combine 3 "shifted" vectors
t0 = vext_u8(tprev[1], tcurr[1], 7);
t1 = tcurr[1];
t2 = vext_u8(tcurr[1], tnext[1], 1);
int16x8_t t0_16s = vreinterpretq_s16_u16(vmovl_u8(t0));
int16x8_t t1_16s = vreinterpretq_s16_u16(vmovl_u8(t1));
int16x8_t t2_16s = vreinterpretq_s16_u16(vmovl_u8(t2));
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t0_16s), kernelBase[5]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t1_16s), kernelBase[4]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t2_16s), kernelBase[3]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t0_16s), kernelBase[5]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t1_16s), kernelBase[4]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t2_16s), kernelBase[3]);
}
{
// combine 3 "shifted" vectors
t0 = vext_u8(tprev[2], tcurr[2], 7);
t1 = tcurr[2];
t2 = vext_u8(tcurr[2], tnext[2], 1);
int16x8_t t0_16s = vreinterpretq_s16_u16(vmovl_u8(t0));
int16x8_t t1_16s = vreinterpretq_s16_u16(vmovl_u8(t1));
int16x8_t t2_16s = vreinterpretq_s16_u16(vmovl_u8(t2));
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t0_16s), kernelBase[2]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t1_16s), kernelBase[1]);
v_dst0 = vmlal_n_s16(v_dst0, vget_low_s16(t2_16s), kernelBase[0]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t0_16s), kernelBase[2]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t1_16s), kernelBase[1]);
v_dst1 = vmlal_n_s16(v_dst1, vget_high_s16(t2_16s), kernelBase[0]);
}

// make scale
v_dst0 = vshrq_s32_p(v_dst0);
v_dst1 = vshrq_s32_p(v_dst1);
// and add them
vst1_u8(drow + x - 8, vqmovn_u16(vcombine_u16(vqmovun_s32(v_dst0),
vqmovun_s32(v_dst1))));
}
x -= 8;
if (x == width)
--x;
for ( ; x < width; ++x)
{
// make extrapolation for the last elements
if (x + 1 >= width)
{
if (border == BORDER_MODE_CONSTANT)
{
nextx[0] = borderValue;
nextx[1] = borderValue;
nextx[2] = borderValue;
}
else if (border == BORDER_MODE_REPLICATE)
{
nextx[0] = srow0[x];
nextx[1] = srow1[x];
nextx[2] = srow2[x];
}
}
else
{
nextx[0] = srow0 ? srow0[x + 1] : borderValue;
nextx[1] =         srow1[x + 1]              ;
nextx[2] = srow2 ? srow2[x + 1] : borderValue;
}
s32 val = 0;
for (s32 _y = 0; _y < 3; ++_y)
val += prevx[_y] * kernelBase[(2 - _y) * 3 + 2] +
currx[_y] * kernelBase[(2 - _y) * 3 + 1] +
nextx[_y] * kernelBase[(2 - _y) * 3 + 0];
drow[x] = internal::saturate_cast<u8>(val >> scale);
// make shift
prevx[0] = currx[0];
currx[0] = nextx[0];
prevx[1] = currx[1];
currx[1] = nextx[1];
prevx[2] = currx[2];
currx[2] = nextx[2];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
(void)ksize;
(void)kernelBase;
(void)scale;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include <limits>
namespace CAROTENE_NS {
s32 countNonZero(const Size2D &_size,
const u8 * srcBase, ptrdiff_t srcStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
size_t roiw16 = size.width & ~15u;
s32 result = 0;
for(size_t k = 0; k < size.height; ++k)
{
const u8* src = internal::getRowPtr( srcBase,  srcStride, k);
size_t i = 0;
#define COUNTNONZERO8U_BLOCK_SIZE (16*255)
uint8x16_t vc1 = vmovq_n_u8(1);
for (; i < roiw16;)
{
size_t lim = std::min(i + COUNTNONZERO8U_BLOCK_SIZE, size.width) - 16;
uint8x16_t vs = vmovq_n_u8(0);
for (; i <= lim; i+= 16)
{
internal::prefetch(src + i);
uint8x16_t vln = vld1q_u8(src + i);
uint8x16_t vnz = vminq_u8(vln, vc1);
vs = vaddq_u8(vs, vnz);
}
uint32x4_t vs4 = vpaddlq_u16(vpaddlq_u8(vs));
uint32x2_t vs2 = vadd_u32(vget_low_u32(vs4), vget_high_u32(vs4));
s32 s[2];
vst1_u32((u32*)s, vs2);
if (s[0] < 0 || s[1] < 0)//saturate in case of overflow ~ 2GB of non-zeros...
{
return 0x7fFFffFF;
}
result += (s[0] += s[1]);
if (s[0] < 0 || result < 0)
{
return 0x7fFFffFF;
}
}
for (; i < size.width; i++)
result += (src[i] != 0)?1:0;
if (result < 0)//saturate in case of overflow ~ 2GB of non-zeros...
{
return 0x7fFFffFF;
}
}
return result;
#else
(void)_size;
(void)srcBase;
(void)srcStride;
return 0;
#endif
}
s32 countNonZero(const Size2D &_size,
const u16 * srcBase, ptrdiff_t srcStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
size_t roiw8 = size.width & ~7u;
s32 result = 0;
for(size_t k = 0; k < size.height; ++k)
{
const u16* src = internal::getRowPtr( srcBase,  srcStride, k);
size_t i = 0;
#define COUNTNONZERO16U_BLOCK_SIZE (8*(256*256-1))
uint16x8_t vc1 = vmovq_n_u16(1);
for (; i < roiw8;)
{
size_t lim = std::min(i + COUNTNONZERO16U_BLOCK_SIZE, size.width) - 8;
uint16x8_t vs = vmovq_n_u16(0);
for (; i <= lim; i+= 8)
{
internal::prefetch(src + i);
uint16x8_t vln = vld1q_u16(src + i);
uint16x8_t vnz = vminq_u16(vln, vc1);
vs = vaddq_u16(vs, vnz);
}
uint32x4_t vs4 = vpaddlq_u16(vs);
uint32x2_t vs2 = vadd_u32(vget_low_u32(vs4), vget_high_u32(vs4));
s32 s[2];
vst1_u32((u32*)s, vs2);
if (s[0] < 0 || s[1] < 0)//saturate in case of overflow ~ 4GB of non-zeros...
{
return 0x7fFFffFF;
}
result += (s[0] += s[1]);
if (s[0] < 0 || result < 0)
{
return 0x7fFFffFF;
}
}
for (; i < size.width; i++)
result += (src[i] != 0)?1:0;
if (result < 0)//saturate in case of overflow ~ 4GB of non-zeros...
{
return 0x7fFFffFF;
}
}
return result;
#else
(void)_size;
(void)srcBase;
(void)srcStride;
return 0;
#endif
}
s32 countNonZero(const Size2D &_size,
const s32 * srcBase, ptrdiff_t srcStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
size_t roiw4 = size.width & ~3u;
s32 result = 0;
for(size_t k = 0; k < size.height; ++k)
{
const u32* src = (const u32*)internal::getRowPtr( srcBase,  srcStride, k);
u32 i = 0;
uint32x4_t vc1 = vmovq_n_u32(1);
uint32x4_t vs = vmovq_n_u32(0);
for (; i < roiw4; i += 4 )
{
internal::prefetch(src + i);
uint32x4_t vln = vld1q_u32(src + i);
uint32x4_t vnz = vminq_u32(vln, vc1);
vs = vqaddq_u32(vs, vnz);
}
uint32x2_t vs2 = vqadd_u32(vget_low_u32(vs), vget_high_u32(vs));
s32 s[2];
vst1_u32((u32*)s, vs2);
if (s[0] < 0 || s[1] < 0)//saturate in case of overflow ~ 8GB of non-zeros...
{
return 0x7fFFffFF;
}
result += (s[0] += s[1]);
if (s[0] < 0 || result < 0)
{
return 0x7fFFffFF;
}
for (; i < size.width; i++)
result += (src[i] != 0)?1:0;
if (result < 0)//saturate in case of overflow ~ 8GB of non-zeros...
{
return 0x7fFFffFF;
}
}
return result;
#else
(void)_size;
(void)srcBase;
(void)srcStride;
return 0;
#endif
}
s32 countNonZero(const Size2D &_size,
const f32 * srcBase, ptrdiff_t srcStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
size_t roiw4 = size.width & ~3u;
s32 result = 0;
for(size_t k = 0; k < size.height; ++k)
{
const f32* src = internal::getRowPtr( srcBase,  srcStride, k);
size_t i = 0;
float32x4_t vc0 = vmovq_n_f32(0);
int32x4_t vs = vmovq_n_s32(0);
for (; i < roiw4; i += 4 )
{
internal::prefetch(src + i);
float32x4_t vln = vld1q_f32(src + i);
int32x4_t vnz = vreinterpretq_s32_u32(vmvnq_u32(vceqq_f32(vln, vc0)));
vs = vqaddq_s32(vs, vnz);
}
int32x2_t vs2 = vqneg_s32(vqadd_s32(vget_low_s32(vs), vget_high_s32(vs)));
int s[2];
vst1_s32(s, vs2);
result += (s[0] += s[1]);
if (s[0] < 0 || result < 0)//case of overflow ~ 8GB of non-zeros...
{
return 0x7fFFffFF;
}
for (; i < size.width; i++)
result += (src[i] < std::numeric_limits<float>::min() && src[i] > -std::numeric_limits<float>::min())?0:1;
if (result < 0)
{
return 0x7fFFffFF;
}
}
return result;
#else
(void)_size;
(void)srcBase;
(void)srcStride;
return 0;
#endif
}
s32 countNonZero(const Size2D &_size,
const f64 * srcBase, ptrdiff_t srcStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
size_t roiw8 = size.width & ~7u;
size_t roiw4 = size.width & ~3u;
size_t roiw2 = size.width & ~1u;
uint64x2_t vmask1 = vdupq_n_u64(0x7fFFffFFffFFffFFULL); //will treat denormals as non-zero
uint32x4_t vc0 = vmovq_n_u32(0);
s32 result = 0;
for(size_t k = 0; k < size.height; ++k)
{
const f64* src = internal::getRowPtr( srcBase,  srcStride, k);
size_t i = 0;
int32x2_t vs1 = vmov_n_s32(0);
int32x2_t vs2 = vmov_n_s32(0);
int32x2_t vs3 = vmov_n_s32(0);
int32x2_t vs4 = vmov_n_s32(0);
for (; i < roiw8; i += 8 )
{
internal::prefetch(src + i + 6);
uint64x2_t vln1 = vld1q_u64((const u64*)(src + i));
uint64x2_t vln2 = vld1q_u64((const u64*)(src + i + 2));
uint64x2_t vln3 = vld1q_u64((const u64*)(src + i + 4));
uint64x2_t vln4 = vld1q_u64((const u64*)(src + i + 6));
uint64x2_t vm1 = vandq_u64(vln1, vmask1);
uint64x2_t vm2 = vandq_u64(vln2, vmask1);
uint64x2_t vm3 = vandq_u64(vln3, vmask1);
uint64x2_t vm4 = vandq_u64(vln4, vmask1);
uint32x4_t vequ1 = vceqq_u32(vreinterpretq_u32_u64(vm1), vc0);
uint32x4_t vequ2 = vceqq_u32(vreinterpretq_u32_u64(vm2), vc0);
uint32x4_t vequ3 = vceqq_u32(vreinterpretq_u32_u64(vm3), vc0);
uint32x4_t vequ4 = vceqq_u32(vreinterpretq_u32_u64(vm4), vc0);
uint32x4_t vlx1 = vmvnq_u32(vequ1);
uint32x4_t vlx2 = vmvnq_u32(vequ2);
uint32x4_t vlx3 = vmvnq_u32(vequ3);
uint32x4_t vlx4 = vmvnq_u32(vequ4);
int32x2_t vnz1 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx1), vget_high_u32(vlx1)));
int32x2_t vnz2 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx2), vget_high_u32(vlx2)));
int32x2_t vnz3 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx3), vget_high_u32(vlx3)));
int32x2_t vnz4 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx4), vget_high_u32(vlx4)));
vs1 = vqadd_s32(vs1, vnz1);
vs2 = vqadd_s32(vs2, vnz2);
vs3 = vqadd_s32(vs3, vnz3);
vs4 = vqadd_s32(vs4, vnz4);
}
if (i < roiw4)
{
internal::prefetch(src + i + 2);
uint64x2_t vln1 = vld1q_u64((const u64*)(src + i));
uint64x2_t vln2 = vld1q_u64((const u64*)(src + i + 2));
uint64x2_t vm1 = vandq_u64(vln1, vmask1);
uint64x2_t vm2 = vandq_u64(vln2, vmask1);
uint32x4_t vequ1 = vceqq_u32(vreinterpretq_u32_u64(vm1), vc0);
uint32x4_t vequ2 = vceqq_u32(vreinterpretq_u32_u64(vm2), vc0);
uint32x4_t vlx1 = vmvnq_u32(vequ1);
uint32x4_t vlx2 = vmvnq_u32(vequ2);
int32x2_t vnz1 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx1), vget_high_u32(vlx1)));
int32x2_t vnz2 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx2), vget_high_u32(vlx2)));
vs1 = vqadd_s32(vs1, vnz1);
vs2 = vqadd_s32(vs2, vnz2);
i += 4;
}
if (i < roiw2)
{
internal::prefetch(src + i);
uint64x2_t vln1 = vld1q_u64((const u64*)(src + i));
uint64x2_t vm1 = vandq_u64(vln1, vmask1);
uint32x4_t vequ1 = vceqq_u32(vreinterpretq_u32_u64(vm1), vc0);
uint32x4_t vlx1 = vmvnq_u32(vequ1);
int32x2_t vnz1 = vreinterpret_s32_u32(vpmax_u32(vget_low_u32(vlx1), vget_high_u32(vlx1)));
vs1 = vqadd_s32(vs1, vnz1);
i += 2;
}
vs1 = vqadd_s32(vs1, vs2);
vs3 = vqadd_s32(vs3, vs4);
vs1 = vqadd_s32(vs1, vs3);
int32x2_t vsneg = vqneg_s32(vs1);
s32 s[2];
vst1_s32(s, vsneg);
result += (s[0] += s[1]);
if (s[0] < 0 || result < 0)//case of overflow ~ 16GB of non-zeros...
{
return 0x7fFFffFF;
}
for (; i < size.width; i++)
result += (src[i] < std::numeric_limits<double>::min() && src[i] > -std::numeric_limits<double>::min())?0:1;
if (result < 0)
{
return 0x7fFFffFF;
}
}
return result;
#else
(void)_size;
(void)srcBase;
(void)srcStride;
return 0;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2016, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
#include "vround_helper.hpp"
#include <cstring>
#include <cfloat>
#include <cmath>
#include <limits>
namespace CAROTENE_NS {
namespace {
#ifdef CAROTENE_NEON
template <typename T>
inline T divSaturateQ(const T &v1, const T &v2, const float scale)
{
return internal::vcombine(internal::vqmovn(divSaturateQ(internal::vmovl(internal::vget_low(v1)),
internal::vmovl(internal::vget_low(v2)), scale)),
internal::vqmovn(divSaturateQ(internal::vmovl(internal::vget_high(v1)),
internal::vmovl(internal::vget_high(v2)), scale))
);
}
template <>
inline int32x4_t divSaturateQ<int32x4_t>(const int32x4_t &v1, const int32x4_t &v2, const float scale)
{ return internal::vroundq_s32_f32(vmulq_f32(vmulq_n_f32(vcvtq_f32_s32(v1), scale), internal::vrecpq_f32(vcvtq_f32_s32(v2)))); }
template <>
inline uint32x4_t divSaturateQ<uint32x4_t>(const uint32x4_t &v1, const uint32x4_t &v2, const float scale)
{ return internal::vroundq_u32_f32(vmulq_f32(vmulq_n_f32(vcvtq_f32_u32(v1), scale), internal::vrecpq_f32(vcvtq_f32_u32(v2)))); }
template <typename T>
inline T divSaturate(const T &v1, const T &v2, const float scale)
{
return internal::vqmovn(divSaturateQ(internal::vmovl(v1), internal::vmovl(v2), scale));
}
template <>
inline int32x2_t divSaturate<int32x2_t>(const int32x2_t &v1, const int32x2_t &v2, const float scale)
{ return internal::vround_s32_f32(vmul_f32(vmul_n_f32(vcvt_f32_s32(v1), scale), internal::vrecp_f32(vcvt_f32_s32(v2)))); }
template <>
inline uint32x2_t divSaturate<uint32x2_t>(const uint32x2_t &v1, const uint32x2_t &v2, const float scale)
{ return internal::vround_u32_f32(vmul_f32(vmul_n_f32(vcvt_f32_u32(v1), scale), internal::vrecp_f32(vcvt_f32_u32(v2)))); }

template <typename T>
inline T divWrapQ(const T &v1, const T &v2, const float scale)
{
return internal::vcombine(internal::vmovn(divWrapQ(internal::vmovl(internal::vget_low(v1)),
internal::vmovl(internal::vget_low(v2)), scale)),
internal::vmovn(divWrapQ(internal::vmovl(internal::vget_high(v1)),
internal::vmovl(internal::vget_high(v2)), scale))
);
}
template <>
inline int32x4_t divWrapQ<int32x4_t>(const int32x4_t &v1, const int32x4_t &v2, const float scale)
{ return vcvtq_s32_f32(vmulq_f32(vmulq_n_f32(vcvtq_f32_s32(v1), scale), internal::vrecpq_f32(vcvtq_f32_s32(v2)))); }
template <>
inline uint32x4_t divWrapQ<uint32x4_t>(const uint32x4_t &v1, const uint32x4_t &v2, const float scale)
{ return vcvtq_u32_f32(vmulq_f32(vmulq_n_f32(vcvtq_f32_u32(v1), scale), internal::vrecpq_f32(vcvtq_f32_u32(v2)))); }
template <typename T>
inline T divWrap(const T &v1, const T &v2, const float scale)
{
return internal::vmovn(divWrapQ(internal::vmovl(v1), internal::vmovl(v2), scale));
}
template <>
inline int32x2_t divWrap<int32x2_t>(const int32x2_t &v1, const int32x2_t &v2, const float scale)
{ return vcvt_s32_f32(vmul_f32(vmul_n_f32(vcvt_f32_s32(v1), scale), internal::vrecp_f32(vcvt_f32_s32(v2)))); }
template <>
inline uint32x2_t divWrap<uint32x2_t>(const uint32x2_t &v1, const uint32x2_t &v2, const float scale)
{ return vcvt_u32_f32(vmul_f32(vmul_n_f32(vcvt_f32_u32(v1), scale), internal::vrecp_f32(vcvt_f32_u32(v2)))); }
inline  uint8x16_t vtstq(const uint8x16_t  & v0, const uint8x16_t  & v1) { return vtstq_u8 (v0, v1); }
inline  uint16x8_t vtstq(const uint16x8_t  & v0, const uint16x8_t  & v1) { return vtstq_u16(v0, v1); }
inline  uint32x4_t vtstq(const uint32x4_t  & v0, const uint32x4_t  & v1) { return vtstq_u32(v0, v1); }
inline   int8x16_t vtstq(const int8x16_t   & v0, const int8x16_t   & v1) { return vreinterpretq_s8_u8  (vtstq_s8 (v0, v1)); }
inline   int16x8_t vtstq(const int16x8_t   & v0, const int16x8_t   & v1) { return vreinterpretq_s16_u16(vtstq_s16(v0, v1)); }
inline   int32x4_t vtstq(const int32x4_t   & v0, const int32x4_t   & v1) { return vreinterpretq_s32_u32(vtstq_s32(v0, v1)); }
inline   uint8x8_t vtst(const uint8x8_t   & v0, const uint8x8_t   & v1) { return vtst_u8 (v0, v1); }
inline  uint16x4_t vtst(const uint16x4_t  & v0, const uint16x4_t  & v1) { return vtst_u16(v0, v1); }
inline  uint32x2_t vtst(const uint32x2_t  & v0, const uint32x2_t  & v1) { return vtst_u32(v0, v1); }
inline    int8x8_t vtst(const int8x8_t    & v0, const int8x8_t    & v1) { return vreinterpret_s8_u8  (vtst_s8 (v0, v1)); }
inline   int16x4_t vtst(const int16x4_t   & v0, const int16x4_t   & v1) { return vreinterpret_s16_u16(vtst_s16(v0, v1)); }
inline   int32x2_t vtst(const int32x2_t   & v0, const int32x2_t   & v1) { return vreinterpret_s32_u32(vtst_s32(v0, v1)); }
#endif
template <typename T>
void div(const Size2D &size,
const T * src0Base, ptrdiff_t src0Stride,
const T * src1Base, ptrdiff_t src1Stride,
T * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
typedef typename internal::VecTraits<T>::vec128 vec128;
typedef typename internal::VecTraits<T>::vec64 vec64;
#if defined(__GNUC__) && (defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103L)
static_assert(std::numeric_limits<T>::is_integer, "template implementation is for integer types only");
#endif
if (scale == 0.0f ||
(std::numeric_limits<T>::is_integer &&
(scale * static_cast<float>(std::numeric_limits<T>::max())) <  1.0f &&
(scale * static_cast<float>(std::numeric_limits<T>::max())) > -1.0f))
{
for (size_t y = 0; y < size.height; ++y)
{
T * dst = internal::getRowPtr(dstBase, dstStride, y);
std::memset(dst, 0, sizeof(T) * size.width);
}
return;
}
const size_t step128 = 16 / sizeof(T);
size_t roiw128 = size.width >= (step128 - 1) ? size.width - step128 + 1 : 0;
const size_t step64 = 8 / sizeof(T);
size_t roiw64 = size.width >= (step64 - 1) ? size.width - step64 + 1 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const T * src0 = internal::getRowPtr(src0Base, src0Stride, i);
const T * src1 = internal::getRowPtr(src1Base, src1Stride, i);
T * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
if (cpolicy == CONVERT_POLICY_SATURATE)
{
for (; j < roiw128; j += step128)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
vec128 v_src0 = internal::vld1q(src0 + j);
vec128 v_src1 = internal::vld1q(src1 + j);
vec128 v_mask = vtstq(v_src1,v_src1);
internal::vst1q(dst + j, internal::vandq(v_mask, divSaturateQ(v_src0, v_src1, scale)));
}
for (; j < roiw64; j += step64)
{
vec64 v_src0 = internal::vld1(src0 + j);
vec64 v_src1 = internal::vld1(src1 + j);
vec64 v_mask = vtst(v_src1,v_src1);
internal::vst1(dst + j, internal::vand(v_mask,divSaturate(v_src0, v_src1, scale)));
}
for (; j < size.width; j++)
{
dst[j] = src1[j] ? internal::saturate_cast<T>(scale * src0[j] / src1[j]) : 0;
}
}
else // CONVERT_POLICY_WRAP
{
for (; j < roiw128; j += step128)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
vec128 v_src0 = internal::vld1q(src0 + j);
vec128 v_src1 = internal::vld1q(src1 + j);
vec128 v_mask = vtstq(v_src1,v_src1);
internal::vst1q(dst + j, internal::vandq(v_mask, divWrapQ(v_src0, v_src1, scale)));
}
for (; j < roiw64; j += step64)
{
vec64 v_src0 = internal::vld1(src0 + j);
vec64 v_src1 = internal::vld1(src1 + j);
vec64 v_mask = vtst(v_src1,v_src1);
internal::vst1(dst + j, internal::vand(v_mask,divWrap(v_src0, v_src1, scale)));
}
for (; j < size.width; j++)
{
dst[j] = src1[j] ? (T)((s32)trunc(scale * src0[j] / src1[j])) : 0;
}
}
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)cpolicy;
(void)scale;
#endif
}
#ifdef CAROTENE_NEON
template <typename T>
inline T recipSaturateQ(const T &v2, const float scale)
{
return internal::vcombine(internal::vqmovn(recipSaturateQ(internal::vmovl(internal::vget_low(v2)), scale)),
internal::vqmovn(recipSaturateQ(internal::vmovl(internal::vget_high(v2)), scale))
);
}
template <>
inline int32x4_t recipSaturateQ<int32x4_t>(const int32x4_t &v2, const float scale)
{ return vcvtq_s32_f32(vmulq_n_f32(internal::vrecpq_f32(vcvtq_f32_s32(v2)), scale)); }
template <>
inline uint32x4_t recipSaturateQ<uint32x4_t>(const uint32x4_t &v2, const float scale)
{ return vcvtq_u32_f32(vmulq_n_f32(internal::vrecpq_f32(vcvtq_f32_u32(v2)), scale)); }
template <typename T>
inline T recipSaturate(const T &v2, const float scale)
{
return internal::vqmovn(recipSaturateQ(internal::vmovl(v2), scale));
}
template <>
inline int32x2_t recipSaturate<int32x2_t>(const int32x2_t &v2, const float scale)
{ return vcvt_s32_f32(vmul_n_f32(internal::vrecp_f32(vcvt_f32_s32(v2)), scale)); }
template <>
inline uint32x2_t recipSaturate<uint32x2_t>(const uint32x2_t &v2, const float scale)
{ return vcvt_u32_f32(vmul_n_f32(internal::vrecp_f32(vcvt_f32_u32(v2)), scale)); }

template <typename T>
inline T recipWrapQ(const T &v2, const float scale)
{
return internal::vcombine(internal::vmovn(recipWrapQ(internal::vmovl(internal::vget_low(v2)), scale)),
internal::vmovn(recipWrapQ(internal::vmovl(internal::vget_high(v2)), scale))
);
}
template <>
inline int32x4_t recipWrapQ<int32x4_t>(const int32x4_t &v2, const float scale)
{ return vcvtq_s32_f32(vmulq_n_f32(internal::vrecpq_f32(vcvtq_f32_s32(v2)), scale)); }
template <>
inline uint32x4_t recipWrapQ<uint32x4_t>(const uint32x4_t &v2, const float scale)
{ return vcvtq_u32_f32(vmulq_n_f32(internal::vrecpq_f32(vcvtq_f32_u32(v2)), scale)); }
template <typename T>
inline T recipWrap(const T &v2, const float scale)
{
return internal::vmovn(recipWrapQ(internal::vmovl(v2), scale));
}
template <>
inline int32x2_t recipWrap<int32x2_t>(const int32x2_t &v2, const float scale)
{ return vcvt_s32_f32(vmul_n_f32(internal::vrecp_f32(vcvt_f32_s32(v2)), scale)); }
template <>
inline uint32x2_t recipWrap<uint32x2_t>(const uint32x2_t &v2, const float scale)
{ return vcvt_u32_f32(vmul_n_f32(internal::vrecp_f32(vcvt_f32_u32(v2)), scale)); }
#endif
template <typename T>
void recip(const Size2D &size,
const T * src1Base, ptrdiff_t src1Stride,
T * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
typedef typename internal::VecTraits<T>::vec128 vec128;
typedef typename internal::VecTraits<T>::vec64 vec64;
#if defined(__GNUC__) && (defined(__GXX_EXPERIMENTAL_CXX0X__) || __cplusplus >= 201103L)
static_assert(std::numeric_limits<T>::is_integer, "template implementation is for integer types only");
#endif
if (scale == 0.0f ||
(std::numeric_limits<T>::is_integer &&
scale <  1.0f &&
scale > -1.0f))
{
for (size_t y = 0; y < size.height; ++y)
{
T * dst = internal::getRowPtr(dstBase, dstStride, y);
std::memset(dst, 0, sizeof(T) * size.width);
}
return;
}
const size_t step128 = 16 / sizeof(T);
size_t roiw128 = size.width >= (step128 - 1) ? size.width - step128 + 1 : 0;
const size_t step64 = 8 / sizeof(T);
size_t roiw64 = size.width >= (step64 - 1) ? size.width - step64 + 1 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const T * src1 = internal::getRowPtr(src1Base, src1Stride, i);
T * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
if (cpolicy == CONVERT_POLICY_SATURATE)
{
for (; j < roiw128; j += step128)
{
internal::prefetch(src1 + j);
vec128 v_src1 = internal::vld1q(src1 + j);
vec128 v_mask = vtstq(v_src1,v_src1);
internal::vst1q(dst + j, internal::vandq(v_mask, recipSaturateQ(v_src1, scale)));
}
for (; j < roiw64; j += step64)
{
vec64 v_src1 = internal::vld1(src1 + j);
vec64 v_mask = vtst(v_src1,v_src1);
internal::vst1(dst + j, internal::vand(v_mask, recipSaturate(v_src1, scale)));
}
for (; j < size.width; j++)
{
dst[j] = src1[j] ? internal::saturate_cast<T>(scale / src1[j]) : 0;
}
}
else // CONVERT_POLICY_WRAP
{
for (; j < roiw128; j += step128)
{
internal::prefetch(src1 + j);
vec128 v_src1 = internal::vld1q(src1 + j);
vec128 v_mask = vtstq(v_src1,v_src1);
internal::vst1q(dst + j, internal::vandq(v_mask, recipWrapQ(v_src1, scale)));
}
for (; j < roiw64; j += step64)
{
vec64 v_src1 = internal::vld1(src1 + j);
vec64 v_mask = vtst(v_src1,v_src1);
internal::vst1(dst + j, internal::vand(v_mask, recipWrap(v_src1, scale)));
}
for (; j < size.width; j++)
{
dst[j] = src1[j] ? (T)((s32)trunc(scale / src1[j])) : 0;
}
}
}
#else
(void)size;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)cpolicy;
(void)scale;
#endif
}
}
void div(const Size2D &size,
const u8 * src0Base, ptrdiff_t src0Stride,
const u8 * src1Base, ptrdiff_t src1Stride,
u8 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
div<u8>(size, src0Base, src0Stride, src1Base, src1Stride, dstBase, dstStride, scale, cpolicy);
}
void div(const Size2D &size,
const s8 * src0Base, ptrdiff_t src0Stride,
const s8 * src1Base, ptrdiff_t src1Stride,
s8 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
div<s8>(size, src0Base, src0Stride, src1Base, src1Stride, dstBase, dstStride, scale, cpolicy);
}
void div(const Size2D &size,
const u16 * src0Base, ptrdiff_t src0Stride,
const u16 * src1Base, ptrdiff_t src1Stride,
u16 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
div<u16>(size, src0Base, src0Stride, src1Base, src1Stride, dstBase, dstStride, scale, cpolicy);
}
void div(const Size2D &size,
const s16 * src0Base, ptrdiff_t src0Stride,
const s16 * src1Base, ptrdiff_t src1Stride,
s16 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
div<s16>(size, src0Base, src0Stride, src1Base, src1Stride, dstBase, dstStride, scale, cpolicy);
}
void div(const Size2D &size,
const s32 * src0Base, ptrdiff_t src0Stride,
const s32 * src1Base, ptrdiff_t src1Stride,
s32 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
div<s32>(size, src0Base, src0Stride, src1Base, src1Stride, dstBase, dstStride, scale, cpolicy);
}
void div(const Size2D &size,
const f32 * src0Base, ptrdiff_t src0Stride,
const f32 * src1Base, ptrdiff_t src1Stride,
f32 * dstBase, ptrdiff_t dstStride,
f32 scale)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (scale == 0.0f)
{
for (size_t y = 0; y < size.height; ++y)
{
f32 * dst = internal::getRowPtr(dstBase, dstStride, y);
std::memset(dst, 0, sizeof(f32) * size.width);
}
return;
}
size_t roiw128 = size.width >= 3 ? size.width - 3 : 0;
size_t roiw64 = size.width >= 1 ? size.width - 1 : 0;
if (std::fabs(scale - 1.0f) < FLT_EPSILON)
{
for (size_t i = 0; i < size.height; ++i)
{
const f32 * src0 = internal::getRowPtr(src0Base, src0Stride, i);
const f32 * src1 = internal::getRowPtr(src1Base, src1Stride, i);
f32 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw128; j += 4)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
float32x4_t v_src0 = vld1q_f32(src0 + j);
float32x4_t v_src1 = vld1q_f32(src1 + j);
vst1q_f32(dst + j, vmulq_f32(v_src0, internal::vrecpq_f32(v_src1)));
}
for (; j < roiw64; j += 2)
{
float32x2_t v_src0 = vld1_f32(src0 + j);
float32x2_t v_src1 = vld1_f32(src1 + j);
vst1_f32(dst + j, vmul_f32(v_src0, internal::vrecp_f32(v_src1)));
}
for (; j < size.width; j++)
{
dst[j] = src0[j] / src1[j];
}
}
}
else
{
for (size_t i = 0; i < size.height; ++i)
{
const f32 * src0 = internal::getRowPtr(src0Base, src0Stride, i);
const f32 * src1 = internal::getRowPtr(src1Base, src1Stride, i);
f32 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw128; j += 4)
{
internal::prefetch(src0 + j);
internal::prefetch(src1 + j);
float32x4_t v_src0 = vld1q_f32(src0 + j);
float32x4_t v_src1 = vld1q_f32(src1 + j);
vst1q_f32(dst + j, vmulq_f32(vmulq_n_f32(v_src0, scale),
internal::vrecpq_f32(v_src1)));
}
for (; j < roiw64; j += 2)
{
float32x2_t v_src0 = vld1_f32(src0 + j);
float32x2_t v_src1 = vld1_f32(src1 + j);
vst1_f32(dst + j, vmul_f32(vmul_n_f32(v_src0, scale),
internal::vrecp_f32(v_src1)));
}
for (; j < size.width; j++)
{
dst[j] = src0[j] * scale / src1[j];
}
}
}
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
(void)scale;
#endif
}
void reciprocal(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
recip<u8>(size, srcBase, srcStride, dstBase, dstStride, scale, cpolicy);
}
void reciprocal(const Size2D &size,
const s8 * srcBase, ptrdiff_t srcStride,
s8 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
recip<s8>(size, srcBase, srcStride, dstBase, dstStride, scale, cpolicy);
}
void reciprocal(const Size2D &size,
const u16 * srcBase, ptrdiff_t srcStride,
u16 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
recip<u16>(size, srcBase, srcStride, dstBase, dstStride, scale, cpolicy);
}
void reciprocal(const Size2D &size,
const s16 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
recip<s16>(size, srcBase, srcStride, dstBase, dstStride, scale, cpolicy);
}
void reciprocal(const Size2D &size,
const s32 * srcBase, ptrdiff_t srcStride,
s32 * dstBase, ptrdiff_t dstStride,
f32 scale,
CONVERT_POLICY cpolicy)
{
recip<s32>(size, srcBase, srcStride, dstBase, dstStride, scale, cpolicy);
}
void reciprocal(const Size2D &size,
const f32 * srcBase, ptrdiff_t srcStride,
f32 * dstBase, ptrdiff_t dstStride,
f32 scale)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
if (scale == 0.0f)
{
for (size_t y = 0; y < size.height; ++y)
{
f32 * dst = internal::getRowPtr(dstBase, dstStride, y);
std::memset(dst, 0, sizeof(f32) * size.width);
}
return;
}
size_t roiw128 = size.width >= 3 ? size.width - 3 : 0;
size_t roiw64 = size.width >= 1 ? size.width - 1 : 0;
if (std::fabs(scale - 1.0f) < FLT_EPSILON)
{
for (size_t i = 0; i < size.height; ++i)
{
const f32 * src1 = internal::getRowPtr(srcBase, srcStride, i);
f32 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw128; j += 4)
{
internal::prefetch(src1 + j);
float32x4_t v_src1 = vld1q_f32(src1 + j);
vst1q_f32(dst + j, internal::vrecpq_f32(v_src1));
}
for (; j < roiw64; j += 2)
{
float32x2_t v_src1 = vld1_f32(src1 + j);
vst1_f32(dst + j, internal::vrecp_f32(v_src1));
}
for (; j < size.width; j++)
{
dst[j] = 1.0f / src1[j];
}
}
}
else
{
for (size_t i = 0; i < size.height; ++i)
{
const f32 * src1 = internal::getRowPtr(srcBase, srcStride, i);
f32 * dst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
for (; j < roiw128; j += 4)
{
internal::prefetch(src1 + j);
float32x4_t v_src1 = vld1q_f32(src1 + j);
vst1q_f32(dst + j, vmulq_n_f32(internal::vrecpq_f32(v_src1), scale));
}
for (; j < roiw64; j += 2)
{
float32x2_t v_src1 = vld1_f32(src1 + j);
vst1_f32(dst + j, vmul_n_f32(internal::vrecp_f32(v_src1), scale));
}
for (; j < size.width; j++)
{
dst[j] = scale / src1[j];
}
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)scale;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
namespace CAROTENE_NS {
f64 dotProduct(const Size2D &_size,
const u8 * src0Base, ptrdiff_t src0Stride,
const u8 * src1Base, ptrdiff_t src1Stride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (src0Stride == src1Stride &&
src0Stride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
// It is possible to accumulate up to 66051 uchar multiplication results in uint32 without overflow
// We process 16 elements and accumulate two new elements per step. So we could handle 66051/2*16 elements
#define DOT_UINT_BLOCKSIZE 66050*8
f64 result = 0.0;
for (size_t row = 0; row < size.height; ++row)
{
const u8 * src0 = internal::getRowPtr(src0Base, src0Stride, row);
const u8 * src1 = internal::getRowPtr(src1Base, src1Stride, row);
size_t i = 0;
uint64x2_t ws = vmovq_n_u64(0);
while(i + 16 <= size.width)
{
size_t lim = std::min(i + DOT_UINT_BLOCKSIZE, size.width) - 16;
uint32x4_t s1 = vmovq_n_u32(0);
uint32x4_t s2 = vmovq_n_u32(0);
for (; i <= lim; i += 16)
{
internal::prefetch(src0 + i);
internal::prefetch(src1 + i);
uint8x16_t vs1 = vld1q_u8(src0 + i);
uint8x16_t vs2 = vld1q_u8(src1 + i);
uint16x8_t vdot1 = vmull_u8(vget_low_u8(vs1), vget_low_u8(vs2));
uint16x8_t vdot2 = vmull_u8(vget_high_u8(vs1), vget_high_u8(vs2));
s1 = vpadalq_u16(s1, vdot1);
s2 = vpadalq_u16(s2, vdot2);
}
ws = vpadalq_u32(ws, s1);
ws = vpadalq_u32(ws, s2);
}
if(i + 8 <= size.width)
{
uint8x8_t vs1 = vld1_u8(src0 + i);
uint8x8_t vs2 = vld1_u8(src1 + i);
ws = vpadalq_u32(ws, vpaddlq_u16(vmull_u8(vs1, vs2)));
i += 8;
}
result += (double)vget_lane_u64(vadd_u64(vget_low_u64(ws), vget_high_u64(ws)), 0);
for (; i < size.width; ++i)
result += s32(src0[i]) * s32(src1[i]);
}
return result;
#else
(void)_size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
return 0;
#endif
}
f64 dotProduct(const Size2D &_size,
const s8 * src0Base, ptrdiff_t src0Stride,
const s8 * src1Base, ptrdiff_t src1Stride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (src0Stride == src1Stride &&
src0Stride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
// It is possible to accumulate up to 131071 schar multiplication results in sint32 without overflow
// We process 16 elements and accumulate two new elements per step. So we could handle 131071/2*16 elements
#define DOT_INT_BLOCKSIZE 131070*8
f64 result = 0.0;
for (size_t row = 0; row < size.height; ++row)
{
const s8 * src0 = internal::getRowPtr(src0Base, src0Stride, row);
const s8 * src1 = internal::getRowPtr(src1Base, src1Stride, row);
size_t i = 0;
int64x2_t ws = vmovq_n_s64(0);
while(i + 16 <= size.width)
{
size_t lim = std::min(i + DOT_UINT_BLOCKSIZE, size.width) - 16;
int32x4_t s1 = vmovq_n_s32(0);
int32x4_t s2 = vmovq_n_s32(0);
for (; i <= lim; i += 16)
{
internal::prefetch(src0 + i);
internal::prefetch(src1 + i);
int8x16_t vs1 = vld1q_s8(src0 + i);
int8x16_t vs2 = vld1q_s8(src1 + i);
int16x8_t vdot1 = vmull_s8(vget_low_s8(vs1), vget_low_s8(vs2));
int16x8_t vdot2 = vmull_s8(vget_high_s8(vs1), vget_high_s8(vs2));
s1 = vpadalq_s16(s1, vdot1);
s2 = vpadalq_s16(s2, vdot2);
}
ws = vpadalq_s32(ws, s1);
ws = vpadalq_s32(ws, s2);
}
if(i + 8 <= size.width)
{
int8x8_t vs1 = vld1_s8(src0 + i);
int8x8_t vs2 = vld1_s8(src1 + i);
ws = vpadalq_s32(ws, vpaddlq_s16(vmull_s8(vs1, vs2)));
i += 8;
}
result += (double)vget_lane_s64(vadd_s64(vget_low_s64(ws), vget_high_s64(ws)), 0);
for (; i < size.width; ++i)
result += s32(src0[i]) * s32(src1[i]);
}
return result;
#else
(void)_size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
return 0;
#endif
}
f64 dotProduct(const Size2D &_size,
const f32 * src0Base, ptrdiff_t src0Stride,
const f32 * src1Base, ptrdiff_t src1Stride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
Size2D size(_size);
if (src0Stride == src1Stride &&
src0Stride == (ptrdiff_t)(size.width * sizeof(f32)))
{
size.width *= size.height;
size.height = 1;
}
#define DOT_FLOAT_BLOCKSIZE (1 << 13)
f64 result = 0.0;
for (size_t row = 0; row < size.height; ++row)
{
const f32 * src0 = internal::getRowPtr(src0Base, src0Stride, row);
const f32 * src1 = internal::getRowPtr(src1Base, src1Stride, row);
size_t i = 0;
while(i + 4 <= size.width)
{
size_t lim = std::min(i + DOT_FLOAT_BLOCKSIZE, size.width) - 4;
float32x4_t v_sum = vdupq_n_f32(0.0f);
for( ; i <= lim; i += 4 )
{
internal::prefetch(src0 + i);
internal::prefetch(src1 + i);
v_sum = vmlaq_f32(v_sum, vld1q_f32(src0 + i), vld1q_f32(src1 + i));
}
float32x2_t vres = vpadd_f32(vget_low_f32(v_sum),vget_high_f32(v_sum));
result += vget_lane_f32(vres, 0) + vget_lane_f32(vres, 1);
}
if(i + 2 <= size.width)
{
float32x2_t vres = vmul_f32(vld1_f32(src0 + i), vld1_f32(src1 + i));
result += vget_lane_f32(vres, 0) + vget_lane_f32(vres, 1);
i += 2;
}
for (; i < size.width; ++i)
result += src0[i] * src1[i];
}
return result;
#else
(void)_size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
return 0;
#endif
}
} // namespace CAROTENE_NS

// This file is needed for compilation on some platforms e.g. with XCode generator
// Related issue: https://gitlab.kitware.com/cmake/cmake/-/issues/17457

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/

/* This is FAST corner detector, contributed to OpenCV by the author, Edward Rosten.
Below is the original copyright and the references */
/*
Copyright (c) 2006, 2008 Edward Rosten
All rights reserved.
Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions
are met:
*Redistributions of source code must retain the above copyright
notice, this list of conditions and the following disclaimer.
*Redistributions in binary form must reproduce the above copyright
notice, this list of conditions and the following disclaimer in the
documentation and/or other materials provided with the distribution.
*Neither the name of the University of Cambridge nor the names of
its contributors may be used to endorse or promote products derived
from this software without specific prior written permission.
THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL THE COPYRIGHT OWNER OR
CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR
PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
*/
/*
The references are:
* Machine learning for high-speed corner detection,
E. Rosten and T. Drummond, ECCV 2006
* Faster and better: A machine learning approach to corner detection
E. Rosten, R. Porter and T. Drummond, PAMI, 2009
*/
#include "common.hpp"
#include <vector>
#include <cstring>
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace
{
void makeOffsets(ptrdiff_t pixel[], ptrdiff_t row_stride)
{
pixel[0] = 0 + row_stride * 3;
pixel[1] = 1 + row_stride * 3;
pixel[2] = 2 + row_stride * 2;
pixel[3] = 3 + row_stride * 1;
pixel[4] = 3 + row_stride * 0;
pixel[5] = 3 + row_stride * -1;
pixel[6] = 2 + row_stride * -2;
pixel[7] = 1 + row_stride * -3;
pixel[8] = 0 + row_stride * -3;
pixel[9] = -1 + row_stride * -3;
pixel[10] = -2 + row_stride * -2;
pixel[11] = -3 + row_stride * -1;
pixel[12] = -3 + row_stride * 0;
pixel[13] = -3 + row_stride * 1;
pixel[14] = -2 + row_stride * 2;
pixel[15] = -1 + row_stride * 3;
}
u8 cornerScore(const u8* ptr, const ptrdiff_t pixel[])
{
const s32 K = 8, N = 16 + K + 1;
s32 k, v = ptr[0];
s16 d[(N + 7) & ~7];
for( k = 0; k < N; k++ )
d[k] = (s16)(v - ptr[pixel[k]]);
int16x8_t q0 = vdupq_n_s16((s16)(-1000));
int16x8_t q1 = vdupq_n_s16((s16)(1000));
int16x8_t d0_7   = vld1q_s16(d +  0);
int16x8_t d8_15  = vld1q_s16(d +  8);
int16x8_t d16_23 = vld1q_s16(d + 16);
int16x8_t d24    = vld1q_s16(d + 24);
//k == 0
int16x8_t v0k0 = vextq_s16(d0_7, d8_15, 1);
int16x8_t v1k0 = vextq_s16(d0_7, d8_15, 2);
int16x8_t ak0 = vminq_s16(v0k0, v1k0);
int16x8_t bk0 = vmaxq_s16(v0k0, v1k0);
v0k0 = vextq_s16(d0_7, d8_15, 3);
ak0 = vminq_s16(ak0, v0k0);
bk0 = vmaxq_s16(bk0, v0k0);
v1k0 = vextq_s16(d0_7, d8_15, 4);
ak0 = vminq_s16(ak0, v1k0);
bk0 = vmaxq_s16(bk0, v1k0);
v0k0 = vextq_s16(d0_7, d8_15, 5);
ak0 = vminq_s16(ak0, v0k0);
bk0 = vmaxq_s16(bk0, v0k0);
v1k0 = vextq_s16(d0_7, d8_15, 6);
ak0 = vminq_s16(ak0, v1k0);
bk0 = vmaxq_s16(bk0, v1k0);
v0k0 = vextq_s16(d0_7, d8_15, 7);
ak0 = vminq_s16(ak0, v0k0);
bk0 = vmaxq_s16(bk0, v0k0);
ak0 = vminq_s16(ak0, d8_15);
bk0 = vmaxq_s16(bk0, d8_15);
q0 = vmaxq_s16(q0, vminq_s16(ak0, d0_7));
q1 = vminq_s16(q1, vmaxq_s16(bk0, d0_7));
v1k0 = vextq_s16(d8_15, d16_23, 1);
q0 = vmaxq_s16(q0, vminq_s16(ak0, v1k0));
q1 = vminq_s16(q1, vmaxq_s16(bk0, v1k0));
//k == 8
int16x8_t v0k8 = v1k0;
int16x8_t v1k8 = vextq_s16(d8_15, d16_23, 2);
int16x8_t ak8 = vminq_s16(v0k8, v1k8);
int16x8_t bk8 = vmaxq_s16(v0k8, v1k8);
v0k8 = vextq_s16(d8_15, d16_23, 3);
ak8 = vminq_s16(ak8, v0k8);
bk8 = vmaxq_s16(bk8, v0k8);
v1k8 = vextq_s16(d8_15, d16_23, 4);
ak8 = vminq_s16(ak8, v1k8);
bk8 = vmaxq_s16(bk8, v1k8);
v0k8 = vextq_s16(d8_15, d16_23, 5);
ak8 = vminq_s16(ak8, v0k8);
bk8 = vmaxq_s16(bk8, v0k8);
v1k8 = vextq_s16(d8_15, d16_23, 6);
ak8 = vminq_s16(ak8, v1k8);
bk8 = vmaxq_s16(bk8, v1k8);
v0k8 = vextq_s16(d8_15, d16_23, 7);
ak8 = vminq_s16(ak8, v0k8);
bk8 = vmaxq_s16(bk8, v0k8);
ak8 = vminq_s16(ak8, d16_23);
bk8 = vmaxq_s16(bk8, d16_23);
q0 = vmaxq_s16(q0, vminq_s16(ak8, d8_15));
q1 = vminq_s16(q1, vmaxq_s16(bk8, d8_15));
v1k8 = vextq_s16(d16_23, d24, 1);
q0 = vmaxq_s16(q0, vminq_s16(ak8, v1k8));
q1 = vminq_s16(q1, vmaxq_s16(bk8, v1k8));
//fin
int16x8_t q = vmaxq_s16(q0, vsubq_s16(vmovq_n_s16(0), q1));
int16x4_t q2 = vmax_s16(vget_low_s16(q), vget_high_s16(q));
int32x4_t q2w = vmovl_s16(q2);
int32x2_t q4 = vmax_s32(vget_low_s32(q2w), vget_high_s32(q2w));
int32x2_t q8 = vmax_s32(q4, vreinterpret_s32_s64(vshr_n_s64(vreinterpret_s64_s32(q4), 32)));
return (u8)(vget_lane_s32(q8, 0) - 1);
}
} //namespace
#endif
void FAST(const Size2D &size,
u8 *srcBase, ptrdiff_t srcStride,
KeypointStore *keypoints,
u8 threshold, bool nonmax_suppression)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
//keypoints.clear();
const s32 K = 8, N = 16 + K + 1;
ptrdiff_t i, j, k, pixel[N];
makeOffsets(pixel, srcStride);
for(k = 16; k < N; k++)
pixel[k] = pixel[k - 16];
uint8x16_t delta = vdupq_n_u8(128);
uint8x16_t t = vdupq_n_u8(threshold);
uint8x16_t K16 = vdupq_n_u8((u8)K);
u8 threshold_tab[512];
for( i = -255; i <= 255; i++ )
threshold_tab[i+255] = (u8)(i < -threshold ? 1 : i > threshold ? 2 : 0);
std::vector<u8> _buf((size.width+16)*3*(sizeof(ptrdiff_t) + sizeof(u8)) + 128);
u8* buf[3];
buf[0] = &_buf[0]; buf[1] = buf[0] + size.width; buf[2] = buf[1] + size.width;
ptrdiff_t* cpbuf[3];
cpbuf[0] = (ptrdiff_t*)internal::alignPtr(buf[2] + size.width, sizeof(ptrdiff_t)) + 1;
cpbuf[1] = cpbuf[0] + size.width + 1;
cpbuf[2] = cpbuf[1] + size.width + 1;
memset(buf[0], 0, size.width*3);
for(i = 3; i < (ptrdiff_t)size.height-2; i++)
{
const u8* ptr = internal::getRowPtr(srcBase, srcStride, i) + 3;
u8* curr = buf[(i - 3)%3];
ptrdiff_t* cornerpos = cpbuf[(i - 3)%3];
memset(curr, 0, size.width);
ptrdiff_t ncorners = 0;
if( i < (ptrdiff_t)size.height - 3 )
{
j = 3;
for(; j < (ptrdiff_t)size.width - 16 - 3; j += 16, ptr += 16)
{
internal::prefetch(ptr);
internal::prefetch(ptr + pixel[0]);
internal::prefetch(ptr + pixel[2]);
uint8x16_t v0 = vld1q_u8(ptr);
int8x16_t v1 = vreinterpretq_s8_u8(veorq_u8(vqsubq_u8(v0, t), delta));
int8x16_t v2 = vreinterpretq_s8_u8(veorq_u8(vqaddq_u8(v0, t), delta));
int8x16_t x0 = vreinterpretq_s8_u8(vsubq_u8(vld1q_u8(ptr + pixel[0]), delta));
int8x16_t x1 = vreinterpretq_s8_u8(vsubq_u8(vld1q_u8(ptr + pixel[4]), delta));
int8x16_t x2 = vreinterpretq_s8_u8(vsubq_u8(vld1q_u8(ptr + pixel[8]), delta));
int8x16_t x3 = vreinterpretq_s8_u8(vsubq_u8(vld1q_u8(ptr + pixel[12]), delta));
uint8x16_t m0 =   vandq_u8(vcgtq_s8(x0, v2), vcgtq_s8(x1, v2));
uint8x16_t m1 =   vandq_u8(vcgtq_s8(v1, x0), vcgtq_s8(v1, x1));
m0 = vorrq_u8(m0, vandq_u8(vcgtq_s8(x1, v2), vcgtq_s8(x2, v2)));
m1 = vorrq_u8(m1, vandq_u8(vcgtq_s8(v1, x1), vcgtq_s8(v1, x2)));
m0 = vorrq_u8(m0, vandq_u8(vcgtq_s8(x2, v2), vcgtq_s8(x3, v2)));
m1 = vorrq_u8(m1, vandq_u8(vcgtq_s8(v1, x2), vcgtq_s8(v1, x3)));
m0 = vorrq_u8(m0, vandq_u8(vcgtq_s8(x3, v2), vcgtq_s8(x0, v2)));
m1 = vorrq_u8(m1, vandq_u8(vcgtq_s8(v1, x3), vcgtq_s8(v1, x0)));
m0 = vorrq_u8(m0, m1);
u64 mask[2];
vst1q_u64(mask, vreinterpretq_u64_u8(m0));
if( mask[0] == 0 )
{
if (mask[1] != 0)
{
j -= 8;
ptr -= 8;
}
continue;
}
uint8x16_t c0 = vmovq_n_u8(0);
uint8x16_t c1 = vmovq_n_u8(0);
uint8x16_t max0 = vmovq_n_u8(0);
uint8x16_t max1 = vmovq_n_u8(0);
for( k = 0; k < N; k++ )
{
int8x16_t x = vreinterpretq_s8_u8(veorq_u8(vld1q_u8(ptr + pixel[k]), delta));
m0 = vcgtq_s8(x, v2);
m1 = vcgtq_s8(v1, x);
c0 = vandq_u8(vsubq_u8(c0, m0), m0);
c1 = vandq_u8(vsubq_u8(c1, m1), m1);
max0 = vmaxq_u8(max0, c0);
max1 = vmaxq_u8(max1, c1);
}
max0 = vmaxq_u8(max0, max1);
u8 m[16];
vst1q_u8(m, vcgtq_u8(max0, K16));
for( k = 0; k < 16; ++k )
if(m[k])
{
cornerpos[ncorners++] = j+k;
if(nonmax_suppression)
curr[j+k] = cornerScore(ptr+k, pixel);
}
}
for( ; j < (s32)size.width - 3; j++, ptr++ )
{
s32 v = ptr[0];
const u8* tab = &threshold_tab[0] - v + 255;
s32 d = tab[ptr[pixel[0]]] | tab[ptr[pixel[8]]];
if( d == 0 )
continue;
d &= tab[ptr[pixel[2]]] | tab[ptr[pixel[10]]];
d &= tab[ptr[pixel[4]]] | tab[ptr[pixel[12]]];
d &= tab[ptr[pixel[6]]] | tab[ptr[pixel[14]]];
if( d == 0 )
continue;
d &= tab[ptr[pixel[1]]] | tab[ptr[pixel[9]]];
d &= tab[ptr[pixel[3]]] | tab[ptr[pixel[11]]];
d &= tab[ptr[pixel[5]]] | tab[ptr[pixel[13]]];
d &= tab[ptr[pixel[7]]] | tab[ptr[pixel[15]]];
if( d & 1 )
{
s32 vt = v - threshold, count = 0;
for( k = 0; k < N; k++ )
{
s32 x = ptr[pixel[k]];
if(x < vt)
{
if( ++count > K )
{
cornerpos[ncorners++] = j;
if(nonmax_suppression)
curr[j] = cornerScore(ptr, pixel);
break;
}
}
else
count = 0;
}
}
if( d & 2 )
{
s32 vt = v + threshold, count = 0;
for( k = 0; k < N; k++ )
{
s32 x = ptr[pixel[k]];
if(x > vt)
{
if( ++count > K )
{
cornerpos[ncorners++] = j;
if(nonmax_suppression)
curr[j] = cornerScore(ptr, pixel);
break;
}
}
else
count = 0;
}
}
}
}
cornerpos[-1] = ncorners;
if( i == 3 )
continue;
const u8* prev = buf[(i - 4 + 3)%3];
const u8* pprev = buf[(i - 5 + 3)%3];
cornerpos = cpbuf[(i - 4 + 3)%3];
ncorners = cornerpos[-1];
for( k = 0; k < ncorners; k++ )
{
j = cornerpos[k];
s32 score = prev[j];
if( !nonmax_suppression ||
(score > prev[j+1] && score > prev[j-1] &&
score > pprev[j-1] && score > pprev[j] && score > pprev[j+1] &&
score > curr[j-1] && score > curr[j] && score > curr[j+1]) )
{
keypoints->push((f32)j, (f32)(i-1), 7.f, -1, (f32)score);
}
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)keypoints;
(void)threshold;
(void)nonmax_suppression;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
template <typename T>
void process(const T * src, size_t j0, size_t j1, size_t i,
T minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
T maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
for (size_t j = j0; j < j1; ++j)
{
T val = src[j];
if (val == maxVal)
{
if (maxLocCount < maxLocCapacity)
{
maxLocPtr[maxLocCount] = j;
maxLocPtr[maxLocCount + 1] = i;
}
maxLocCount += 2;
}
if (val == minVal)
{
if (minLocCount < minLocCapacity)
{
minLocPtr[minLocCount] = j;
minLocPtr[minLocCount + 1] = i;
}
minLocCount += 2;
}
}
}
} // namespace
#endif
void fillMinMaxLocs(const Size2D & size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
u8 maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
uint8x16_t v_maxval16 = vdupq_n_u8(maxVal), v_minval16 = vdupq_n_u8(minVal);
uint8x8_t v_maxval8 = vdup_n_u8(maxVal), v_minval8 = vdup_n_u8(minVal);
u64 mask[2] = { 0ul };
minLocCapacity <<= 1;
maxLocCapacity <<= 1;
for (size_t i = 0; i < size.height; ++i)
{
const u8 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0;
for ( ; j < roiw16; j += 16)
{
internal::prefetch(src + j);
uint8x16_t v_src = vld1q_u8(src + j);
uint8x16_t v_maxmask = vceqq_u8(v_src, v_maxval16);
uint8x16_t v_minmask = vceqq_u8(v_src, v_minval16);
uint8x16_t v_mask = vorrq_u8(v_maxmask, v_minmask);
vst1q_u8((u8 *)&mask[0], v_mask);
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
if (mask[1])
process(src, j + 8, j + 16, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
for ( ; j < roiw8; j += 8)
{
uint8x8_t v_src = vld1_u8(src + j);
uint8x8_t v_maxmask = vceq_u8(v_src, v_maxval8);
uint8x8_t v_minmask = vceq_u8(v_src, v_minval8);
uint8x8_t v_mask = vorr_u8(v_maxmask, v_minmask);
vst1_u8((u8 *)&mask[0], v_mask);
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
process(src, j, size.width, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
minLocCount >>= 1;
maxLocCount >>= 1;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)minVal;
(void)minLocPtr;
(void)minLocCount;
(void)minLocCapacity;
(void)maxVal;
(void)maxLocPtr;
(void)maxLocCount;
(void)maxLocCapacity;
#endif
}
void fillMinMaxLocs(const Size2D & size,
const u16 * srcBase, ptrdiff_t srcStride,
u16 minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
u16 maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
uint16x8_t v_maxval8 = vdupq_n_u16(maxVal),
v_minval8 = vdupq_n_u16(minVal);
u64 mask[2] = { 0ul };
minLocCapacity <<= 1;
maxLocCapacity <<= 1;
for (size_t i = 0; i < size.height; ++i)
{
const u16 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0;
for ( ; j < roiw16; j += 16)
{
internal::prefetch(src + j);
uint16x8_t v_src0 = vld1q_u16(src + j), v_src1 = vld1q_u16(src + j + 8);
uint16x8_t v_mask0 = vorrq_u16(vceqq_u16(v_src0, v_maxval8), vceqq_u16(v_src0, v_minval8));
uint16x8_t v_mask1 = vorrq_u16(vceqq_u16(v_src1, v_maxval8), vceqq_u16(v_src1, v_minval8));
vst1q_u8((u8 *)&mask[0], vcombine_u8(vmovn_u16(v_mask0), vmovn_u16(v_mask1)));
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
if (mask[1])
process(src, j + 8, j + 16, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
for ( ; j < roiw8; j += 8)
{
internal::prefetch(src + j);
uint16x8_t v_src = vld1q_u16(src + j);
uint16x8_t v_maxmask = vceqq_u16(v_src, v_maxval8);
uint16x8_t v_minmask = vceqq_u16(v_src, v_minval8);
uint16x8_t v_mask = vorrq_u16(v_maxmask, v_minmask);
vst1_u8((u8 *)&mask[0], vmovn_u16(v_mask));
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
process(src, j, size.width, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
minLocCount >>= 1;
maxLocCount >>= 1;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)minVal;
(void)minLocPtr;
(void)minLocCount;
(void)minLocCapacity;
(void)maxVal;
(void)maxLocPtr;
(void)maxLocCount;
(void)maxLocCapacity;
#endif
}
void fillMinMaxLocs(const Size2D & size,
const s16 * srcBase, ptrdiff_t srcStride,
s16 minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
s16 maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw16 = size.width >= 15 ? size.width - 15 : 0;
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
int16x8_t v_maxval8 = vdupq_n_s16(maxVal),
v_minval8 = vdupq_n_s16(minVal);
u64 mask[2] = { 0ul };
minLocCapacity <<= 1;
maxLocCapacity <<= 1;
for (size_t i = 0; i < size.height; ++i)
{
const s16 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0;
for ( ; j < roiw16; j += 16)
{
internal::prefetch(src + j);
int16x8_t v_src0 = vld1q_s16(src + j), v_src1 = vld1q_s16(src + j + 8);
uint16x8_t v_mask0 = vorrq_u16(vceqq_s16(v_src0, v_maxval8), vceqq_s16(v_src0, v_minval8));
uint16x8_t v_mask1 = vorrq_u16(vceqq_s16(v_src1, v_maxval8), vceqq_s16(v_src1, v_minval8));
vst1q_u8((u8 *)&mask[0], vcombine_u8(vmovn_u16(v_mask0), vmovn_u16(v_mask1)));
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
if (mask[1])
process(src, j + 8, j + 16, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
for ( ; j < roiw8; j += 8)
{
internal::prefetch(src + j);
int16x8_t v_src = vld1q_s16(src + j);
uint16x8_t v_maxmask = vceqq_s16(v_src, v_maxval8);
uint16x8_t v_minmask = vceqq_s16(v_src, v_minval8);
uint16x8_t v_mask = vorrq_u16(v_maxmask, v_minmask);
vst1_u8((u8 *)&mask[0], vmovn_u16(v_mask));
if (mask[0])
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
process(src, j, size.width, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
minLocCount >>= 1;
maxLocCount >>= 1;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)minVal;
(void)minLocPtr;
(void)minLocCount;
(void)minLocCapacity;
(void)maxVal;
(void)maxLocPtr;
(void)maxLocCount;
(void)maxLocCapacity;
#endif
}
void fillMinMaxLocs(const Size2D & size,
const s32 * srcBase, ptrdiff_t srcStride,
s32 minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
s32 maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
int32x4_t v_maxval4 = vdupq_n_s32(maxVal),
v_minval4 = vdupq_n_s32(minVal);
u64 mask = 0ul;
minLocCapacity <<= 1;
maxLocCapacity <<= 1;
for (size_t i = 0; i < size.height; ++i)
{
const s32 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0;
for ( ; j < roiw8; j += 8)
{
internal::prefetch(src + j);
int32x4_t v_src0 = vld1q_s32(src + j), v_src1 = vld1q_s32(src + j + 4);
uint32x4_t v_mask0 = vorrq_u32(vceqq_s32(v_src0, v_maxval4), vceqq_s32(v_src0, v_minval4));
uint32x4_t v_mask1 = vorrq_u32(vceqq_s32(v_src1, v_maxval4), vceqq_s32(v_src1, v_minval4));
vst1_u8((u8 *)&mask, vmovn_u16(vcombine_u16(vmovn_u32(v_mask0), vmovn_u32(v_mask1))));
if (mask)
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
process(src, j, size.width, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
minLocCount >>= 1;
maxLocCount >>= 1;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)minVal;
(void)minLocPtr;
(void)minLocCount;
(void)minLocCapacity;
(void)maxVal;
(void)maxLocPtr;
(void)maxLocCount;
(void)maxLocCapacity;
#endif
}
void fillMinMaxLocs(const Size2D & size,
const u32 * srcBase, ptrdiff_t srcStride,
u32 minVal, size_t * minLocPtr, s32 & minLocCount, s32 minLocCapacity,
u32 maxVal, size_t * maxLocPtr, s32 & maxLocCount, s32 maxLocCapacity)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t roiw8 = size.width >= 7 ? size.width - 7 : 0;
uint32x4_t v_maxval4 = vdupq_n_u32(maxVal),
v_minval4 = vdupq_n_u32(minVal);
u64 mask = 0ul;
minLocCapacity <<= 1;
maxLocCapacity <<= 1;
for (size_t i = 0; i < size.height; ++i)
{
const u32 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0;
for ( ; j < roiw8; j += 8)
{
internal::prefetch(src + j);
uint32x4_t v_src0 = vld1q_u32(src + j), v_src1 = vld1q_u32(src + j + 4);
uint32x4_t v_mask0 = vorrq_u32(vceqq_u32(v_src0, v_maxval4), vceqq_u32(v_src0, v_minval4));
uint32x4_t v_mask1 = vorrq_u32(vceqq_u32(v_src1, v_maxval4), vceqq_u32(v_src1, v_minval4));
vst1_u8((u8 *)&mask, vmovn_u16(vcombine_u16(vmovn_u32(v_mask0), vmovn_u32(v_mask1))));
if (mask)
process(src, j, j + 8, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
process(src, j, size.width, i,
minVal, minLocPtr, minLocCount, minLocCapacity,
maxVal, maxLocPtr, maxLocCount, maxLocCapacity);
}
minLocCount >>= 1;
maxLocCount >>= 1;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)minVal;
(void)minLocPtr;
(void)minLocCount;
(void)minLocCapacity;
(void)maxVal;
(void)maxLocPtr;
(void)maxLocCount;
(void)maxLocCapacity;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
#include <cstring>
namespace CAROTENE_NS {
bool isFlipSupported(FLIP_MODE flipMode, u32 elemSize)
{
bool supportedElemSize = (elemSize == 1) || (elemSize == 2) || (elemSize == 3) || (elemSize == 4);
return isSupportedConfiguration() &&
((supportedElemSize && ((flipMode == FLIP_BOTH_MODE) || (flipMode == FLIP_HORIZONTAL_MODE))) ||
(flipMode == FLIP_VERTICAL_MODE));
}
#ifdef CAROTENE_NEON
namespace {
template <typename T>
void flip(const Size2D & size,
const void * srcBase, ptrdiff_t srcStride,
void * dstBase, ptrdiff_t dstStride,
FLIP_MODE flipMode)
{
using namespace internal;
typedef typename VecTraits<T>::vec128 vec128;
typedef typename VecTraits<T>::vec64 vec64;
u32 step_base = 16 / sizeof(T), step_tail = 8 / sizeof(T);
size_t roiw_base = size.width >= (step_base - 1) ? size.width - step_base + 1 : 0;
size_t roiw_tail = size.width >= (step_tail - 1) ? size.width - step_tail + 1 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const T * src = getRowPtr((const T *)srcBase, srcStride, i);
T * dst = getRowPtr((T *)dstBase, dstStride, (flipMode & FLIP_VERTICAL_MODE) != 0 ? size.height - i - 1 : i);
size_t js = 0, jd = size.width;
for (; js < roiw_base; js += step_base, jd -= step_base)
{
prefetch(src + js);
vec128 v_src = vld1q(src + js);
vec128 v_dst = vrev64q(v_src);
v_dst = vcombine(vget_high(v_dst), vget_low(v_dst));
vst1q(dst + jd - step_base, v_dst);
}
for (; js < roiw_tail; js += step_tail, jd -= step_tail)
{
vec64 v_src = vld1(src + js);
vst1(dst + jd - step_tail, vrev64(v_src));
}
for (--jd; js < size.width; ++js, --jd)
dst[jd] = src[js];
}
}
template <typename T>
void flip3(const Size2D & size,
const void * srcBase, ptrdiff_t srcStride,
void * dstBase, ptrdiff_t dstStride,
FLIP_MODE flipMode)
{
using namespace internal;
#ifndef __ANDROID__
typedef typename VecTraits<T, 3>::vec128 vec128;
#endif
typedef typename VecTraits<T, 3>::vec64 vec64;
#ifndef __ANDROID__
u32 step_base = 16 / sizeof(T), step_base3 = step_base * 3;
size_t roiw_base = size.width >= (step_base - 1) ? size.width - step_base + 1 : 0;
#endif
u32 step_tail = 8 / sizeof(T), step_tail3 = step_tail * 3;
size_t roiw_tail = size.width >= (step_tail - 1) ? size.width - step_tail + 1 : 0;
for (size_t i = 0; i < size.height; ++i)
{
const T * src = getRowPtr((const T *)srcBase, srcStride, i);
T * dst = getRowPtr((T *)dstBase, dstStride, (flipMode & FLIP_VERTICAL_MODE) != 0 ? size.height - i - 1 : i);
size_t j = 0, js = 0, jd = size.width * 3;
#ifndef __ANDROID__
for (; j < roiw_base; j += step_base, js += step_base3, jd -= step_base3)
{
prefetch(src + js);
vec128 v_src = vld3q(src + js), v_dst;
v_src.val[0] = vrev64q(v_src.val[0]);
v_src.val[1] = vrev64q(v_src.val[1]);
v_src.val[2] = vrev64q(v_src.val[2]);
v_dst.val[0] = vcombine(vget_high(v_src.val[0]), vget_low(v_src.val[0]));
v_dst.val[1] = vcombine(vget_high(v_src.val[1]), vget_low(v_src.val[1]));
v_dst.val[2] = vcombine(vget_high(v_src.val[2]), vget_low(v_src.val[2]));
vst3q(dst + jd - step_base3, v_dst);
}
#endif // __ANDROID__
for (; j < roiw_tail; j += step_tail, js += step_tail3, jd -= step_tail3)
{
vec64 v_src = vld3(src + js), v_dst;
v_dst.val[0] = vrev64(v_src.val[0]);
v_dst.val[1] = vrev64(v_src.val[1]);
v_dst.val[2] = vrev64(v_src.val[2]);
vst3(dst + jd - step_tail3, v_dst);
}
for (jd -= 3; j < size.width; ++j, js += 3, jd -= 3)
{
dst[jd] = src[js];
dst[jd + 1] = src[js + 1];
dst[jd + 2] = src[js + 2];
}
}
}
typedef void (* flipFunc)(const Size2D &size,
const void * srcBase, ptrdiff_t srcStride,
void * dstBase, ptrdiff_t dstStride,
FLIP_MODE flipMode);
} // namespace
#endif
void flip(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
FLIP_MODE flipMode, u32 elemSize)
{
internal::assertSupportedConfiguration(isFlipSupported(flipMode, elemSize));
#ifdef CAROTENE_NEON
if (flipMode == FLIP_VERTICAL_MODE)
{
for (size_t y = 0; y < size.height; ++y)
{
const u8 * src_row = internal::getRowPtr(srcBase, srcStride, y);
u8 * dst_row = internal::getRowPtr(dstBase, dstStride, size.height - y - 1);
std::memcpy(dst_row, src_row, elemSize * size.width);
}
return;
}
flipFunc func = NULL;
if (elemSize == (u32)sizeof(u8))
func = &flip<u8>;
if (elemSize == (u32)sizeof(u16))
func = &flip<u16>;
if (elemSize == (u32)sizeof(u32))
func = &flip<u32>;
if (elemSize == (u32)sizeof(u8) * 3)
func = &flip3<u8>;
if (func == NULL)
return;
func(size,
srcBase, srcStride,
dstBase, dstStride,
flipMode);
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)flipMode;
(void)elemSize;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "saturate_cast.hpp"
#include "separable_filter.hpp"
namespace CAROTENE_NS {
bool isGaussianBlur3x3Supported(const Size2D &size, BORDER_MODE border)
{
return isSupportedConfiguration() && size.width >= 8 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REPLICATE);
}
void gaussianBlur3x3(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isGaussianBlur3x3Supported(size, border));
#ifdef CAROTENE_NEON
const uint16x8_t v_border_x4 = vdupq_n_u16(borderValue << 2);
const uint16x8_t v_zero = vdupq_n_u16(0);
const uint8x8_t v_border = vdup_n_u8(borderValue);
uint16x8_t tprev = v_zero, tcurr = v_zero, tnext = v_zero;
uint16x8_t t0 = v_zero, t1 = v_zero, t2 = v_zero;
ptrdiff_t width = (ptrdiff_t)size.width, height = (ptrdiff_t)size.height;
for (ptrdiff_t y = 0; y < height; ++y)
{
const u8 * srow0 = y == 0 && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::max<ptrdiff_t>(y - 1, 0));
const u8 * srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8 * srow2 = y + 1 == height && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::min(y + 1, height - 1));
u8 * drow = internal::getRowPtr(dstBase, dstStride, y);
s16 prevx = 0, currx = 0, nextx = 0;
ptrdiff_t x = 0;
const ptrdiff_t bwidth = y + 2 < height ? width : (width - 8);
// perform vertical convolution
for ( ; x <= bwidth; x += 8)
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = !srow0 ? v_border : vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = !srow2 ? v_border : vld1_u8(srow2 + x);
// calculate values for plain CPU part below if needed
if (x + 8 >= bwidth)
{
ptrdiff_t x3 = x == width ? width - 1 : x;
ptrdiff_t x4 = border == BORDER_MODE_CONSTANT ? x3 - 1 : std::max<ptrdiff_t>(x3 - 1, 0);
if (border == BORDER_MODE_CONSTANT && x4 < 0)
prevx = borderValue;
else
prevx = (srow2 ? srow2[x4] : borderValue) + (srow1[x4] << 1) + (srow0 ? srow0[x4] : borderValue);
currx = (srow2 ? srow2[x3] : borderValue) + (srow1[x3] << 1) + (srow0 ? srow0[x3] : borderValue);
}
// make shift
if (x)
{
tprev = tcurr;
tcurr = tnext;
}
// and calculate next value
tnext = vaddq_u16(vaddl_u8(x0, x2), vshll_n_u8(x1, 1));
// make extrapolation for the first elements
if (!x)
{
// make border
if (border == BORDER_MODE_CONSTANT)
tcurr = v_border_x4;
else if (border == BORDER_MODE_REPLICATE)
tcurr = vdupq_n_u16(vgetq_lane_u16(tnext, 0));
continue;
}
// combine 3 "shifted" vectors
t0 = vextq_u16(tprev, tcurr, 7);
t1 = tcurr;
t2 = vextq_u16(tcurr, tnext, 1);
// and add them
t0 = vqaddq_u16(vshlq_n_u16(t1, 1), vqaddq_u16(t0, t2));
vst1_u8(drow + x - 8, vshrn_n_u16(t0, 4));
}
x -= 8;
if (x == width)
--x;
for ( ; x < width; ++x)
{
// make extrapolation for the last elements
if (x + 1 >= width)
{
if (border == BORDER_MODE_CONSTANT)
nextx = borderValue << 2;
else if (border == BORDER_MODE_REPLICATE)
nextx = srow2[x] + (srow1[x] << 1) + srow0[x];
}
else
nextx = (srow2 ? srow2[x + 1] : borderValue) +
(srow1[x + 1] << 1) +
(srow0 ? srow0[x + 1] : borderValue);
f32 val = (prevx + (currx << 1) + nextx) >> 4;
drow[x] = internal::saturate_cast<u8>((s32)val);
// make shift
prevx = currx;
currx = nextx;
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
#endif
}
bool isGaussianBlur3x3MarginSupported(const Size2D &size, BORDER_MODE border, Margin borderMargin)
{
return isSeparableFilter3x3Supported(size, border, 0, 0, borderMargin);
}
void gaussianBlur3x3Margin(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isGaussianBlur3x3MarginSupported(size, border, borderMargin));
#ifdef CAROTENE_NEON
internal::sepFilter3x3<internal::RowFilter3x3S16_121, internal::ColFilter3x3U8_121>::process(
size, srcBase, srcStride, dstBase, dstStride,
0, 0, border, borderValue, borderMargin);
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
#endif
}
bool isGaussianBlur5x5Supported(const Size2D &size, s32 cn, BORDER_MODE border)
{
return isSupportedConfiguration() &&
cn > 0 && cn <= 4 &&
size.width >= 8 && size.height >= 2 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REFLECT101 ||
border == BORDER_MODE_REFLECT ||
border == BORDER_MODE_REPLICATE ||
border == BORDER_MODE_WRAP);
}
void gaussianBlur5x5(const Size2D &size, s32 cn,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, u8 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isGaussianBlur5x5Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 4*cn, borderValue);
tmp = &_tmp[cn << 1];
}
ptrdiff_t idx_l1 = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_l2 = internal::borderInterpolate(-2, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r1 = internal::borderInterpolate(size.width + 0, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r2 = internal::borderInterpolate(size.width + 1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//1-line buffer
std::vector<u16> _buf(cn * (size.width + 4) + 32 / sizeof(u16));
u16* lane = internal::alignPtr(&_buf[cn << 1], 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = borderValue;
lane[-cn-cn+k] = borderValue;
lane[colsn+k] = borderValue;
lane[colsn+cn+k] = borderValue;
}
uint8x8_t vc6u8 = vmov_n_u8(6);
uint16x8_t vc6u16 = vmovq_n_u16(6);
uint16x8_t vc4u16 = vmovq_n_u16(4);
for (size_t i = 0; i < size.height; ++i)
{
u8* dst = internal::getRowPtr(dstBase, dstStride, i);
//vertical convolution
ptrdiff_t idx_rm2 = internal::borderInterpolate(i - 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const u8* ln0 = idx_rm2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm2) : tmp;
const u8* ln1 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const u8* ln2 = internal::getRowPtr(srcBase, srcStride, i);
const u8* ln3 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
const u8* ln4 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 8; x += 8)
{
internal::prefetch(internal::getRowPtr(ln2 + x, srcStride, x % 5 - 2));
uint8x8_t v0 = vld1_u8(ln0+x);
uint8x8_t v1 = vld1_u8(ln1+x);
uint8x8_t v2 = vld1_u8(ln2+x);
uint8x8_t v3 = vld1_u8(ln3+x);
uint8x8_t v4 = vld1_u8(ln4+x);
uint16x8_t v = vaddl_u8(v0, v4);
uint16x8_t v13 = vaddl_u8(v1, v3);
v = vmlal_u8(v, v2, vc6u8);
v = vmlaq_u16(v, v13, vc4u16);
vst1q_u16(lane + x, v);
}
for (; x < colsn; ++x)
lane[x] = ln0[x] + ln4[x] + u16(4) * (ln1[x] + ln3[x]) + u16(6) * ln2[x];
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = lane[idx_l1 + k];
lane[-cn-cn+k] = lane[idx_l2 + k];
lane[colsn+k] = lane[idx_r1 + k];
lane[colsn+cn+k] = lane[idx_r2 + k];
}
//horizontal convolution
x = 0;
switch(cn)
{
case 1:
for (; x <= colsn - 8; x += 8)
{
internal::prefetch(lane + x);
uint16x8_t lane0 = vld1q_u16(lane + x - 2);
uint16x8_t lane4 = vld1q_u16(lane + x + 2);
uint16x8_t lane1 = vld1q_u16(lane + x - 1);
uint16x8_t lane3 = vld1q_u16(lane + x + 1);
uint16x8_t lane2 = vld1q_u16(lane + x + 0);
uint16x8_t ln04 = vaddq_u16(lane0, lane4);
uint16x8_t ln13 = vaddq_u16(lane1, lane3);
uint16x8_t ln042 = vmlaq_u16(ln04, lane2, vc6u16);
uint16x8_t lsw = vmlaq_u16(ln042, ln13, vc4u16);
uint8x8_t ls = vrshrn_n_u16(lsw, 8);
vst1_u8(dst + x, ls);
}
break;
case 2:
for (; x <= colsn - 8*2; x += 8*2)
{
internal::prefetch(lane + x);
u16* lidx0 = lane + x - 2*2;
u16* lidx1 = lane + x - 1*2;
u16* lidx3 = lane + x + 1*2;
u16* lidx4 = lane + x + 2*2;
#if !defined(__aarch64__) && defined(__GNUC__) && __GNUC__ == 4 &&  __GNUC_MINOR__ < 7 && !defined(__clang__)
__asm__ __volatile__ (
"vld2.16 {d0, d2}, [%[in0]]!                              \n\t"
"vld2.16 {d1, d3}, [%[in0]]                               \n\t"
"vld2.16 {d8, d10}, [%[in4]]!                             \n\t"
"vld2.16 {d9, d11}, [%[in4]]                              \n\t"
"vadd.i16 q0, q4                                          \n\t"
"vadd.i16 q1, q5                                          \n\t"
"vld2.16 {d16, d18}, [%[in1]]!                            \n\t"
"vld2.16 {d17, d19}, [%[in1]]                             \n\t"
"vld2.16 {d8, d10}, [%[in3]]!                             \n\t"
"vld2.16 {d9, d11}, [%[in3]]                              \n\t"
"vadd.i16 q4, q8                                          \n\t"
"vadd.i16 q5, q9                                          \n\t"
"vld2.16 {d16, d18}, [%[in2]]                             \n\t"
"vld2.16 {d17, d19}, [%[in22]]                            \n\t"
"vmla.i16 q0, q4, %q[c4]                                  \n\t"
"vmla.i16 q1, q5, %q[c4]                                  \n\t"
"vmla.i16 q0, q8, %q[c6]                                  \n\t"
"vmla.i16 q1, q9, %q[c6]                                  \n\t"
"vrshrn.u16 d8, q0, #8                                    \n\t"
"vrshrn.u16 d9, q1, #8                                    \n\t"
"vst2.8 {d8-d9}, [%[out]]                                 \n\t"
: [in0] "=r" (lidx0),
[in1] "=r" (lidx1),
[in3] "=r" (lidx3),
[in4] "=r" (lidx4)
: [out] "r" (dst + x),
"0" (lidx0),
"1" (lidx1),
"2" (lidx3),
"3" (lidx4),
[in2] "r" (lane + x),
[in22] "r" (lane + x + 4*2),
[c4] "w" (vc4u16), [c6] "w" (vc6u16)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23"
);
#else
uint16x8x2_t vLane0 = vld2q_u16(lidx0);
uint16x8x2_t vLane1 = vld2q_u16(lidx1);
uint16x8x2_t vLane2 = vld2q_u16(lane + x);
uint16x8x2_t vLane3 = vld2q_u16(lidx3);
uint16x8x2_t vLane4 = vld2q_u16(lidx4);
uint16x8_t vSum_0_4 = vaddq_u16(vLane0.val[0], vLane4.val[0]);
uint16x8_t vSum_1_5 = vaddq_u16(vLane0.val[1], vLane4.val[1]);
uint16x8_t vSum_4_8 = vaddq_u16(vLane1.val[0], vLane3.val[0]);
uint16x8_t vSum_5_9 = vaddq_u16(vLane1.val[1], vLane3.val[1]);
vSum_0_4 = vmlaq_u16(vSum_0_4, vSum_4_8, vc4u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vSum_5_9, vc4u16);
vSum_0_4 = vmlaq_u16(vSum_0_4, vLane2.val[0], vc6u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vLane2.val[1], vc6u16);
uint8x8x2_t vRes;
vRes.val[0] = vrshrn_n_u16(vSum_0_4, 8);
vRes.val[1] = vrshrn_n_u16(vSum_1_5, 8);
vst2_u8(dst + x, vRes);
#endif
}
break;
case 3:
for (; x <= colsn - 8*3; x += 8*3)
{
internal::prefetch(lane + x);
u16* lidx0 = lane + x - 2*3;
u16* lidx1 = lane + x - 1*3;
u16* lidx3 = lane + x + 1*3;
u16* lidx4 = lane + x + 2*3;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
__asm__ __volatile__ (
"vld3.16 {d0, d2, d4}, [%[in0]]!                          \n\t"
"vld3.16 {d1, d3, d5}, [%[in0]]                           \n\t"
"vld3.16 {d8, d10, d12}, [%[in4]]!                        \n\t"
"vld3.16 {d9, d11, d13}, [%[in4]]                         \n\t"
"vadd.i16 q0, q4                                          \n\t"
"vadd.i16 q1, q5                                          \n\t"
"vadd.i16 q2, q6                                          \n\t"
"vld3.16 {d16, d18, d20}, [%[in1]]!                       \n\t"
"vld3.16 {d17, d19, d21}, [%[in1]]                        \n\t"
"vld3.16 {d8, d10, d12}, [%[in3]]!                        \n\t"
"vld3.16 {d9, d11, d13}, [%[in3]]                         \n\t"
"vadd.i16 q4, q8                                          \n\t"
"vadd.i16 q5, q9                                          \n\t"
"vadd.i16 q6, q10                                         \n\t"
"vld3.16 {d16, d18, d20}, [%[in2]]                        \n\t"
"vld3.16 {d17, d19, d21}, [%[in22]]                       \n\t"
"vmla.i16 q0, q4, %q[c4]                                  \n\t"
"vmla.i16 q1, q5, %q[c4]                                  \n\t"
"vmla.i16 q2, q6, %q[c4]                                  \n\t"
"vmla.i16 q0, q8, %q[c6]                                  \n\t"
"vmla.i16 q1, q9, %q[c6]                                  \n\t"
"vmla.i16 q2, q10, %q[c6]                                 \n\t"
"vrshrn.u16 d8, q0, #8                                    \n\t"
"vrshrn.u16 d9, q1, #8                                    \n\t"
"vrshrn.u16 d10, q2, #8                                   \n\t"
"vst3.8 {d8-d10}, [%[out]]                                \n\t"
: [in0] "=r" (lidx0),
[in1] "=r" (lidx1),
[in3] "=r" (lidx3),
[in4] "=r" (lidx4)
: [out] "r" (dst + x),
"0" (lidx0),
"1" (lidx1),
"2" (lidx3),
"3" (lidx4),
[in2] "r" (lane + x),
[in22] "r" (lane + x + 4*3),
[c4] "w" (vc4u16), [c6] "w" (vc6u16)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23"
);
#else
uint16x8x3_t vLane0 = vld3q_u16(lidx0);
uint16x8x3_t vLane1 = vld3q_u16(lidx1);
uint16x8x3_t vLane2 = vld3q_u16(lane + x);
uint16x8x3_t vLane3 = vld3q_u16(lidx3);
uint16x8x3_t vLane4 = vld3q_u16(lidx4);
uint16x8_t vSum_0_4 = vaddq_u16(vLane0.val[0], vLane4.val[0]);
uint16x8_t vSum_1_5 = vaddq_u16(vLane0.val[1], vLane4.val[1]);
uint16x8_t vSum_2_6 = vaddq_u16(vLane0.val[2], vLane4.val[2]);
uint16x8_t vSum_3_1 = vaddq_u16(vLane3.val[0], vLane1.val[0]);
uint16x8_t vSum_4_2 = vaddq_u16(vLane3.val[1], vLane1.val[1]);
uint16x8_t vSum_5_6 = vaddq_u16(vLane3.val[2], vLane1.val[2]);
vSum_0_4 = vmlaq_u16(vSum_0_4, vSum_3_1, vc4u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vSum_4_2, vc4u16);
vSum_2_6 = vmlaq_u16(vSum_2_6, vSum_5_6, vc4u16);
vSum_0_4 = vmlaq_u16(vSum_0_4, vLane2.val[0], vc6u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vLane2.val[1], vc6u16);
vSum_2_6 = vmlaq_u16(vSum_2_6, vLane2.val[2], vc6u16);
uint8x8x3_t vRes;
vRes.val[0] = vrshrn_n_u16(vSum_0_4, 8);
vRes.val[1] = vrshrn_n_u16(vSum_1_5, 8);
vRes.val[2] = vrshrn_n_u16(vSum_2_6, 8);
vst3_u8(dst + x, vRes);
#endif
}
break;
case 4:
for (; x <= colsn - 8*4; x += 8*4)
{
internal::prefetch(lane + x);
internal::prefetch(lane + x + 16);
u16* lidx0 = lane + x - 2*4;
u16* lidx1 = lane + x - 1*4;
u16* lidx3 = lane + x + 1*4;
u16* lidx4 = lane + x + 2*4;
#if !defined(__aarch64__) && defined(__GNUC__) && defined(__arm__)
__asm__ __volatile__ (
"vld4.16 {d0, d2, d4, d6}, [%[in0]]!                      \n\t"
"vld4.16 {d1, d3, d5, d7}, [%[in0]]                       \n\t"
"vld4.16 {d8, d10, d12, d14}, [%[in4]]!                   \n\t"
"vld4.16 {d9, d11, d13, d15}, [%[in4]]                    \n\t"
"vadd.i16 q0, q4                                          \n\t"
"vadd.i16 q1, q5                                          \n\t"
"vadd.i16 q2, q6                                          \n\t"
"vadd.i16 q3, q7                                          \n\t"
"vld4.16 {d16, d18, d20, d22}, [%[in1]]!                  \n\t"
"vld4.16 {d17, d19, d21, d23}, [%[in1]]                   \n\t"
"vld4.16 {d8, d10, d12, d14}, [%[in3]]!                   \n\t"
"vld4.16 {d9, d11, d13, d15}, [%[in3]]                    \n\t"
"vadd.i16 q4, q8                                          \n\t"
"vadd.i16 q5, q9                                          \n\t"
"vadd.i16 q6, q10                                         \n\t"
"vadd.i16 q7, q11                                         \n\t"
"vld4.16 {d16, d18, d20, d22}, [%[in2],:256]              \n\t"
"vld4.16 {d17, d19, d21, d23}, [%[in22],:256]             \n\t"
"vmla.i16 q0, q4, %q[c4]                                  \n\t"
"vmla.i16 q1, q5, %q[c4]                                  \n\t"
"vmla.i16 q2, q6, %q[c4]                                  \n\t"
"vmla.i16 q3, q7, %q[c4]                                  \n\t"
"vmla.i16 q0, q8, %q[c6]                                  \n\t"
"vmla.i16 q1, q9, %q[c6]                                  \n\t"
"vmla.i16 q2, q10, %q[c6]                                 \n\t"
"vmla.i16 q3, q11, %q[c6]                                 \n\t"
"vrshrn.u16 d8, q0, #8                                    \n\t"
"vrshrn.u16 d9, q1, #8                                    \n\t"
"vrshrn.u16 d10, q2, #8                                   \n\t"
"vrshrn.u16 d11, q3, #8                                   \n\t"
"vst4.8 {d8-d11}, [%[out]]                                \n\t"
: [in0] "=r" (lidx0),
[in1] "=r" (lidx1),
[in3] "=r" (lidx3),
[in4] "=r" (lidx4)
: [out] "r" (dst + x),
"0" (lidx0),
"1" (lidx1),
"2" (lidx3),
"3" (lidx4),
[in2] "r" (lane + x),
[in22] "r" (lane + x + 4*4),
[c4] "w" (vc4u16), [c6] "w" (vc6u16)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23"
);
#else
uint16x8x4_t vLane0 = vld4q_u16(lidx0);
uint16x8x4_t vLane2 = vld4q_u16(lidx4);
uint16x8x4_t vLane4 = vld4q_u16(lidx1);
uint16x8x4_t vLane6 = vld4q_u16(lidx3);
uint16x8x4_t vLane8 = vld4q_u16(lane + x);
uint16x8_t vSum_0_4  = vaddq_u16(vLane0.val[0], vLane2.val[0]);
uint16x8_t vSum_1_5  = vaddq_u16(vLane0.val[1], vLane2.val[1]);
uint16x8_t vSum_2_6  = vaddq_u16(vLane0.val[2], vLane2.val[2]);
uint16x8_t vSum_3_7  = vaddq_u16(vLane0.val[3], vLane2.val[3]);
uint16x8_t vSum_4_8  = vaddq_u16(vLane4.val[0], vLane6.val[0]);
uint16x8_t vSum_5_9  = vaddq_u16(vLane4.val[1], vLane6.val[1]);
uint16x8_t vSum_6_10 = vaddq_u16(vLane4.val[2], vLane6.val[2]);
uint16x8_t vSum_7_11 = vaddq_u16(vLane4.val[3], vLane6.val[3]);
vSum_0_4 = vmlaq_u16(vSum_0_4, vSum_4_8, vc4u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vSum_5_9, vc4u16);
vSum_2_6 = vmlaq_u16(vSum_2_6, vSum_6_10, vc4u16);
vSum_3_7 = vmlaq_u16(vSum_3_7, vSum_7_11, vc4u16);
vSum_0_4 = vmlaq_u16(vSum_0_4, vLane8.val[0], vc6u16);
vSum_1_5 = vmlaq_u16(vSum_1_5, vLane8.val[1], vc6u16);
vSum_2_6 = vmlaq_u16(vSum_2_6, vLane8.val[2], vc6u16);
vSum_3_7 = vmlaq_u16(vSum_3_7, vLane8.val[3], vc6u16);
uint8x8x4_t vRes;
vRes.val[0] = vrshrn_n_u16(vSum_0_4, 8);
vRes.val[1] = vrshrn_n_u16(vSum_1_5, 8);
vRes.val[2] = vrshrn_n_u16(vSum_2_6, 8);
vRes.val[3] = vrshrn_n_u16(vSum_3_7, 8);
vst4_u8(dst + x, vRes);
#endif
}
break;
}
for (s32 h = 0; h < cn; ++h)
{
u16* ln = lane + h;
u8* dt = dst + h;
for (size_t k = x; k < colsn; k += cn)
{
dt[k] = (u8)((ln[k-2*cn] + ln[k+2*cn]
+ u16(4) * (ln[k-cn] + ln[k+cn])
+ u16(6) * ln[k] + (1 << 7)) >> 8);
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
void gaussianBlur5x5(const Size2D &size, s32 cn,
const u16 * srcBase, ptrdiff_t srcStride,
u16 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, u16 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isGaussianBlur5x5Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<u16> _tmp;
u16 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 4*cn, borderValue);
tmp = &_tmp[cn << 1];
}
ptrdiff_t idx_l1 = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_l2 = internal::borderInterpolate(-2, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r1 = internal::borderInterpolate(size.width + 0, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r2 = internal::borderInterpolate(size.width + 1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//1-line buffer
std::vector<u32> _buf(cn * (size.width + 4) + 32 / sizeof(u32));
u32* lane = internal::alignPtr(&_buf[cn << 1], 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = borderValue;
lane[-cn-cn+k] = borderValue;
lane[colsn+k] = borderValue;
lane[colsn+cn+k] = borderValue;
}
uint16x4_t vc6u16 = vmov_n_u16(6);
uint32x4_t vc6u32 = vmovq_n_u32(6);
uint32x4_t vc4u32 = vmovq_n_u32(4);
for (size_t i = 0; i < size.height; ++i)
{
u16* dst = internal::getRowPtr(dstBase, dstStride, i);
//vertical convolution
ptrdiff_t idx_rm2 = internal::borderInterpolate(i - 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const u16* ln0 = idx_rm2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm2) : tmp;
const u16* ln1 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const u16* ln2 = internal::getRowPtr(srcBase, srcStride, i);
const u16* ln3 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
const u16* ln4 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(internal::getRowPtr(ln2 + x, srcStride, x % 5 - 2));
uint16x4_t v0 = vld1_u16(ln0+x);
uint16x4_t v1 = vld1_u16(ln1+x);
uint16x4_t v2 = vld1_u16(ln2+x);
uint16x4_t v3 = vld1_u16(ln3+x);
uint16x4_t v4 = vld1_u16(ln4+x);
uint32x4_t v = vaddl_u16(v0, v4);
uint32x4_t v13 = vaddl_u16(v1, v3);
v = vmlal_u16(v, v2, vc6u16);
v = vmlaq_u32(v, v13, vc4u32);
vst1q_u32(lane + x, v);
}
for (; x < colsn; ++x)
lane[x] = ln0[x] + ln4[x] + 4*(ln1[x] + ln3[x]) + 6*ln2[x];
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = lane[idx_l1 + k];
lane[-cn-cn+k] = lane[idx_l2 + k];
lane[colsn+k] = lane[idx_r1 + k];
lane[colsn+cn+k] = lane[idx_r2 + k];
}
//horizontal convolution
x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(lane + x);
uint32x4_t lane0 = vld1q_u32(lane + x - 2);
uint32x4_t lane4 = vld1q_u32(lane + x + 2);
uint32x4_t lane1 = vld1q_u32(lane + x - 1);
uint32x4_t lane3 = vld1q_u32(lane + x + 1);
uint32x4_t lane2 = vld1q_u32(lane + x + 0);
uint32x4_t ln04 = vaddq_u32(lane0, lane4);
uint32x4_t ln13 = vaddq_u32(lane1, lane3);
uint32x4_t ln042 = vmlaq_u32(ln04, lane2, vc6u32);
uint32x4_t lsw = vmlaq_u32(ln042, ln13, vc4u32);
uint16x4_t ls = vrshrn_n_u32(lsw, 8);
vst1_u16(dst + x, ls);
}
for (s32 h = 0; h < cn; ++h)
{
u32* ln = lane + h;
u16* dt = dst + h;
for (size_t k = x; k < colsn; k += cn)
{
dt[k] = (u16)((ln[k-2*cn] + ln[k+2*cn] + 4*(ln[k-cn] + ln[k+cn]) + 6*ln[k] + (1<<7))>>8);
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
void gaussianBlur5x5(const Size2D &size, s32 cn,
const s16 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, s16 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isGaussianBlur5x5Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<s16> _tmp;
s16 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 4*cn, borderValue);
tmp = &_tmp[cn << 1];
}
ptrdiff_t idx_l1 = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_l2 = internal::borderInterpolate(-2, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r1 = internal::borderInterpolate(size.width + 0, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r2 = internal::borderInterpolate(size.width + 1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//1-line buffer
std::vector<s32> _buf(cn * (size.width + 4) + 32 / sizeof(s32));
s32* lane = internal::alignPtr(&_buf[cn << 1], 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = borderValue;
lane[-cn-cn+k] = borderValue;
lane[colsn+k] = borderValue;
lane[colsn+cn+k] = borderValue;
}
int16x4_t vc6s16 = vmov_n_s16(6);
int32x4_t vc6s32 = vmovq_n_s32(6);
int32x4_t vc4s32 = vmovq_n_s32(4);
for (size_t i = 0; i < size.height; ++i)
{
s16* dst = internal::getRowPtr(dstBase, dstStride, i);
//vertical convolution
ptrdiff_t idx_rm2 = internal::borderInterpolate(i - 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const s16* ln0 = idx_rm2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm2) : tmp;
const s16* ln1 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const s16* ln2 = internal::getRowPtr(srcBase, srcStride, i);
const s16* ln3 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
const s16* ln4 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(internal::getRowPtr(ln2 + x, srcStride, x % 5 - 2));
int16x4_t v0 = vld1_s16(ln0+x);
int16x4_t v1 = vld1_s16(ln1+x);
int16x4_t v2 = vld1_s16(ln2+x);
int16x4_t v3 = vld1_s16(ln3+x);
int16x4_t v4 = vld1_s16(ln4+x);
int32x4_t v = vaddl_s16(v0, v4);
int32x4_t v13 = vaddl_s16(v1, v3);
v = vmlal_s16(v, v2, vc6s16);
v = vmlaq_s32(v, v13, vc4s32);
vst1q_s32(lane + x, v);
}
for (; x < colsn; ++x)
lane[x] = ln0[x] + ln4[x] + 4*(ln1[x] + ln3[x]) + 6*ln2[x];
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = lane[idx_l1 + k];
lane[-cn-cn+k] = lane[idx_l2 + k];
lane[colsn+k] = lane[idx_r1 + k];
lane[colsn+cn+k] = lane[idx_r2 + k];
}
//horizontal convolution
x = 0;
switch(cn)
{
case 1:
case 2:
case 3:
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(lane + x);
int32x4_t lane0 = vld1q_s32(lane + x - 2);
int32x4_t lane4 = vld1q_s32(lane + x + 2);
int32x4_t lane1 = vld1q_s32(lane + x - 1);
int32x4_t lane3 = vld1q_s32(lane + x + 1);
int32x4_t lane2 = vld1q_s32(lane + x + 0);
int32x4_t ln04 = vaddq_s32(lane0, lane4);
int32x4_t ln13 = vaddq_s32(lane1, lane3);
int32x4_t ln042 = vmlaq_s32(ln04, lane2, vc6s32);
int32x4_t lsw = vmlaq_s32(ln042, ln13, vc4s32);
int16x4_t ls = vrshrn_n_s32(lsw, 8);
vst1_s16(dst + x, ls);
}
break;
case 4:
/*            for (; x <= colsn - 4*4; x += 4*4)
{
internal::prefetch(lane + x);
internal::prefetch(lane + x + 16);
ptrdiff_t* lidx0 = lane + x - 2*4;
ptrdiff_t* lidx1 = lane + x - 1*4;
ptrdiff_t* lidx3 = lane + x + 1*4;
ptrdiff_t* lidx4 = lane + x + 2*4;
__asm__ __volatile__ (
"vld4.32 {d0, d2, d4, d6}, [%[in0]]!                      \n\t"
"vld4.32 {d1, d3, d5, d7}, [%[in0]]                       \n\t"
"vld4.32 {d8, d10, d12, d14}, [%[in4]]!                   \n\t"
"vld4.32 {d9, d11, d13, d15}, [%[in4]]                    \n\t"
"vadd.i32 q0, q4                                          \n\t"
"vadd.i32 q1, q5                                          \n\t"
"vadd.i32 q2, q6                                          \n\t"
"vadd.i32 q3, q7                                          \n\t"
"vld4.32 {d16, d18, d20, d22}, [%[in1]]!                  \n\t"
"vld4.32 {d17, d19, d21, d23}, [%[in1]]                   \n\t"
"vld4.32 {d8, d10, d12, d14}, [%[in3]]!                   \n\t"
"vld4.32 {d9, d11, d13, d15}, [%[in3]]                    \n\t"
"vadd.i32 q4, q8                                          \n\t"
"vadd.i32 q5, q9                                          \n\t"
"vadd.i32 q6, q10                                         \n\t"
"vadd.i32 q7, q11                                         \n\t"
"vld4.32 {d16, d18, d20, d22}, [%[in2],:256]              \n\t"
"vld4.32 {d17, d19, d21, d23}, [%[in22],:256]             \n\t"
"vmla.i32 q0, q4, %q[c4]                                  \n\t"
"vmla.i32 q1, q5, %q[c4]                                  \n\t"
"vmla.i32 q2, q6, %q[c4]                                  \n\t"
"vmla.i32 q3, q7, %q[c4]                                  \n\t"
"vmla.i32 q0, q8, %q[c6]                                  \n\t"
"vmla.i32 q1, q9, %q[c6]                                  \n\t"
"vmla.i32 q2, q10, %q[c6]                                 \n\t"
"vmla.i32 q3, q11, %q[c6]                                 \n\t"
"vrshrn.i32 d8, q0, #8                                    \n\t"
"vrshrn.i32 d9, q1, #8                                    \n\t"
"vrshrn.i32 d10, q2, #8                                   \n\t"
"vrshrn.i32 d11, q3, #8                                   \n\t"
"vst4.16 {d8-d11}, [%[out]]                                \n\t"
: [in0] "=r" (lidx0),
[in1] "=r" (lidx1),
[in3] "=r" (lidx3),
[in4] "=r" (lidx4)
: [out] "r" (dst + x),
"0" (lidx0),
"1" (lidx1),
"2" (lidx3),
"3" (lidx4),
[in2] "r" (lane + x),
[in22] "r" (lane + x + 4*2),
[c4] "w" (vc4s32), [c6] "w" (vc6s32)
: "d0","d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14","d15","d16","d17","d18","d19","d20","d21","d22","d23"
);
*/
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(lane + x);
int32x4_t lane0 = vld1q_s32(lane + x - 2);
int32x4_t lane4 = vld1q_s32(lane + x + 2);
int32x4_t lane1 = vld1q_s32(lane + x - 1);
int32x4_t lane3 = vld1q_s32(lane + x + 1);
int32x4_t lane2 = vld1q_s32(lane + x + 0);
int32x4_t ln04 = vaddq_s32(lane0, lane4);
int32x4_t ln13 = vaddq_s32(lane1, lane3);
int32x4_t ln042 = vmlaq_s32(ln04, lane2, vc6s32);
int32x4_t lsw = vmlaq_s32(ln042, ln13, vc4s32);
int16x4_t ls = vrshrn_n_s32(lsw, 8);
vst1_s16(dst + x, ls);
}
break;
}
for (s32 h = 0; h < cn; ++h)
{
s32* ln = lane + h;
s16* dt = dst + h;
for (size_t k = x; k < colsn; k += cn)
{
dt[k] = (s16)((ln[k-2*cn] + ln[k+2*cn] + 4*(ln[k-cn] + ln[k+cn]) + 6*ln[k] + (1<<7))>>8);
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
void gaussianBlur5x5(const Size2D &size, s32 cn,
const s32 * srcBase, ptrdiff_t srcStride,
s32 * dstBase, ptrdiff_t dstStride,
BORDER_MODE borderType, s32 borderValue, Margin borderMargin)
{
internal::assertSupportedConfiguration(isGaussianBlur5x5Supported(size, cn, borderType));
#ifdef CAROTENE_NEON
size_t colsn = size.width * cn;
std::vector<s32> _tmp;
s32 *tmp = 0;
if (borderType == BORDER_MODE_CONSTANT)
{
_tmp.assign(colsn + 4*cn, borderValue);
tmp = &_tmp[cn << 1];
}
ptrdiff_t idx_l1 = internal::borderInterpolate(-1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_l2 = internal::borderInterpolate(-2, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r1 = internal::borderInterpolate(size.width + 0, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
ptrdiff_t idx_r2 = internal::borderInterpolate(size.width + 1, size.width, borderType, borderMargin.left, borderMargin.right) * cn;
//1-line buffer
std::vector<s32> _buf(cn * (size.width + 4) + 32 / sizeof(s32));
s32* lane = internal::alignPtr(&_buf[cn << 1], 32);
if (borderType == BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = borderValue;
lane[-cn-cn+k] = borderValue;
lane[colsn+k] = borderValue;
lane[colsn+cn+k] = borderValue;
}
int32x4_t vc6s32 = vmovq_n_s32(6);
int32x4_t vc4s32 = vmovq_n_s32(4);
for (size_t i = 0; i < size.height; ++i)
{
s32* dst = internal::getRowPtr(dstBase, dstStride, i);
//vertical convolution
ptrdiff_t idx_rm2 = internal::borderInterpolate(i - 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rm1 = internal::borderInterpolate(i - 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp1 = internal::borderInterpolate(i + 1, size.height, borderType, borderMargin.top, borderMargin.bottom);
ptrdiff_t idx_rp2 = internal::borderInterpolate(i + 2, size.height, borderType, borderMargin.top, borderMargin.bottom);
const s32* ln0 = idx_rm2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm2) : tmp;
const s32* ln1 = idx_rm1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rm1) : tmp;
const s32* ln2 = internal::getRowPtr(srcBase, srcStride, i);
const s32* ln3 = idx_rp1 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp1) : tmp;
const s32* ln4 = idx_rp2 >= -(ptrdiff_t)borderMargin.top ? internal::getRowPtr(srcBase, srcStride, idx_rp2) : tmp;
size_t x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(internal::getRowPtr(ln2 + x, srcStride, x % 5 - 2));
int32x4_t v0 = vld1q_s32(ln0+x);
int32x4_t v1 = vld1q_s32(ln1+x);
int32x4_t v2 = vld1q_s32(ln2+x);
int32x4_t v3 = vld1q_s32(ln3+x);
int32x4_t v4 = vld1q_s32(ln4+x);
int32x4_t v = vaddq_s32(v0, v4);
int32x4_t v13 = vaddq_s32(v1, v3);
v = vmlaq_s32(v, v2, vc6s32);
v = vmlaq_s32(v, v13, vc4s32);
vst1q_s32(lane + x, v);
}
for (; x < colsn; ++x)
lane[x] = ln0[x] + ln4[x] + 4*(ln1[x] + ln3[x]) + 6*ln2[x];
//left&right borders
if (borderType != BORDER_MODE_CONSTANT)
for (s32 k = 0; k < cn; ++k)
{
lane[-cn+k] = lane[idx_l1 + k];
lane[-cn-cn+k] = lane[idx_l2 + k];
lane[colsn+k] = lane[idx_r1 + k];
lane[colsn+cn+k] = lane[idx_r2 + k];
}
//horizontal convolution
x = 0;
for (; x <= colsn - 4; x += 4)
{
internal::prefetch(lane + x);
int32x4_t lane0 = vld1q_s32(lane + x - 2);
int32x4_t lane4 = vld1q_s32(lane + x + 2);
int32x4_t lane1 = vld1q_s32(lane + x - 1);
int32x4_t lane3 = vld1q_s32(lane + x + 1);
int32x4_t lane2 = vld1q_s32(lane + x + 0);
int32x4_t ln04 = vaddq_s32(lane0, lane4);
int32x4_t ln13 = vaddq_s32(lane1, lane3);
int32x4_t ln042 = vmlaq_s32(ln04, lane2, vc6s32);
int32x4_t lsw = vmlaq_s32(ln042, ln13, vc4s32);
vst1q_s32(dst + x, lsw);
}
for (s32 h = 0; h < cn; ++h)
{
s32* ln = lane + h;
s32* dt = dst + h;
for (size_t k = x; k < colsn; k += cn)
{
dt[k] = ln[k-2*cn] + ln[k+2*cn] + 4*(ln[k-cn] + ln[k+cn]) + 6*ln[k];
}
}
}
#else
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)borderValue;
(void)borderMargin;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
namespace CAROTENE_NS {
void integral(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u32 * sumBase, ptrdiff_t sumStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
uint32x4_t v_zero = vmovq_n_u32(0u);
// the first iteration
const u8 * src = internal::getRowPtr(srcBase, srcStride, 0);
u32 * sum = internal::getRowPtr(sumBase, sumStride, 0);
uint32x4_t prev = v_zero;
size_t j = 0u;
for ( ; j + 7 < size.width; j += 8)
{
internal::prefetch(sum + j);
internal::prefetch(src + j);
uint8x8_t el8shr0 = vld1_u8(src + j);
uint8x8_t el8shr1 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 8));
uint8x8_t el8shr2 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 16));
uint8x8_t el8shr3 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 24));
uint16x8_t el8shr12 =  vaddl_u8(el8shr1, el8shr2);
uint16x8_t el8shr03 =  vaddl_u8(el8shr0, el8shr3);
uint16x8_t el8 = vaddq_u16(el8shr12, el8shr03);
uint16x4_t el4h = vadd_u16(vget_low_u16(el8), vget_high_u16(el8));
uint32x4_t vsuml = vaddw_u16(prev, vget_low_u16(el8));
uint32x4_t vsumh = vaddw_u16(prev, el4h);
vst1q_u32(sum + j, vsuml);
vst1q_u32(sum + j + 4, vsumh);
prev = vaddw_u16(prev, vdup_lane_u16(el4h, 3));
}
for (u32 v = vgetq_lane_u32(prev, 3); j < size.width; ++j)
sum[j] = (v += src[j]);
// the others
for (size_t i = 1; i < size.height ; ++i)
{
src = internal::getRowPtr(srcBase, srcStride, i);
u32 * prevSum = internal::getRowPtr(sumBase, sumStride, i - 1);
sum = internal::getRowPtr(sumBase, sumStride, i);
prev = v_zero;
j = 0u;
for ( ; j + 7 < size.width; j += 8)
{
internal::prefetch(sum + j);
internal::prefetch(src + j);
uint32x4_t vsuml = vld1q_u32(prevSum + j);
uint32x4_t vsumh = vld1q_u32(prevSum + j + 4);
uint8x8_t el8shr0 = vld1_u8(src + j);
uint8x8_t el8shr1 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 8));
uint8x8_t el8shr2 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 16));
uint8x8_t el8shr3 = vreinterpret_u8_u64(vshl_n_u64(vreinterpret_u64_u8(el8shr0), 24));
vsuml = vaddq_u32(vsuml, prev);
vsumh = vaddq_u32(vsumh, prev);
uint16x8_t el8shr12 =  vaddl_u8(el8shr1, el8shr2);
uint16x8_t el8shr03 =  vaddl_u8(el8shr0, el8shr3);
uint16x8_t el8 = vaddq_u16(el8shr12, el8shr03);
uint16x4_t el4h = vadd_u16(vget_low_u16(el8), vget_high_u16(el8));
vsuml = vaddw_u16(vsuml, vget_low_u16(el8));
vsumh = vaddw_u16(vsumh, el4h);
vst1q_u32(sum + j, vsuml);
vst1q_u32(sum + j + 4, vsumh);
prev = vaddw_u16(prev, vdup_lane_u16(el4h, 3));
}
for (u32 v = vgetq_lane_u32(prev, 3); j < size.width; ++j)
sum[j] = (v += src[j]) + prevSum[j];
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)sumBase;
(void)sumStride;
#endif
}
void sqrIntegral(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
f64 * sqsumBase, ptrdiff_t sqsumStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
uint16x8_t v_zero8 = vmovq_n_u16(0u);
// the first iteration
const u8 * src = internal::getRowPtr(srcBase, srcStride, 0);
f64 * sqsum = internal::getRowPtr(sqsumBase, sqsumStride, 0);
double prev = 0.;
size_t j = 0u;
for ( ; j + 7 < size.width; j += 8)
{
internal::prefetch(sqsum + j);
internal::prefetch(src + j);
uint8x8_t vsrc = vld1_u8(src + j);
uint16x8_t el8shr0 = vmull_u8(vsrc, vsrc);
uint16x8_t el8shr1 = vextq_u16(v_zero8, el8shr0, 7);
uint32x4_t el8shr01l =  vaddl_u16(vget_low_u16(el8shr0), vget_low_u16(el8shr1));
uint32x4_t el8shr01h =  vaddl_u16(vget_high_u16(el8shr0), vget_high_u16(el8shr1));
uint32x4_t el4h = vaddq_u32(el8shr01l, el8shr01h);
uint32x2_t el2l = vadd_u32(vget_low_u32(el8shr01l), vget_high_u32(el8shr01l));
uint32x2_t el2hl = vadd_u32(vget_low_u32(el4h), vget_high_u32(el8shr01l));
uint32x2_t el2hh = vadd_u32(vget_low_u32(el4h), vget_high_u32(el4h));
u32 buf[8];
vst1_u32(buf, vget_low_u32(el8shr01l));
vst1_u32(buf+2, el2l);
vst1_u32(buf+4, el2hl);
vst1_u32(buf+6, el2hh);
for(u32 k=0; k < 8; k++)
sqsum[j+k] = prev + buf[k];
prev += buf[7];
}
for (; j < size.width; ++j)
sqsum[j] = (prev += src[j]*src[j]);
// the others
for (size_t i = 1; i < size.height ; ++i)
{
src = internal::getRowPtr(srcBase, srcStride, i);
f64 * prevSqSum = internal::getRowPtr(sqsumBase, sqsumStride, i - 1);
sqsum = internal::getRowPtr(sqsumBase, sqsumStride, i);
prev = 0.;
j = 0u;
for ( ; j + 7 < size.width; j += 8)
{
internal::prefetch(sqsum + j);
internal::prefetch(src + j);
uint8x8_t vsrc = vld1_u8(src + j);
uint16x8_t el8shr0 = vmull_u8(vsrc, vsrc);
uint16x8_t el8shr1 = vextq_u16(v_zero8, el8shr0, 7);
uint32x4_t el8shr01l =  vaddl_u16(vget_low_u16(el8shr0), vget_low_u16(el8shr1));
uint32x4_t el8shr01h =  vaddl_u16(vget_high_u16(el8shr0), vget_high_u16(el8shr1));
uint32x4_t el4h = vaddq_u32(el8shr01l, el8shr01h);
uint32x2_t el2l = vadd_u32(vget_low_u32(el8shr01l), vget_high_u32(el8shr01l));
uint32x2_t el2hl = vadd_u32(vget_low_u32(el4h), vget_high_u32(el8shr01l));
uint32x2_t el2hh = vadd_u32(vget_low_u32(el4h), vget_high_u32(el4h));
u32 buf[8];
vst1_u32(buf, vget_low_u32(el8shr01l));
vst1_u32(buf+2, el2l);
vst1_u32(buf+4, el2hl);
vst1_u32(buf+6, el2hh);
for(u32 k=0; k < 8; k++)
sqsum[j+k] = prev + prevSqSum[j+k] + buf[k];
prev += buf[7];
}
for (; j < size.width; ++j)
sqsum[j] = (prev += src[j]*src[j]) + prevSqSum[j];
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)sqsumBase;
(void)sqsumStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
inline void vnst(u8* dst, uint8x16_t v1, uint8x16_t v2) { vst1q_u8(dst, v1); vst1q_u8(dst+16, v2); }
inline void vnst(u8* dst, uint16x8_t v1, uint16x8_t v2) { vst1q_u8(dst, vcombine_u8(vmovn_u16(v1), vmovn_u16(v2))); }
inline void vnst(u8* dst, uint32x4_t v1, uint32x4_t v2) { vst1_u8(dst, vmovn_u16(vcombine_u16(vmovn_u32(v1), vmovn_u32(v2)))); }
template <typename T, int elsize> struct vtail
{
static inline void inRange(const T *, const T *, const T *,
u8 *, size_t &, size_t)
{
//do nothing since there couldn't be enough data
}
};
template <typename T> struct vtail<T, 2>
{
static inline void inRange(const T * src, const T * rng1, const T * rng2,
u8 * dst, size_t &x, size_t width)
{
typedef typename internal::VecTraits<T>::vec128 vec128;
typedef typename internal::VecTraits<T>::unsign::vec128 uvec128;
//There no more than 15 elements in the tail, so we could handle 8 element vector only once
if( x + 8 < width)
{
vec128  vs = internal::vld1q( src + x);
vec128 vr1 = internal::vld1q(rng1 + x);
vec128 vr2 = internal::vld1q(rng2 + x);
uvec128  vd = internal::vandq(internal::vcgeq(vs, vr1), internal::vcgeq(vr2, vs));
internal::vst1(dst + x, internal::vmovn(vd));
x+=8;
}
}
};
template <typename T> struct vtail<T, 1>
{
static inline void inRange(const T * src, const T * rng1, const T * rng2,
u8 * dst, size_t &x, size_t width)
{
typedef typename internal::VecTraits<T>::vec128 vec128;
typedef typename internal::VecTraits<T>::unsign::vec128 uvec128;
typedef typename internal::VecTraits<T>::vec64 vec64;
typedef typename internal::VecTraits<T>::unsign::vec64 uvec64;
//There no more than 31 elements in the tail, so we could handle once 16+8 or 16 or 8 elements
if( x + 16 < width)
{
vec128  vs = internal::vld1q( src + x);
vec128 vr1 = internal::vld1q(rng1 + x);
vec128 vr2 = internal::vld1q(rng2 + x);
uvec128  vd = internal::vandq(internal::vcgeq(vs, vr1), internal::vcgeq(vr2, vs));
internal::vst1q(dst + x, vd);
x+=16;
}
if( x + 8 < width)
{
vec64  vs = internal::vld1( src + x);
vec64 vr1 = internal::vld1(rng1 + x);
vec64 vr2 = internal::vld1(rng2 + x);
uvec64  vd = internal::vand(internal::vcge(vs, vr1), internal::vcge(vr2, vs));
internal::vst1(dst + x, vd);
x+=8;
}
}
};
template <typename T>
inline void inRangeCheck(const Size2D &_size,
const T * srcBase, ptrdiff_t srcStride,
const T * rng1Base, ptrdiff_t rng1Stride,
const T * rng2Base, ptrdiff_t rng2Stride,
u8 * dstBase, ptrdiff_t dstStride)
{
typedef typename internal::VecTraits<T>::vec128 vec128;
typedef typename internal::VecTraits<T>::unsign::vec128 uvec128;
Size2D size(_size);
if (srcStride == dstStride &&
srcStride == rng1Stride &&
srcStride == rng2Stride &&
srcStride == (ptrdiff_t)(size.width))
{
size.width *= size.height;
size.height = 1;
}
const size_t width = size.width & ~( 32/sizeof(T) - 1 );
for(size_t j = 0; j < size.height; ++j)
{
const T *  src = internal::getRowPtr( srcBase,  srcStride, j);
const T * rng1 = internal::getRowPtr(rng1Base, rng1Stride, j);
const T * rng2 = internal::getRowPtr(rng2Base, rng2Stride, j);
u8 *  dst = internal::getRowPtr( dstBase,  dstStride, j);
size_t i = 0;
for( ; i < width; i += 32/sizeof(T) )
{
internal::prefetch(src + i);
internal::prefetch(rng1 + i);
internal::prefetch(rng2 + i);
vec128  vs = internal::vld1q( src + i);
vec128 vr1 = internal::vld1q(rng1 + i);
vec128 vr2 = internal::vld1q(rng2 + i);
uvec128 vd1 = internal::vandq(internal::vcgeq(vs, vr1), internal::vcgeq(vr2, vs));
vs = internal::vld1q( src + i + 16/sizeof(T));
vr1 = internal::vld1q(rng1 + i + 16/sizeof(T));
vr2 = internal::vld1q(rng2 + i + 16/sizeof(T));
uvec128 vd2 = internal::vandq(internal::vcgeq(vs, vr1), internal::vcgeq(vr2, vs));
vnst(dst + i, vd1, vd2);
}
vtail<T, sizeof(T)>::inRange(src, rng1, rng2, dst, i, size.width);
for( ; i < size.width; i++ )
dst[i] = (u8)(-(rng1[i] <= src[i] && src[i] <= rng2[i]));
}
}
}
#define INRANGEFUNC(T)                                       \
void inRange(const Size2D &_size,                            \
const T * srcBase, ptrdiff_t srcStride,         \
const T * rng1Base, ptrdiff_t rng1Stride,       \
const T * rng2Base, ptrdiff_t rng2Stride,       \
u8 * dstBase, ptrdiff_t dstStride)              \
{                                                            \
internal::assertSupportedConfiguration();                \
inRangeCheck(_size, srcBase, srcStride,                  \
rng1Base, rng1Stride, rng2Base, rng2Stride, \
dstBase, dstStride);                        \
}
#else
#define INRANGEFUNC(T)                                       \
void inRange(const Size2D &,                                 \
const T *, ptrdiff_t,                           \
const T *, ptrdiff_t,                           \
const T *, ptrdiff_t,                           \
u8 *, ptrdiff_t)                                \
{                                                            \
internal::assertSupportedConfiguration();                \
}
#endif
INRANGEFUNC(u8)
INRANGEFUNC(s8)
INRANGEFUNC(u16)
INRANGEFUNC(s16)
INRANGEFUNC(s32)
INRANGEFUNC(f32)
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2015, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "saturate_cast.hpp"
#include <vector>
namespace CAROTENE_NS {
bool isLaplacian3x3Supported(const Size2D &size, BORDER_MODE border)
{
return isSupportedConfiguration() && size.width >= 8 &&
(border == BORDER_MODE_CONSTANT ||
border == BORDER_MODE_REPLICATE);
}
void Laplacian3x3(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
u8 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isLaplacian3x3Supported(size, border));
#ifdef CAROTENE_NEON
const uint16x8_t v_border_x3 = vdupq_n_u16(borderValue * 3);
const uint16x8_t v_zero = vdupq_n_u16(0);
const uint8x8_t v_border = vdup_n_u8(borderValue);
uint8x8_t vsub;
uint16x8_t tprev = v_zero, tcurr = v_zero, tnext = v_zero;
uint16x8_t t0 = v_zero, t1 = v_zero, t2 = v_zero;
ptrdiff_t width = (ptrdiff_t)size.width, height = (ptrdiff_t)size.height;
for (ptrdiff_t y = 0; y < height; ++y)
{
const u8 * srow0 = y == 0 && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::max<ptrdiff_t>(y - 1, 0));
const u8 * srow1 = internal::getRowPtr(srcBase, srcStride, y);
const u8 * srow2 = y + 1 == height && border == BORDER_MODE_CONSTANT ? NULL : internal::getRowPtr(srcBase, srcStride, std::min(y + 1, height - 1));
u8 * drow = internal::getRowPtr(dstBase, dstStride, y);
s16 prevx = 0, currx = 0, nextx = 0;
ptrdiff_t x = 0;
const ptrdiff_t bwidth = y + 2 < height ? width : (width - 8);
// perform vertical convolution
for ( ; x <= bwidth; x += 8)
{
internal::prefetch(srow0 + x);
internal::prefetch(srow1 + x);
internal::prefetch(srow2 + x);
uint8x8_t x0 = !srow0 ? v_border : vld1_u8(srow0 + x);
uint8x8_t x1 = vld1_u8(srow1 + x);
uint8x8_t x2 = !srow2 ? v_border : vld1_u8(srow2 + x);
// calculate values for plain CPU part below if needed
if (x + 8 >= bwidth)
{
ptrdiff_t x3 = x == width ? width - 1 : x;
ptrdiff_t x4 = border == BORDER_MODE_CONSTANT ? x3 - 1 : std::max<ptrdiff_t>(x3 - 1, 0);
if (border == BORDER_MODE_CONSTANT && x4 < 0)
prevx = borderValue;
else
prevx = (srow2 ? srow2[x4] : borderValue) + srow1[x4] + (srow0 ? srow0[x4] : borderValue);
currx = (srow2 ? srow2[x3] : borderValue) + srow1[x3] + (srow0 ? srow0[x3] : borderValue);
}
// make shift
if (x)
{
tprev = tcurr;
tcurr = tnext;
}
// and calculate next value
tnext = vaddw_u8(vaddl_u8(x0, x1), x2);
// make extrapolation for the first elements
if (!x)
{
// make border
if (border == BORDER_MODE_CONSTANT)
tcurr = v_border_x3;
else if (border == BORDER_MODE_REPLICATE)
tcurr = vdupq_n_u16(vgetq_lane_u16(tnext, 0));
vsub = x1;
continue;
}
// combine 3 "shifted" vectors
t0 = vextq_u16(tprev, tcurr, 7);
t1 = tcurr;
t2 = vextq_u16(tcurr, tnext, 1);
// and add them
t0 = vqaddq_u16(t0, vqaddq_u16(t1, t2));
int16x8_t tt0 = vsubq_s16(vreinterpretq_s16_u16(t0),
vreinterpretq_s16_u16(vaddw_u8(vshll_n_u8(vsub, 3), vsub)));
uint8x8_t it0 = vqmovun_s16(tt0);
vst1_u8(drow + x - 8, it0);
vsub = x1;
}
x -= 8;
if (x == width)
--x;
for ( ; x < width; ++x)
{
// make extrapolation for the last elements
if (x + 1 >= width)
{
if (border == BORDER_MODE_CONSTANT)
nextx = borderValue * 3;
else if (border == BORDER_MODE_REPLICATE)
nextx = srow2[x] + srow1[x] + srow0[x];
}
else
{
nextx = (srow2 ? srow2[x + 1] : borderValue) +
srow1[x + 1] +
(srow0 ? srow0[x + 1] : borderValue);
}
s32 val = (prevx + currx + nextx) - 9 * srow1[x];
drow[x] = internal::saturate_cast<u8>((s32)val);
// make shift
prevx = currx;
currx = nextx;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
#endif
}
bool isLaplacianOpenCVSupported(const Size2D &size, BORDER_MODE border)
{
return isSupportedConfiguration() &&
size.width >= 8 && size.height >= 1 &&
(border == BORDER_MODE_CONSTANT   ||
border == BORDER_MODE_REFLECT    ||
border == BORDER_MODE_REFLECT101 ||
border == BORDER_MODE_REPLICATE);
}
void Laplacian1OpenCV(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isLaplacianOpenCVSupported(size, border));
#ifdef CAROTENE_NEON
ptrdiff_t rows = size.height, cols = size.width;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (border == BORDER_MODE_CONSTANT)
{
_tmp.assign(cols + 4,borderValue);
tmp = &_tmp[2];
}
for( ptrdiff_t y = 0; y < rows; y++ )
{
const u8* v0 = 0;
const u8* v1 = internal::getRowPtr(srcBase, srcStride, y);
const u8* v2 = 0;
// make border
if (border == BORDER_MODE_REFLECT101) {
v0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : y+1);
v2 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 1 ? rows-2 : 0);
} else  if (border == BORDER_MODE_CONSTANT) {
v0 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
v2 =  y < rows-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
} else {
v0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
v2 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 0 ? rows-1 : 0);
}
s16* drow = internal::getRowPtr(dstBase, dstStride, y);
int16x8_t tcurr = vmovq_n_s16(0x0);
int16x8_t tnext = vmovq_n_s16(0x0);
int16x8_t t0, t2;
uint8x8_t xx0 = vmov_n_u8(0x0);
uint8x8_t xx1 = vmov_n_u8(0x0);
uint8x8_t xx2 = vmov_n_u8(0x0);
ptrdiff_t x = 0;
const ptrdiff_t bcols = y + 2 < rows ? cols : (cols - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(v0 + x);
internal::prefetch(v1 + x);
internal::prefetch(v2 + x);
uint8x8_t x0 = vld1_u8(v0 + x);
uint8x8_t x1 = vld1_u8(v1 + x);
uint8x8_t x2 = vld1_u8(v2 + x);
if(x) {
xx0 = xx1;
xx1 = xx2;
} else {
xx1 = x1;
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT)
{
xx1 = vset_lane_u8(vget_lane_u8(x1, 0),x1, 7);
}
else if (border == BORDER_MODE_CONSTANT)
{
xx1 = vset_lane_u8(borderValue, x1, 7);
}
else if (border == BORDER_MODE_REFLECT101)
{
xx1 = vset_lane_u8(vget_lane_u8(x1, 1),x1, 7);
}
}
xx2 = x1;
if(x) {
tcurr = tnext;
}
tnext = vsubq_s16(vreinterpretq_s16_u16(vaddl_u8(x0, x2)),
vreinterpretq_s16_u16(vshll_n_u8(x1, 2)));
if(!x) {
tcurr = tnext;
continue;
}
t0 = vreinterpretq_s16_u16(vmovl_u8(vext_u8(xx0, xx1, 7)));
t2 = vreinterpretq_s16_u16(vmovl_u8(vext_u8(xx1, xx2, 1)));
t0 = vaddq_s16(vqaddq_s16(t0, t2), tcurr);
vst1q_s16(drow + x - 8, t0);
}
x -= 8;
if(x == cols){
x--;
}
for( ; x < cols; x++ )
{
s16 nextx;
s16 prevx;
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT)
{
prevx = x == 0 ? v1[0] : v1[x-1];
nextx = x == cols-1 ? v1[x] : v1[x+1];
}
else if (border == BORDER_MODE_REFLECT101)
{
prevx = x == 0 ? v1[1] : v1[x-1];
nextx = x == cols-1 ? v1[x-1] : v1[x+1];
}
else //if (border == BORDER_MODE_CONSTANT)
{
prevx = x == 0 ? borderValue : v1[x-1];
nextx = x == cols-1 ? borderValue : v1[x+1];
}
*(drow+x) = prevx + nextx - 4*v1[x] + v0[x] + v2[x];
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
#endif
}
void Laplacian3OpenCV(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isLaplacianOpenCVSupported(size, border));
#ifdef CAROTENE_NEON
ptrdiff_t rows = size.height, cols = size.width;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (border == BORDER_MODE_CONSTANT)
{
_tmp.assign(cols + 4,borderValue);
tmp = &_tmp[2];
}
for( ptrdiff_t y = 0; y < rows; y++ )
{
const u8* v0 = 0;
const u8* v1 = internal::getRowPtr(srcBase, srcStride, y);
const u8* v2 = 0;
// make border
if (border == BORDER_MODE_REFLECT101) {
v0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : y+1);
v2 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 1 ? rows-2 : 0);
} else  if (border == BORDER_MODE_CONSTANT) {
v0 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
v2 = y < rows-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
} else {
v0 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
v2 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 0 ? rows-1 : 0);
}
s16* drow = internal::getRowPtr(dstBase, dstStride, y);
int16x8_t tprev = vmovq_n_s16(0x0);
int16x8_t tcurr = vmovq_n_s16(0x0);
int16x8_t tnext = vmovq_n_s16(0x0);
int16x8_t tc = vmovq_n_s16(0x0);
int16x8_t t0, t2, tcnext;
ptrdiff_t x = 0;
const ptrdiff_t bcols = y + 2 < rows ? cols : (cols - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(v0 + x);
internal::prefetch(v1 + x);
internal::prefetch(v2 + x);
uint8x8_t x0 = vld1_u8(v0 + x);
uint8x8_t x1 = vld1_u8(v1 + x);
uint8x8_t x2 = vld1_u8(v2 + x);
tcnext = vreinterpretq_s16_u16(vshll_n_u8(x1, 2));
if(x) {
tprev = tcurr;
tcurr = tnext;
}
tnext = vreinterpretq_s16_u16(vaddl_u8(x0, x2));
if(!x) {
tcurr = tnext;
tc = tcnext;
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT)
{
tcurr = vsetq_lane_s16(vgetq_lane_s16(tcurr, 0),tcurr, 7);
}
else if (border == BORDER_MODE_CONSTANT)
{
tcurr = vsetq_lane_s16(borderValue, tcurr, 7);
}
else if (border == BORDER_MODE_REFLECT101)
{
tcurr = vsetq_lane_s16(vgetq_lane_s16(tcurr, 1),tcurr, 7);
}
continue;
}
t0 = vextq_s16(tprev, tcurr, 7);
t2 = vextq_s16(tcurr, tnext, 1);
t0 = vsubq_s16(vqaddq_s16(t0, t2), tc);
tc = tcnext;
t0 = vshlq_n_s16(t0, 1);
vst1q_s16(drow + x - 8, t0);
}
x -= 8;
if(x == cols){
x--;
}
for( ; x < cols; x++ )
{
s16 nextx, nextx2;
s16 prevx, prevx2;
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT)
{
prevx = x == 0 ? v0[0] : v0[x-1];
prevx2 = x == 0 ? v2[0] : v2[x-1];
nextx = x == cols-1 ? v0[x] : v0[x+1];
nextx2 = x == cols-1 ? v2[x] : v2[x+1];
}
else if (border == BORDER_MODE_REFLECT101)
{
prevx = x == 0 ? v0[1] : v0[x-1];
prevx2 = x == 0 ? v2[1] : v2[x-1];
nextx = x == cols-1 ? v0[x-1] : v0[x+1];
nextx2 = x == cols-1 ? v2[x-1] : v2[x+1];
}
else //if (border == BORDER_MODE_CONSTANT)
{
prevx = x == 0 ? borderValue : v0[x-1];
prevx2 = x == 0 ? borderValue : v2[x-1];
nextx = x == cols-1 ? borderValue : v0[x+1];
nextx2 = x == cols-1 ? borderValue : v2[x+1];
}
s16 res = prevx + nextx - 4*v1[x] + prevx2 + nextx2;
*(drow+x) = 2*res;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
#endif
}
void Laplacian5OpenCV(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
s16 * dstBase, ptrdiff_t dstStride,
BORDER_MODE border, u8 borderValue)
{
internal::assertSupportedConfiguration(isLaplacianOpenCVSupported(size, border));
#ifdef CAROTENE_NEON
ptrdiff_t rows = size.height, cols = size.width;
std::vector<u8> _tmp;
u8 *tmp = 0;
if (border == BORDER_MODE_CONSTANT)
{
_tmp.assign(cols + 4,borderValue);
tmp = &_tmp[2];
}
for( ptrdiff_t y = 0; y < rows; y++ )
{
const u8* v0 = 0;
const u8* v1 = 0;
const u8* v2 = internal::getRowPtr(srcBase, srcStride, y);
const u8* v3 = 0;
const u8* v4 = 0;
// make border
if (border == BORDER_MODE_REPLICATE) {
v0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : 0);
v1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
v3 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 0 ? rows-1 : 0);
v4 = internal::getRowPtr(srcBase, srcStride, y < rows-2 ? y+2 : rows > 0 ? rows-1 : 0);
} else if (border == BORDER_MODE_REFLECT) {
v0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : rows > 1 ? 1-y : 0);
v1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : 0);
v3 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 0 ? rows-1 : 0);
v4 = internal::getRowPtr(srcBase, srcStride, y < rows-2 ? y+2 : rows > 1 ? 2*rows-(y+3) : 0);
} else if (border == BORDER_MODE_REFLECT101) {
v0 = internal::getRowPtr(srcBase, srcStride, y > 1 ? y-2 : rows > 2-y ? 2-y : 0); ///check
v1 = internal::getRowPtr(srcBase, srcStride, y > 0 ? y-1 : rows > 1 ? 1 : 0);
v3 = internal::getRowPtr(srcBase, srcStride, y < rows-1 ? y+1 : rows > 1 ? rows-2 : 0);
v4 = internal::getRowPtr(srcBase, srcStride, y < rows-2 ? y+2 : rows > 2 ? 2*rows-(y+4) : 0);///bad if rows=2 y=1   rows - 4 + (2,1)
} else if (border == BORDER_MODE_CONSTANT) {
v0 = y > 1 ? internal::getRowPtr(srcBase, srcStride, y-2) : tmp;
v1 = y > 0 ? internal::getRowPtr(srcBase, srcStride, y-1) : tmp;
v3 = y < rows-1 ? internal::getRowPtr(srcBase, srcStride, y+1) : tmp;
v4 = y < rows-2 ? internal::getRowPtr(srcBase, srcStride, y+2) : tmp;
}
s16* drow = internal::getRowPtr(dstBase, dstStride, y);
int16x8_t tnext, tc, t0;
int16x8_t tnext2, tnext3;
int16x8_t tnext1Old, tnext2Old, tnext3Old;
int16x8_t tnext4OldOldOld, tnext5OldOldOld;
int16x8_t tcurr1 = vmovq_n_s16(0x0);
int16x8_t tnext1 = vmovq_n_s16(0x0);
int16x8_t tprev1 = vmovq_n_s16(0x0);
int16x8_t tpprev1 = vmovq_n_s16(0x0);
int16x8_t tppprev1 = vmovq_n_s16(0x0);
int16x8_t tnext4Old = vmovq_n_s16(0x0);
int16x8_t tnext5Old = vmovq_n_s16(0x0);
int16x8_t tnext1OldOld = vmovq_n_s16(0x0);
int16x8_t tnext2OldOld = vmovq_n_s16(0x0);
int16x8_t tnext3OldOld = vmovq_n_s16(0x0);
int16x8_t tnext4OldOld = vmovq_n_s16(0x0);
int16x8_t tnext5OldOld = vmovq_n_s16(0x0);
// do vertical convolution
ptrdiff_t x = 0;
const ptrdiff_t bcols = y + 3 < rows ? cols : (cols - 8);
for( ; x <= bcols; x += 8 )
{
internal::prefetch(v0 + x);
internal::prefetch(v1 + x);
internal::prefetch(v2 + x);
internal::prefetch(v3 + x);
internal::prefetch(v4 + x);
uint8x8_t x0 = vld1_u8(v0 + x);
uint8x8_t x1 = vld1_u8(v1 + x);
uint8x8_t x2 = vld1_u8(v2 + x);
uint8x8_t x3 = vld1_u8(v3 + x);
uint8x8_t x4 = vld1_u8(v4 + x);
if(x) {
tcurr1 = tnext1;
}
tnext4OldOldOld = tnext4Old;
tnext5OldOldOld = tnext5Old;
tnext1Old = tnext1OldOld;
tnext2Old = tnext2OldOld;
tnext3Old = tnext3OldOld;
tnext4Old = tnext4OldOld;
tnext5Old = tnext5OldOld;
tnext3 = vreinterpretq_s16_u16(vaddq_u16(vaddl_u8(x3, x2),vaddl_u8(x2, x1)));
tnext3 = vshlq_n_s16(tnext3, 1);
tc = vreinterpretq_s16_u16(vsubl_u8(x4, x2));
tnext = vreinterpretq_s16_u16(vsubl_u8(x2, x0));
tnext2 = vsubq_s16(tc, tnext);
tnext1 = vaddq_s16(tnext3, tnext2);
// tnext1 = x0 + 2*x1 + 2*x2 + 2*x3 + x4
tnext2 = vshlq_n_s16(tnext2, 1);
// tnext2 = 2*x4 - 4*x2 + 2*x0
tnext3 = vsubq_s16(tnext2, vshlq_n_s16(tnext3, 1));
// tnext3 = 2*x0 - 4*x1 - 12*x2 - 4*x3  + 2*x4
tnext1OldOld = tnext1;
tnext2OldOld = tnext2;
tnext3OldOld = tnext3;
tnext4OldOld = tnext2;
tnext5OldOld = tnext1;
if(x) {
tnext1 = vextq_s16(tnext1Old, tnext1, 2);
tcurr1 = vextq_s16(tnext2Old, tnext2, 1);
tprev1 = tnext3Old;
if(x!=8) {
tpprev1 = vextq_s16(tnext4OldOldOld, tnext4Old, 7);
tppprev1 = vextq_s16(tnext5OldOldOld, tnext5Old, 6);
}
}
if(!x) {
// make border
if (border == BORDER_MODE_REPLICATE) {
tpprev1 = vextq_s16(tnext2, tnext2, 7);
tpprev1 = vsetq_lane_s16(vgetq_lane_s16(tpprev1, 1),tpprev1, 0);
tprev1 = vextq_s16(tnext1, tnext1, 6);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 2),tprev1, 0);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 2),tprev1, 1);
} else if (border == BORDER_MODE_REFLECT) {
tpprev1 = vextq_s16(tnext2, tnext2, 7);
tpprev1 = vsetq_lane_s16(vgetq_lane_s16(tpprev1, 1),tpprev1, 0);
tprev1 = vextq_s16(tnext1, tnext1, 6);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 3),tprev1, 0);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 2),tprev1, 1);
} else if (border == BORDER_MODE_REFLECT101) {
tpprev1 = vextq_s16(tnext2, tnext2, 7);
tpprev1 = vsetq_lane_s16(vgetq_lane_s16(tpprev1, 2),tpprev1, 0);
tprev1 = vextq_s16(tnext1, tnext1, 6);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 3),tprev1, 1);
tprev1 = vsetq_lane_s16(vgetq_lane_s16(tprev1, 4),tprev1, 0);
} else if (border == BORDER_MODE_CONSTANT) {
tpprev1 = vextq_s16(tnext2, tnext2, 7);
tpprev1 = vsetq_lane_s16(borderValue, tpprev1, 0);
tprev1 = vextq_s16(tnext1, tnext1, 6);
tprev1 = vsetq_lane_s16(borderValue, tprev1, 0);
tprev1 = vsetq_lane_s16(borderValue, tprev1, 1);
}
tppprev1 = tprev1;
continue;
}
t0 = vaddq_s16(vaddq_s16(vqaddq_s16(tcurr1, tprev1), vqaddq_s16(tpprev1, tppprev1)), tnext1);
t0 = vaddq_s16(t0, t0);
vst1q_s16(drow + x - 8, t0);
}
x -= 8;
if(x >= cols - 1)
x = cols-2;
s16 pprevx = 0;
s16 prevx = 0;
s16 nextx = 0;
s16 nnextx = 0;
for( ; x < cols; x++ )
{
if (x == 0) {
// make border
if (border == BORDER_MODE_REPLICATE) {
pprevx = v0[0] + 2*v1[0] + 2*v2[0] + 2*v3[0] + v4[0];
prevx = 2*v0[0] - 4*v2[0] + 2*v4[0];
} else if (border == BORDER_MODE_REFLECT) {
pprevx = v0[1] + 2*v1[1] + 2*v2[1] + 2*v3[1] + v4[1];
prevx = 2*v0[0] - 4*v2[0] + 2*v4[0];
} else if (border == BORDER_MODE_REFLECT101) {
pprevx = v0[2] + 2*v1[2] + 2*v2[2] + 2*v3[2] + v4[2];
prevx = 2*v0[1] - 4*v2[1] + 2*v4[1];
} else if (border == BORDER_MODE_CONSTANT) {
pprevx = 8 * borderValue;
prevx = 0;
}
} else if (x == 1) {
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT) {
pprevx = v0[0] + 2*v1[0] + 2*v2[0] + 2*v3[0] + v4[0];
} else if (border == BORDER_MODE_REFLECT101) {
pprevx = v0[1] + 2*v1[1] + 2*v2[1] + 2*v3[1] + v4[1];
} else if (border == BORDER_MODE_CONSTANT) {
pprevx = 8 * borderValue;
}
prevx = 2*v0[0] - 4*v2[0] + 2*v4[0];
} else {
pprevx = v0[x-2] + 2*v1[x-2] + 2*v2[x-2] + 2*v3[x-2] + v4[x-2];
prevx = 2*v0[x-1] - 4*v2[x-1] + 2*v4[x-1];
}
s16 currx = 2*v0[x] - 4*v1[x] - 12*v2[x] - 4*v3[x] + 2*v4[x];
if (x == cols-1) {
// make border
if (border == BORDER_MODE_REPLICATE) {
nextx = 2*v0[x] - 4*v2[x] + 2*v4[x];
nnextx = v0[x] + 2*v1[x] + 2*v2[x] + 2*v3[x] + v4[x];
} else if (border == BORDER_MODE_REFLECT) {
nextx = 2*v0[x] - 4*v2[x] + 2*v4[x];
nnextx = v0[x-1] + 2*v1[x-1] + 2*v2[x-1] + 2*v3[x-1] + v4[x-1];
} else if (border == BORDER_MODE_REFLECT101) {
nextx = 2*v0[x-1] - 4*v2[x-1] + 2*v4[x-1];
nnextx = v0[x-2] + 2*v1[x-2] + 2*v2[x-2] + 2*v3[x-2] + v4[x-2];
} else if (border == BORDER_MODE_CONSTANT) {
nextx = 0;
nnextx = 8 * borderValue;
}
} else if (x == cols-2) {
// make border
if (border == BORDER_MODE_REPLICATE || border == BORDER_MODE_REFLECT) {
nnextx = v0[x+1] + 2*v1[x+1] + 2*v2[x+1] + 2*v3[x+1] + v4[x+1];
} else if (border == BORDER_MODE_REFLECT101) {
nnextx = v0[x] + 2*v1[x] + 2*v2[x] + 2*v3[x] + v4[x];
} else if (border == BORDER_MODE_CONSTANT) {
nnextx = 8 * borderValue;
}
nextx = 2*v0[x+1] - 4*v2[x+1] + 2*v4[x+1];
} else {
nextx = 2*v0[x+1] - 4*v2[x+1] + 2*v4[x+1];
nnextx = v0[x+2] + 2*v1[x+2] + 2*v2[x+2] + 2*v3[x+2] + v4[x+2];
}
s16 res = pprevx + prevx + currx + nextx + nnextx;
*(drow+x) = 2*res;
}
}
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)dstBase;
(void)dstStride;
(void)border;
(void)borderValue;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include "vtransform.hpp"
#include <cmath>
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
struct Magnitude
{
typedef s16 type;
void operator() (const int16x8_t & v_src0, const int16x8_t & v_src1,
int16x8_t & v_dst) const
{
int16x4_t v_src0_p = vget_low_s16(v_src0), v_src1_p = vget_low_s16(v_src1);
float32x4_t v_sqr0 = vaddq_f32(vcvtq_f32_s32(vmull_s16(v_src0_p, v_src0_p)),
vcvtq_f32_s32(vmull_s16(v_src1_p, v_src1_p)));
v_src0_p = vget_high_s16(v_src0);
v_src1_p = vget_high_s16(v_src1);
float32x4_t v_sqr1 = vaddq_f32(vcvtq_f32_s32(vmull_s16(v_src0_p, v_src0_p)),
vcvtq_f32_s32(vmull_s16(v_src1_p, v_src1_p)));
int32x4_t v_sqrt0 = vcvtq_s32_f32(internal::vsqrtq_f32(v_sqr0));
int32x4_t v_sqrt1 = vcvtq_s32_f32(internal::vsqrtq_f32(v_sqr1));
v_dst = vcombine_s16(vqmovn_s32(v_sqrt0), vqmovn_s32(v_sqrt1));
}
void operator() (const int16x4_t & v_src0, const int16x4_t & v_src1,
int16x4_t & v_dst) const
{
float32x4_t v_tmp = vaddq_f32(vcvtq_f32_s32(vmull_s16(v_src0, v_src0)),
vcvtq_f32_s32(vmull_s16(v_src1, v_src1)));
int32x4_t v_sqrt = vcvtq_s32_f32(internal::vsqrtq_f32(v_tmp));
v_dst = vqmovn_s32(v_sqrt);
}
void operator() (const short * src0, const short * src1, short * dst) const
{
f32 src0val = (f32)src0[0], src1val = (f32)src1[0];
dst[0] = internal::saturate_cast<s16>((s32)sqrtf(src0val * src0val + src1val * src1val));
}
};
struct MagnitudeF32
{
typedef f32 type;
void operator() (const float32x4_t & v_src0, const float32x4_t & v_src1,
float32x4_t & v_dst) const
{
v_dst = internal::vsqrtq_f32(vaddq_f32(vmulq_f32(v_src0, v_src0), vmulq_f32(v_src1, v_src1)));
}
void operator() (const float32x2_t & v_src0, const float32x2_t & v_src1,
float32x2_t & v_dst) const
{
v_dst = internal::vsqrt_f32(vadd_f32(vmul_f32(v_src0, v_src0), vmul_f32(v_src1, v_src1)));
}
void operator() (const f32 * src0, const f32 * src1, f32 * dst) const
{
dst[0] = sqrtf(src0[0] * src0[0] + src1[0] * src1[0]);
}
};
} // namespace
#endif
void magnitude(const Size2D &size,
const s16 * src0Base, ptrdiff_t src0Stride,
const s16 * src1Base, ptrdiff_t src1Stride,
s16 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
Magnitude());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
void magnitude(const Size2D &size,
const f32 * src0Base, ptrdiff_t src0Stride,
const f32 * src1Base, ptrdiff_t src1Stride,
f32 * dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
internal::vtransform(size,
src0Base, src0Stride,
src1Base, src1Stride,
dstBase, dstStride,
MagnitudeF32());
#else
(void)size;
(void)src0Base;
(void)src0Stride;
(void)src1Base;
(void)src1Stride;
(void)dstBase;
(void)dstStride;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
#include <cmath>
namespace CAROTENE_NS {
void meanStdDev(const Size2D &size,
const u8 * srcBase, ptrdiff_t srcStride,
f32 * pMean, f32 * pStdDev)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
f64 fsum = 0.0f, fsqsum = 0.0f;
sqsum(size, srcBase, srcStride, &fsum, &fsqsum, 1);
// calc mean and stddev
f64 itotal = 1.0 / size.total();
f64 mean = fsum * itotal;
f64 stddev = sqrt(std::max(fsqsum * itotal - mean * mean, 0.0));
if (pMean)
*pMean = mean;
if (pStdDev)
*pStdDev = stddev;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)pMean;
(void)pStdDev;
#endif
}
void meanStdDev(const Size2D &size,
const u16 * srcBase, ptrdiff_t srcStride,
f32 * pMean, f32 * pStdDev)
{
internal::assertSupportedConfiguration();
#ifdef CAROTENE_NEON
size_t blockSize0 = 1 << 10, roiw4 = size.width & ~3;
f64 fsum = 0.0f, fsqsum = 0.0f;
f32 arsum[8];
uint32x4_t v_zero = vdupq_n_u32(0u), v_sum;
float32x4_t v_zero_f = vdupq_n_f32(0.0f), v_sqsum;
for (size_t i = 0; i < size.height; ++i)
{
const u16 * src = internal::getRowPtr(srcBase, srcStride, i);
size_t j = 0u;
while (j < roiw4)
{
size_t blockSize = std::min(roiw4 - j, blockSize0) + j;
v_sum = v_zero;
v_sqsum = v_zero_f;
for ( ; j + 16 < blockSize ; j += 16)
{
internal::prefetch(src + j);
uint16x8_t v_src0 = vld1q_u16(src + j), v_src1 = vld1q_u16(src + j + 8);
// 0
uint32x4_t v_srclo = vmovl_u16(vget_low_u16(v_src0));
uint32x4_t v_srchi = vmovl_u16(vget_high_u16(v_src0));
v_sum = vaddq_u32(v_sum, vaddq_u32(v_srclo, v_srchi));
float32x4_t v_srclo_f = vcvtq_f32_u32(v_srclo);
float32x4_t v_srchi_f = vcvtq_f32_u32(v_srchi);
v_sqsum = vmlaq_f32(v_sqsum, v_srclo_f, v_srclo_f);
v_sqsum = vmlaq_f32(v_sqsum, v_srchi_f, v_srchi_f);
// 1
v_srclo = vmovl_u16(vget_low_u16(v_src1));
v_srchi = vmovl_u16(vget_high_u16(v_src1));
v_sum = vaddq_u32(v_sum, vaddq_u32(v_srclo, v_srchi));
v_srclo_f = vcvtq_f32_u32(v_srclo);
v_srchi_f = vcvtq_f32_u32(v_srchi);
v_sqsum = vmlaq_f32(v_sqsum, v_srclo_f, v_srclo_f);
v_sqsum = vmlaq_f32(v_sqsum, v_srchi_f, v_srchi_f);
}
for ( ; j < blockSize; j += 4)
{
uint32x4_t v_src = vmovl_u16(vld1_u16(src + j));
float32x4_t v_src_f = vcvtq_f32_u32(v_src);
v_sum = vaddq_u32(v_sum, v_src);
v_sqsum = vmlaq_f32(v_sqsum, v_src_f, v_src_f);
}
vst1q_f32(arsum, vcvtq_f32_u32(v_sum));
vst1q_f32(arsum + 4, v_sqsum);
fsum += (f64)arsum[0] + arsum[1] + arsum[2] + arsum[3];
fsqsum += (f64)arsum[4] + arsum[5] + arsum[6] + arsum[7];
}
// collect a few last elements in the current row
for ( ; j < size.width; ++j)
{
f32 srcval = src[j];
fsum += srcval;
fsqsum += srcval * srcval;
}
}
// calc mean and stddev
f64 itotal = 1.0 / size.total();
f64 mean = fsum * itotal;
f64 stddev = sqrt(std::max(fsqsum * itotal - mean * mean, 0.0));
if (pMean)
*pMean = mean;
if (pStdDev)
*pStdDev = stddev;
#else
(void)size;
(void)srcBase;
(void)srcStride;
(void)pMean;
(void)pStdDev;
#endif
}
} // namespace CAROTENE_NS

/*
* By downloading, copying, installing or using the software you agree to this license.
* If you do not agree to this license, do not download, install,
* copy or use the software.
*
*
*                           License Agreement
*                For Open Source Computer Vision Library
*                        (3-clause BSD License)
*
* Copyright (C) 2012-2014, NVIDIA Corporation, all rights reserved.
* Third party copyrights are property of their respective owners.
*
* Redistribution and use in source and binary forms, with or without modification,
* are permitted provided that the following conditions are met:
*
*   * Redistributions of source code must retain the above copyright notice,
*     this list of conditions and the following disclaimer.
*
*   * Redistributions in binary form must reproduce the above copyright notice,
*     this list of conditions and the following disclaimer in the documentation
*     and/or other materials provided with the distribution.
*
*   * Neither the names of the copyright holders nor the names of the contributors
*     may be used to endorse or promote products derived from this software
*     without specific prior written permission.
*
* This software is provided by the copyright holders and contributors "as is" and
* any express or implied warranties, including, but not limited to, the implied
* warranties of merchantability and fitness for a particular purpose are disclaimed.
* In no event shall copyright holders or contributors be liable for any direct,
* indirect, incidental, special, exemplary, or consequential damages
* (including, but not limited to, procurement of substitute goods or services;
* loss of use, data, or profits; or business interruption) however caused
* and on any theory of liability, whether in contract, strict liability,
* or tort (including negligence or otherwise) arising in any way out of
* the use of this software, even if advised of the possibility of such damage.
*/
#include "common.hpp"
/*
* The code here is based on the code in
* <http://ndevilla.free.fr/median/median/src/optmed.c>, which is in public domain.
* See also <http://ndevilla.free.fr/median/median/index.html>.
*/
namespace CAROTENE_NS {
#ifdef CAROTENE_NEON
namespace {
uint8x16_t getLeftReplicate(uint8x16_t r, u32 cn)
{
u8 buf[16+8];
vst1q_u8(buf+cn, r);
for (u32 i = 0; i < cn; ++i) buf[i] = buf[cn+i];
return vld1q_u8(buf);
}
uint8x8_t getRightReplicate(uint8x8_t r, u32 cn)
{
u8 buf[8+8];
vst1_u8(buf, r);
for (u32 i = 0; i < cn; ++i) buf[8+i] = buf[8-cn+i];
return vld1_u8(buf+cn);
}
} // namespace
//o------^-------^-----------------------------o 0
//       |       |
//o--^---v---^---|-------^---------------------o 1
//   |       |   |       |
//o--v-------v---|-------|-^-------^-------^---o 2
//               |       | |       |       |
//o------^-------v-----^-|-|-------|-------|---o 3
//       |             | | |       |       |
//o--^---v---^-----^---|-v-|---^---v---^---v---o 4
//   |       |     |   |   |   |       |
//o--v-------v---^-|---|---v---|-------|-------o 5
//               | |   |       |       |
//o------^-------|-|---v-------|-------v-------o 6
//       |       | |           |
//o--^---v---^---|-v-----------v---------------o 7
//   |       |   |
//o--v-------v---v-----------------------------o 8
#define ELT(num, level) v ## num ## _lv ## level
#define PIX_SORT(a, alvl, b, blvl, newlvl) \
PIX_MIN(a, alvl, b, blvl, newlvl); \
PIX_MAX(a, alvl, b, blvl, newlvl);
#define SORT9 \
PIX_SORT(1, 00, 2, 00, 01); \
PIX_SORT(4, 00, 5, 00, 02); \
PIX_SORT(7, 00, 8, 00, 03); \
PIX_SORT(0, 00, 1, 01, 04); \
PIX_SORT(3, 00, 4, 02, 05); \
PIX_SORT(6, 00, 7, 03, 06); \
PIX_SORT(1, 04, 2, 01, 07); \
PIX_SORT(4, 05, 5, 02, 08); \
PIX_SORT(7, 06, 8, 03, 09); \
PIX_MAX (0, 04, 3, 05, 10); \
PIX_MIN (5, 08, 8, 09, 11); \
PIX_SORT(4, 08, 7, 09, 12); \
PIX_MAX (3, 10, 6, 06, 13); \
PIX_MAX (1, 07, 4, 12, 14); \
PIX_MIN (2, 07, 5, 11, 15); \
PIX_MIN (4, 14, 7, 12, 16); \
PIX_SORT(4, 16, 2, 15, 17); \
PIX_MAX (6, 13, 4, 17, 18); \
PIX_MIN (4, 18, 2, 17, 19);
#endif
bool isMedianFilter3x3Supported(const Size2D &size, u32 numChannels)
{
return isSupportedConfiguration() && size.width >= 16 + numChannels && numChannels <= 8;
}
void medianFilter3x3(const Size2D &size, u32 numChannels,
const u8 *srcBase, ptrdiff_t srcStride,
const Margin &srcMargin,
u8 *dstBase, ptrdiff_t dstStride)
{
internal::assertSupportedConfiguration(isMedianFilter3x3Supported(size, numChannels));
#ifdef CAROTENE_NEON
u32 cn = numChannels;
size_t colsn = size.width * cn;
for (size_t i = 0; i < size.height; ++i) {
const u8* psrc1 = internal::getRowPtr(srcBase, srcStride, i);
const u8* psrc0 = i == 0 && srcMargin.top == 0 ? psrc1 : psrc1 - srcStride;
const u8* psrc2 = i + 1 == size.height && srcMargin.bottom == 0 ? psrc1 : psrc1 + srcStride;
u8* pdst = internal::getRowPtr(dstBase, dstStride, i);
size_t j = 0;
{
uint8x16_t v3_lv00 = vld1q_u8(psrc0);
uint8x16_t v4_lv00 = vld1q_u8(psrc1);
uint8x16_t v5_lv00 = vld1q_u8(psrc2);
uint8x16_t v6_lv00 = vld1q_u8(psrc0 + cn);
uint8x16_t v7_lv00 = vld1q_u8(psrc1 + cn);
uint8x16_t v8_lv00 = vld1q_u8(psrc2 + cn);
uint8x16_t v0_lv00 = srcMargin.left > 0 ? vld1q_u8(psrc0 - cn) : getLeftReplicate(v3_lv00, cn);
uint8x16_t v1_lv00 = srcMargin.left > 0 ? vld1q_u8(psrc1 - cn) : getLeftReplicate(v4_lv00, cn);
uint8x16_t v2_lv00 = srcMargin.left > 0 ? vld1q_u8(psrc2 - cn) : getLeftReplicate(v5_lv00, cn);
goto medianBlur3x3_mainBody;
for (; j < colsn - 16; j += 16) {
internal::prefetch(psrc0 + j);
internal::prefetch(psrc1 + j);
internal::prefetch(psrc2 + j);
v0_lv00 = vld1q_u8(psrc0 + j - cn);
v1_lv00 = vld1q_u8(psrc1 + j - cn);
v2_lv00 = vld1q_u8(psrc2 + j - cn);
v3_lv00 = vld1q_u8(psrc0 + j);
v4_lv00 = vld1q_u8(psrc1 + j);
v5_lv00 = vld1q_u8(psrc2 + j);
v6_lv00 = vld1q_u8(psrc0 + j + cn);
v7_lv00 = vld1q_u8(psrc1 + j + cn);
v8_lv00 = vld1q_u8(psrc2 + j + cn);
medianBlur3x3_mainBody:
#define PIX_MIN(a, alvl, b, blvl, newlvl) uint8x16_t ELT(a, newlvl) = vminq_u8(ELT(a, alvl), ELT(b, blvl))
#define PIX_MAX(a, alvl, b, blvl, newlvl) uint8x16_t ELT(b, newlvl) = vmaxq_u8(ELT(a, alvl), ELT(b, blvl))
SORT9;
#undef PIX_MAX